{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/KISS_Final_CX_CB_Run_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want something closer to a minimal base run, you'd want:\n",
        "\n",
        "This is the most basic statistical models (mean, median, mode, naive, et al.)\n",
        "- No hyperparameter optimization\n",
        "- Minimal feature engineering\n",
        "- Simple train/test splits\n",
        "-Default model parameters\n",
        "\n",
        "**Business Explanation: Call Center Forecasting Base Run Test**\n",
        "\n",
        "## What We're Doing\n",
        "\n",
        "We're testing the most basic ways to predict how many calls your call center will receive each day. Think of this as establishing a \"starting line\" before we try more complex approaches.\n",
        "\n",
        "## The Four Simple Methods We're Testing\n",
        "\n",
        "**1. Average Method:** \"Tomorrow will be like our typical day\"\n",
        "- Uses the overall average of all past days\n",
        "- Simple but ignores seasonal patterns\n",
        "\n",
        "**2. Median Method:** \"Tomorrow will be like our middle-performing day\"\n",
        "- Uses the middle value, less affected by unusually busy or slow days\n",
        "- More stable than average when you have extreme outliers\n",
        "\n",
        "**3. Most Common Method:** \"Tomorrow will be like our most frequent type of day\"\n",
        "- Finds the call volume that happens most often\n",
        "- Good when your operation has very consistent patterns\n",
        "\n",
        "**4. Day-of-Week Average:** \"Tomorrow will be like our typical Monday/Tuesday/etc.\"\n",
        "- Uses historical average for each day of the week (all past Mondays, Tuesdays, etc.)\n",
        "- Captures day-of-week patterns (like \"Mondays are always busier\")\n",
        "\n",
        "## Why Start Here?\n",
        "\n",
        "Before investing in complex algorithms, we need to know: \"How well can simple common-sense approaches work?\" If a basic method works well enough, you might not need expensive complex solutions.\n",
        "\n",
        "## What the Results Tell You\n",
        "\n",
        "**If \"Same Day Last Week\" wins:** Your call patterns are strongly driven by day-of-week effects. Focus on scheduling models that account for weekly cycles.\n",
        "\n",
        "**If \"Average\" or \"Median\" wins:** Your call volume is relatively stable with minimal patterns. Simple capacity planning may be sufficient.\n",
        "\n",
        "**If no method beats \"Same Day Last Week\":** You have strong weekly patterns that any forecasting solution must account for.\n",
        "\n",
        "## Business Value\n",
        "\n",
        "- **Cost:** These simple methods cost almost nothing to implement\n",
        "- **Speed:** Results available immediately\n",
        "- **Baseline:** Establishes minimum acceptable performance\n",
        "- **Decision Framework:** Helps determine if you need more sophisticated (expensive) forecasting tools\n",
        "\n",
        "This test answers: \"What's the simplest approach that could work?\" before you invest in complex solutions."
      ],
      "metadata": {
        "id": "2ZcGbg9nJXJA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xi7myhJJVqq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "tIszMQMI-7UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Example: Move a tensor to the GPU\n",
        "x = torch.randn(10, 10).to(device)\n",
        "\n",
        "# Example: Move a model to the GPU\n",
        "# model = YourModel().to(device)"
      ],
      "metadata": {
        "id": "PrvOMMXU-652"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4\n",
        "!pip install tensorflow\n",
        "!pip install tbats\n",
        "!pip install pmdarima"
      ],
      "metadata": {
        "id": "x9QF-zGq_YkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yLAeiKH4aFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BASE RUN CONFIGURATION - SET THESE FIRST\n",
        "print(\"CONFIGURATION: BASE RUN MODE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Base Run Flags\n",
        "BASE_RUN_MODE = True\n",
        "ENABLE_ONLY_BASIC_MODELS = True\n",
        "ENABLE_ML_MODELS = False\n",
        "ENABLE_NEURAL = False\n",
        "ENABLE_ADVANCED_TS = False\n",
        "ENABLE_OPTIMIZATION = False\n",
        "# Note: Cross-validation hardcoded to 1 split for base run speed\n",
        "\n",
        "# MODEL SELECTION CONFIGURATION\n",
        "print(\"\\nMODEL SELECTION:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Core Basic Models\n",
        "ENABLE_MEAN = True              # Overall historical average\n",
        "ENABLE_MEDIAN = True            # Middle value (robust to outliers)\n",
        "ENABLE_MODE = True              # Most frequent value\n",
        "\n",
        "\n",
        "# Additional Standard Benchmarks\n",
        "ENABLE_NAIVE = True             # Yesterday's value (important baseline!)\n",
        "ENABLE_MOVING_AVG_7 = True      # Average of last 7 days\n",
        "ENABLE_DAY_OF_WEEK_AVG = True   # Historical Monday avg, Tuesday avg, etc.\n",
        "ENABLE_LINEAR_TREND = False     # Simple time trend (can be unstable)\n",
        "\n",
        "# Automatically count enabled models\n",
        "model_config = {\n",
        "    \"Mean\": ENABLE_MEAN,\n",
        "    \"Median\": ENABLE_MEDIAN,\n",
        "    \"Mode\": ENABLE_MODE,\n",
        "    \"Naive\": ENABLE_NAIVE,\n",
        "    \"Moving Avg 7d\": ENABLE_MOVING_AVG_7,\n",
        "    \"Day-of-Week Avg\": ENABLE_DAY_OF_WEEK_AVG,\n",
        "    \"Linear Trend\": ENABLE_LINEAR_TREND\n",
        "}\n",
        "\n",
        "enabled_models = [name for name, enabled in model_config.items() if enabled]\n",
        "print(f\"Enabled Models ({len(enabled_models)}): {', '.join(enabled_models)}\")\n",
        "\n",
        "# Model Version\n",
        "MODEL_VERSION = \"BASE_RUN_V1\" if BASE_RUN_MODE else \"V1_EXPANDED_FINAL\"\n",
        "print(f\"Model Version: {MODEL_VERSION}\")\n",
        "print(\"Cross-Validation: 1 split only\")\n",
        "print(\"Feature Engineering: Minimal\")\n",
        "print(\"Optimization: DISABLED\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Configuration complete. Ready for configurable base run evaluation.\")"
      ],
      "metadata": {
        "id": "I30j-HBV4dJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries for base run\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "# ML metrics only (no complex models)\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Environment check\n",
        "print(\"ENVIRONMENT CHECK - BASE RUN\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# RAM Check\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print(f'RAM Available: {ram_gb:.1f} GB')\n",
        "\n",
        "# Visualization setup\n",
        "plt.style.use('seaborn-v0_8') if 'seaborn-v0_8' in plt.style.available else plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(\"Base run environment ready\")\n",
        "print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "JvIPcvcL5XHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rgn8yksB4ptm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_call_center_data_base_run(file_path='enhanced_eda_data.csv'):\n",
        "    \"\"\"\n",
        "    Load call center data - BASE RUN VERSION\n",
        "    Minimal preprocessing, no complex feature engineering\n",
        "    \"\"\"\n",
        "    print(\"LOADING CALL CENTER DATA - BASE RUN\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    try:\n",
        "        # Load main data file\n",
        "        df = pd.read_csv(file_path, index_col='Date', parse_dates=True)\n",
        "        print(f\"Loaded {len(df)} records from {file_path}\")\n",
        "\n",
        "        # Auto-detect call volume column\n",
        "        volume_cols = ['calls', 'Calls', 'call_volume', 'Call_Volume', 'volume', 'Volume']\n",
        "        volume_col = None\n",
        "\n",
        "        for col in volume_cols:\n",
        "            if col in df.columns:\n",
        "                volume_col = col\n",
        "                break\n",
        "\n",
        "        if volume_col is None:\n",
        "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "            volume_col = numeric_cols[0] if len(numeric_cols) > 0 else df.columns[0]\n",
        "\n",
        "        print(f\"Call volume column identified: {volume_col}\")\n",
        "\n",
        "        # Standardize column name and keep only call volume\n",
        "        if volume_col != 'calls':\n",
        "            df = df.rename(columns={volume_col: 'calls'})\n",
        "\n",
        "        # BASE RUN: Keep only the call volume column\n",
        "        df = df[['calls']].copy()\n",
        "\n",
        "        # Basic data cleaning: Remove first and last rows if needed\n",
        "        print(\"Basic data cleaning...\")\n",
        "        original_len = len(df)\n",
        "        if len(df) > 2:\n",
        "            df = df.iloc[1:-1]\n",
        "            print(f\"   Cleaned: {original_len} -> {len(df)} rows\")  # FIXED: removed unicode arrow\n",
        "\n",
        "        # Remove any missing values\n",
        "        df = df.dropna()\n",
        "\n",
        "        print(f\"\\nBASE RUN DATASET OVERVIEW\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"   Date range: {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\")\n",
        "        print(f\"   Total days: {len(df)}\")\n",
        "        print(f\"   Call volume range: {df['calls'].min():.0f} to {df['calls'].max():.0f}\")\n",
        "        print(f\"   Average daily calls: {df['calls'].mean():.0f}\")\n",
        "        print(\"-\" * 30)\n",
        "        print(\"Base run data loading complete!\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "G7eLXlSJ7Rpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7_mPkFbU7R0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_time_series_splits_base_run(df, n_splits=1, test_size=7, gap=0):\n",
        "    \"\"\"\n",
        "    Create time series cross-validation splits - BASE RUN VERSION\n",
        "    Uses only 1 split for speed and simplicity\n",
        "    \"\"\"\n",
        "    print(\"TIME SERIES CROSS-VALIDATION - BASE RUN\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    splits = []\n",
        "    total_size = len(df)\n",
        "\n",
        "    # Base run uses only 1 split for simplicity\n",
        "    actual_splits = min(n_splits, 1) if BASE_RUN_MODE else n_splits\n",
        "\n",
        "    for i in range(actual_splits):\n",
        "        test_end = total_size - i * test_size\n",
        "        test_start = test_end - test_size\n",
        "        train_end = test_start - gap\n",
        "\n",
        "        if train_end < 30:  # Minimum training size\n",
        "            print(f\"   Warning: Insufficient training data for split {i+1}\")\n",
        "            break\n",
        "\n",
        "        train_idx = df.index[:train_end]\n",
        "        test_idx = df.index[test_start:test_end]\n",
        "\n",
        "        splits.append({\n",
        "            'train_idx': train_idx,\n",
        "            'test_idx': test_idx,\n",
        "            'train_size': len(train_idx),\n",
        "            'test_size': len(test_idx),\n",
        "            'split_date': test_idx[0] if len(test_idx) > 0 else None\n",
        "        })\n",
        "\n",
        "        print(f\"Split {i+1}:\")\n",
        "        print(f\"   Training: {len(train_idx)} days ({train_idx[0].strftime('%Y-%m-%d')} to {train_idx[-1].strftime('%Y-%m-%d')})\")\n",
        "        print(f\"   Testing:  {len(test_idx)} days ({test_idx[0].strftime('%Y-%m-%d')} to {test_idx[-1].strftime('%Y-%m-%d')})\")\n",
        "\n",
        "    print(f\"\\nBase run cross-validation setup complete: {len(splits)} split(s)\")\n",
        "    print(\"-\" * 45)\n",
        "    return splits"
      ],
      "metadata": {
        "id": "KDdvjppW9TWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_basic_models_base_run(y_train, forecast_steps):\n",
        "    \"\"\"\n",
        "    CONFIGURABLE: Fit selected basic statistical models\n",
        "    Uses configuration flags to determine which models to run\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"FITTING CONFIGURABLE BASIC STATISTICAL MODELS\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. MEAN\n",
        "    if ENABLE_MEAN:\n",
        "        mean_val = y_train.mean()\n",
        "        results[f\"mean_{MODEL_VERSION}\"] = np.full(forecast_steps, mean_val)\n",
        "        print(f\"     MEAN: {mean_val:.1f} calls/day\")\n",
        "\n",
        "    # 2. MEDIAN\n",
        "    if ENABLE_MEDIAN:\n",
        "        median_val = y_train.median()\n",
        "        results[f\"median_{MODEL_VERSION}\"] = np.full(forecast_steps, median_val)\n",
        "        print(f\"     MEDIAN: {median_val:.1f} calls/day\")\n",
        "\n",
        "    # 3. MODE\n",
        "    if ENABLE_MODE:\n",
        "        hist, bin_edges = np.histogram(y_train, bins=20)\n",
        "        mode_bin = np.argmax(hist)\n",
        "        mode_val = (bin_edges[mode_bin] + bin_edges[mode_bin + 1]) / 2\n",
        "        results[f\"mode_{MODEL_VERSION}\"] = np.full(forecast_steps, mode_val)\n",
        "        print(f\"     MODE: {mode_val:.1f} calls/day\")\n",
        "\n",
        "    # 4. NAIVE (Yesterday's Value)\n",
        "    if ENABLE_NAIVE:\n",
        "        naive_val = y_train.iloc[-1]\n",
        "        results[f\"naive_{MODEL_VERSION}\"] = np.full(forecast_steps, naive_val)\n",
        "        print(f\"     NAIVE: {naive_val:.1f} calls/day (yesterday's value)\")\n",
        "\n",
        "    # 5. MOVING AVERAGE (7-day)\n",
        "    if ENABLE_MOVING_AVG_7:\n",
        "        ma_val = y_train.iloc[-7:].mean() if len(y_train) >= 7 else y_train.mean()\n",
        "        results[f\"moving_avg_7_{MODEL_VERSION}\"] = np.full(forecast_steps, ma_val)\n",
        "        print(f\"     MOVING AVG (7d): {ma_val:.1f} calls/day\")\n",
        "\n",
        "    # 6. DAY-OF-WEEK AVERAGE\n",
        "    if ENABLE_DAY_OF_WEEK_AVG:\n",
        "        try:\n",
        "            # Calculate average for each day of week (0=Monday, 6=Sunday)\n",
        "            dow_averages = y_train.groupby(y_train.index.dayofweek).mean()\n",
        "\n",
        "            # Create forecast based on day of week pattern\n",
        "            dow_forecasts = []\n",
        "            start_dow = y_train.index[-1].dayofweek  # Last training day's day-of-week\n",
        "\n",
        "            for step in range(forecast_steps):\n",
        "                forecast_dow = (start_dow + 1 + step) % 7  # Next day's day-of-week\n",
        "                if forecast_dow in dow_averages.index:\n",
        "                    dow_forecasts.append(dow_averages[forecast_dow])\n",
        "                else:\n",
        "                    dow_forecasts.append(y_train.mean())  # Fallback\n",
        "\n",
        "            results[f\"day_of_week_avg_{MODEL_VERSION}\"] = np.array(dow_forecasts)\n",
        "            print(f\"     DAY-OF-WEEK AVG: Using historical daily patterns\")\n",
        "        except:\n",
        "            print(f\"     DAY-OF-WEEK AVG: Failed - using mean fallback\")\n",
        "            results[f\"day_of_week_avg_{MODEL_VERSION}\"] = np.full(forecast_steps, y_train.mean())\n",
        "\n",
        "    # 7. LINEAR TREND\n",
        "    if ENABLE_LINEAR_TREND:\n",
        "        try:\n",
        "            from sklearn.linear_model import LinearRegression\n",
        "            X = np.arange(len(y_train)).reshape(-1, 1)\n",
        "            lr = LinearRegression().fit(X, y_train.values)\n",
        "            future_X = np.arange(len(y_train), len(y_train) + forecast_steps).reshape(-1, 1)\n",
        "            trend_forecasts = lr.predict(future_X)\n",
        "            results[f\"linear_trend_{MODEL_VERSION}\"] = trend_forecasts\n",
        "            slope = lr.coef_[0]\n",
        "            print(f\"     LINEAR TREND: Slope = {slope:.2f} calls/day per day\")\n",
        "        except:\n",
        "            print(f\"     LINEAR TREND: Failed - using mean fallback\")\n",
        "            results[f\"linear_trend_{MODEL_VERSION}\"] = np.full(forecast_steps, y_train.mean())\n",
        "\n",
        "\n",
        "    print(f\"\\n   Generated {forecast_steps} forecasts for {len(results)} selected models\")\n",
        "    print(f\"   Models fitted: {', '.join([name.split('_')[0].replace('moving', 'MA').replace('day', 'DOW').title() for name in results.keys()])}\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "doQovh3z4p4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aIcy3-S_1dSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XXWk5QwI1d0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_standardized_base_run(y_true, y_pred, y_train, model_name):\n",
        "    \"\"\"\n",
        "    Standardized model evaluation with all metrics - BASE RUN VERSION\n",
        "    Same evaluation framework as complex models for fair comparison\n",
        "    \"\"\"\n",
        "\n",
        "    # Handle NaNs and infinite values\n",
        "    mask = ~(np.isnan(y_true) | np.isnan(y_pred) | np.isinf(y_true) | np.isinf(y_pred))\n",
        "    y_true_clean = y_true[mask]\n",
        "    y_pred_clean = y_pred[mask]\n",
        "\n",
        "    if len(y_true_clean) == 0:\n",
        "        print(f\"   Warning: {model_name} - No valid predictions after cleaning\")\n",
        "        return {\n",
        "            'Model': model_name,\n",
        "            'MAE': np.nan,\n",
        "            'RMSE': np.nan,\n",
        "            'MAPE': np.nan,\n",
        "            'R2': np.nan,\n",
        "            'MASE': np.nan\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        # 1. Mean Absolute Error\n",
        "        mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
        "\n",
        "        # 2. Root Mean Square Error\n",
        "        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
        "\n",
        "        # 3. Mean Absolute Percentage Error\n",
        "        mape = mean_absolute_percentage_error(y_true_clean, y_pred_clean) * 100\n",
        "\n",
        "        # 4. R-squared Score (coefficient of determination)\n",
        "        ss_res = np.sum((y_true_clean - y_pred_clean) ** 2)\n",
        "        ss_tot = np.sum((y_true_clean - np.mean(y_true_clean)) ** 2)\n",
        "        r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
        "\n",
        "        # 5. Mean Absolute Scaled Error (MASE) - using naive as benchmark\n",
        "        if len(y_train) > 1:\n",
        "            # Calculate naive errors (benchmark)\n",
        "            naive_errors = np.abs(y_train.iloc[1:].values - y_train.iloc[:-1].values)\n",
        "            naive_mae = np.mean(naive_errors)\n",
        "        else:\n",
        "            naive_mae = np.mean(np.abs(y_train.values - y_train.mean()))\n",
        "\n",
        "        mase = mae / naive_mae if naive_mae != 0 else np.inf\n",
        "\n",
        "        return {\n",
        "            'Model': model_name,\n",
        "            'MAE': round(mae, 2),\n",
        "            'RMSE': round(rmse, 2),\n",
        "            'MAPE': round(mape, 2),\n",
        "            'R2': round(r2, 4),\n",
        "            'MASE': round(mase, 4)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Warning: {model_name} evaluation failed: {e}\")\n",
        "        return {\n",
        "            'Model': model_name,\n",
        "            'MAE': np.nan,\n",
        "            'RMSE': np.nan,\n",
        "            'MAPE': np.nan,\n",
        "            'R2': np.nan,\n",
        "            'MASE': np.nan\n",
        "        }"
      ],
      "metadata": {
        "id": "RVb_Le3Cn_9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8Dxqnzx7mrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_base_run_evaluation():\n",
        "    \"\"\"\n",
        "    Run complete BASE RUN evaluation - Only basic benchmarks\n",
        "    Focus on Mean, Median, Mode, Naive with full reporting\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"BASE RUN: BASIC BENCHMARK EVALUATION\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Purpose: Establish fundamental baseline performance\")\n",
        "    print(\"Models: Mean, Median, Mode, Naive\")\n",
        "    print(\"Cross-Validation: Single split for computational efficiency\")\n",
        "    print(\"Optimization: DISABLED (using default parameters only)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    all_results = []\n",
        "    evaluation_start_time = time.time()\n",
        "\n",
        "    # Use only 1 split for base run (computational efficiency)\n",
        "    splits_to_use = 1\n",
        "\n",
        "    for split_idx, split in enumerate(cv_splits[:splits_to_use]):\n",
        "        print(f\"\\nEVALUATING SPLIT {split_idx + 1}/{splits_to_use}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Get data for this split\n",
        "        train_data = df_raw.loc[split['train_idx']]\n",
        "        test_data = df_raw.loc[split['test_idx']]\n",
        "\n",
        "        print(f\"Training Period: {train_data.index[0].strftime('%Y-%m-%d')} to {train_data.index[-1].strftime('%Y-%m-%d')}\")\n",
        "        print(f\"Testing Period:  {test_data.index[0].strftime('%Y-%m-%d')} to {test_data.index[-1].strftime('%Y-%m-%d')}\")\n",
        "        print(f\"Training Days: {len(train_data)} | Testing Days: {len(test_data)}\")\n",
        "\n",
        "        # Extract target variables\n",
        "        y_train = train_data['calls']\n",
        "        y_test = test_data['calls'].values\n",
        "        forecast_steps = len(test_data)\n",
        "\n",
        "        # Display basic statistics\n",
        "        print(f\"\\nTraining Data Statistics:\")\n",
        "        print(f\"   Mean: {y_train.mean():.1f} calls/day\")\n",
        "        print(f\"   Median: {y_train.median():.1f} calls/day\")\n",
        "        print(f\"   Std Dev: {y_train.std():.1f} calls/day\")\n",
        "        print(f\"   Range: {y_train.min():.0f} - {y_train.max():.0f} calls/day\")\n",
        "\n",
        "        # Fit ONLY basic statistical models\n",
        "        print(f\"\\nFitting Basic Models for {forecast_steps}-step forecast...\")\n",
        "        basic_results = fit_basic_models_base_run(y_train, forecast_steps)\n",
        "\n",
        "        # Evaluate each model\n",
        "        print(f\"\\nEvaluating Model Performance:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        for model_name, predictions in basic_results.items():\n",
        "            if len(predictions) == len(y_test):\n",
        "                # Calculate metrics\n",
        "                metrics = evaluate_model_standardized_base_run(y_test, predictions, y_train, model_name)\n",
        "\n",
        "                # Add split information\n",
        "                metrics['split'] = split_idx + 1\n",
        "                metrics['category'] = 'Basic Benchmark'\n",
        "\n",
        "                # Store results\n",
        "                all_results.append(metrics)\n",
        "\n",
        "                # Display individual results\n",
        "                clean_name = model_name.replace(f'_{MODEL_VERSION}', '').replace('_', ' ').title()\n",
        "                print(f\"   {clean_name:<20} MAE: {metrics['MAE']:<8.1f} MASE: {metrics['MASE']:<8.3f}\")\n",
        "            else:\n",
        "                print(f\"   {model_name}: X Length mismatch ({len(predictions)} vs {len(y_test)})\")  # FIXED: removed unicode\n",
        "\n",
        "        print(f\"\\nSplit {split_idx + 1} completed: {len([r for r in all_results if r['split'] == split_idx + 1])} models evaluated\")\n",
        "\n",
        "    # Compile final results\n",
        "    evaluation_time = time.time() - evaluation_start_time\n",
        "\n",
        "    if not all_results:\n",
        "        print(\"ERROR: No valid results generated!\")\n",
        "        return None, None\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "\n",
        "    # Calculate average performance across splits\n",
        "    avg_results = results_df.groupby('Model').agg({\n",
        "        'MAE': 'mean',\n",
        "        'RMSE': 'mean',\n",
        "        'MAPE': 'mean',\n",
        "        'R2': 'mean',\n",
        "        'MASE': 'mean'\n",
        "    }).round(4)\n",
        "\n",
        "    # Sort by MASE (lower is better)\n",
        "    avg_results = avg_results.sort_values('MASE')\n",
        "\n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(\"BASE RUN EVALUATION COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Evaluation Time: {evaluation_time:.1f} seconds\")\n",
        "    print(f\"Models Evaluated: {len(avg_results)}\")\n",
        "    print(f\"Best Performing Model: {avg_results.index[0]}\")\n",
        "    print(f\"Best MASE Score: {avg_results.iloc[0]['MASE']:.4f}\")\n",
        "    print(f\"Models Beating Naive: {(avg_results['MASE'] < 1.0).sum()}/{len(avg_results)}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return results_df, avg_results"
      ],
      "metadata": {
        "id": "ePdl05u37mzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwj1ADLj7wsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_base_run_report(results_df, avg_results, notebook_name=\"BASE RUN - BASIC BENCHMARKS\"):\n",
        "    \"\"\"\n",
        "    Create standardized performance report for base run\n",
        "    Same format as complex model evaluations for consistency\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"{notebook_name} PERFORMANCE REPORT\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if len(avg_results) == 0:\n",
        "        print(\"ERROR: No results to report!\")\n",
        "        return None\n",
        "\n",
        "    # Get champion model\n",
        "    best_model = avg_results.iloc[0]\n",
        "\n",
        "    print(f\"Champion Model: {best_model.name.replace(f'_{MODEL_VERSION}', '').replace('_', ' ').title()}\")\n",
        "    print(f\"Report Generated: {timestamp}\")\n",
        "    print(f\"Evaluation Mode: {notebook_name}\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    # Performance Rankings Table\n",
        "    print(\"MODEL PERFORMANCE RANKINGS:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Rank':<6} {'Model':<25} {'MAE':<10} {'RMSE':<10} {'MAPE':<10} {'R²':<10} {'MASE':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for rank, (model_name, row) in enumerate(avg_results.iterrows(), 1):\n",
        "        # Clean model name for display\n",
        "        display_name = model_name.replace(f'_{MODEL_VERSION}', '').replace('_', ' ').title()[:23]\n",
        "\n",
        "        print(f\"{rank:<6} {display_name:<25} {row['MAE']:<10.1f} {row['RMSE']:<10.1f} \"\n",
        "              f\"{row['MAPE']:<10.2f} {row['R2']:<10.4f} {row['MASE']:<10.4f}\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Performance Analysis\n",
        "    print(\"\\nPERFORMANCE ANALYSIS:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Best model details\n",
        "    best_display_name = best_model.name.replace(f'_{MODEL_VERSION}', '').replace('_', ' ').title()\n",
        "    print(f\"CHAMPION: {best_display_name}\")\n",
        "    print(f\"   * Mean Absolute Error: {best_model['MAE']:.1f} calls/day\")\n",
        "    print(f\"   * Root Mean Square Error: {best_model['RMSE']:.1f} calls/day\")\n",
        "    print(f\"   * Mean Absolute Percentage Error: {best_model['MAPE']:.2f}%\")\n",
        "    print(f\"   * R-squared Score: {best_model['R2']:.4f}\")\n",
        "    print(f\"   * MASE (vs Naive): {best_model['MASE']:.4f}\")\n",
        "\n",
        "    # Benchmark Analysis\n",
        "    models_beating_benchmark = (avg_results['MASE'] < 1.0).sum()\n",
        "    total_models = len(avg_results)\n",
        "    success_rate = (models_beating_benchmark / total_models) * 100\n",
        "\n",
        "    print(f\"\\nBENCHMARK ANALYSIS:\")\n",
        "    print(f\"   * Models beating Naive: {models_beating_benchmark}/{total_models}\")\n",
        "    print(f\"   * Success Rate: {success_rate:.1f}%\")\n",
        "\n",
        "    if models_beating_benchmark == 0:\n",
        "        print(\"   WARNING: No models beat the naive baseline\")\n",
        "        print(\"      This suggests either strong seasonality or data challenges\")\n",
        "    elif success_rate == 100:\n",
        "        print(\"   SUCCESS: All models beat the naive baseline\")\n",
        "        print(\"      This indicates good model performance across all approaches\")\n",
        "    else:\n",
        "        print(f\"   MIXED: Mixed performance with {models_beating_benchmark} successful models\")\n",
        "\n",
        "    # Model-specific insights\n",
        "    print(f\"\\nMODEL INSIGHTS:\")\n",
        "\n",
        "    model_insights = {\n",
        "        'mean': 'Constant forecast using historical average',\n",
        "        'median': 'Robust to outliers, uses middle value',\n",
        "        'mode': 'Most frequent value, good for stable patterns',\n",
        "        'naive': 'Yesterday\\'s value, simple baseline',\n",
        "        'moving_avg_7': 'Average of last 7 days, smoothed recent trend',\n",
        "        'day_of_week_avg': 'Historical day-of-week patterns',\n",
        "        'seasonal_naive': 'Forecast using value from same day last week (key seasonal benchmark)'\n",
        "    }\n",
        "\n",
        "    for model_name, row in avg_results.iterrows():\n",
        "        clean_name = model_name.replace(f'_{MODEL_VERSION}', '')\n",
        "        if clean_name in model_insights:\n",
        "            performance = \"Strong\" if row['MASE'] < 1.0 else \"Moderate\" if row['MASE'] < 1.5 else \"Weak\"\n",
        "            print(f\"   * {clean_name.title()}: {performance} performance (MASE: {row['MASE']:.3f})\")\n",
        "            print(f\"     {model_insights[clean_name]}\")\n",
        "\n",
        "    # Summary recommendations\n",
        "    print(f\"\\nKEY TAKEAWAYS:\")\n",
        "\n",
        "    if avg_results.iloc[0]['MASE'] < 1.0:\n",
        "        print(f\"   SUCCESS: The {best_display_name} model shows promise for this dataset\")\n",
        "        print(f\"   RECOMMENDATION: Consider this as the baseline for more complex models\")\n",
        "    else:\n",
        "        print(f\"   CHALLENGE: Even the best model ({best_display_name}) struggles vs naive\")\n",
        "        print(f\"   RECOMMENDATION: Strong weekly patterns may dominate - consider time series models\")\n",
        "\n",
        "    print(f\"   * Average MASE across all models: {avg_results['MASE'].mean():.3f}\")\n",
        "    print(f\"   * Best possible improvement vs naive: {(1 - avg_results.iloc[0]['MASE']) * 100:.1f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BASE RUN ANALYSIS COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return avg_results"
      ],
      "metadata": {
        "id": "vpTxlv7Z8Fof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wxrLGr98FwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_base_run_visualizations(results_df, avg_results, title_prefix=\"BASE RUN\"):\n",
        "    \"\"\"\n",
        "    Create standardized visualizations for base run results\n",
        "    Same format as complex model visualizations for consistency\n",
        "    \"\"\"\n",
        "\n",
        "    # Set consistent visualization style\n",
        "    plt.style.use('seaborn-v0_8') if 'seaborn-v0_8' in plt.style.available else plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "\n",
        "    # Create figure with 2x2 subplot layout\n",
        "    fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Clean model names for display\n",
        "    display_results = avg_results.copy()\n",
        "    display_results.index = [name.replace(f'_{MODEL_VERSION}', '').replace('_', ' ').title()\n",
        "                           for name in display_results.index]\n",
        "\n",
        "    # 1. MASE Performance Ranking (Primary Metric)\n",
        "    ax1 = plt.subplot(2, 2, 1)\n",
        "    mase_data = display_results.sort_values('MASE')\n",
        "    colors = ['green' if x < 1.0 else 'orange' if x < 1.5 else 'red' for x in mase_data['MASE']]\n",
        "\n",
        "    bars = mase_data['MASE'].plot(kind='barh', ax=ax1, color=colors, alpha=0.7)\n",
        "    ax1.axvline(x=1.0, color='black', linestyle='--', linewidth=2,\n",
        "                label='Naive Benchmark', alpha=0.8)\n",
        "    ax1.set_title(f'{title_prefix}: MASE Performance\\n(Lower = Better Performance)',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "    ax1.set_xlabel('Mean Absolute Scaled Error (MASE)', fontsize=12)\n",
        "    ax1.legend(loc='lower right')\n",
        "    ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, (idx, val) in enumerate(mase_data['MASE'].items()):\n",
        "        ax1.text(val + 0.01, i, f'{val:.3f}', va='center', fontsize=10)\n",
        "\n",
        "    # 2. Multi-Metric Comparison\n",
        "    ax2 = plt.subplot(2, 2, 2)\n",
        "\n",
        "    # Normalize metrics for fair comparison (0-1 scale)\n",
        "    metrics_norm = pd.DataFrame()\n",
        "    metrics_norm['MASE'] = mase_data['MASE'] / mase_data['MASE'].max()\n",
        "    metrics_norm['MAPE'] = mase_data['MAPE'] / mase_data['MAPE'].max()\n",
        "    metrics_norm['MAE'] = mase_data['MAE'] / mase_data['MAE'].max()\n",
        "\n",
        "    metrics_norm.plot(kind='bar', ax=ax2, width=0.8, alpha=0.7)\n",
        "    ax2.set_title(f'{title_prefix}: Multi-Metric Comparison\\n(Normalized, Lower = Better)',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "    ax2.set_ylabel('Normalized Error Score', fontsize=12)\n",
        "    ax2.set_xlabel('Models', fontsize=12)\n",
        "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
        "    ax2.legend(loc='upper right')\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # 3. R² Score Performance\n",
        "    ax3 = plt.subplot(2, 2, 3)\n",
        "    r2_data = display_results.sort_values('R2', ascending=False)\n",
        "    colors_r2 = ['green' if x > 0.5 else 'orange' if x > 0.0 else 'red' for x in r2_data['R2']]\n",
        "\n",
        "    r2_data['R2'].plot(kind='barh', ax=ax3, color=colors_r2, alpha=0.7)\n",
        "    ax3.set_title(f'{title_prefix}: R² Score\\n(Higher = Better Fit)',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "    ax3.set_xlabel('R² Score (Coefficient of Determination)', fontsize=12)\n",
        "    ax3.set_xlim([-0.1, max(1.0, r2_data['R2'].max() + 0.1)])\n",
        "    ax3.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, (idx, val) in enumerate(r2_data['R2'].items()):\n",
        "        ax3.text(val + 0.02, i, f'{val:.3f}', va='center', fontsize=10)\n",
        "\n",
        "    # 4. Error Distribution Summary\n",
        "    ax4 = plt.subplot(2, 2, 4)\n",
        "\n",
        "    # Create error summary statistics\n",
        "    error_summary = pd.DataFrame({\n",
        "        'MAE': mase_data['MAE'],\n",
        "        'RMSE': mase_data['RMSE']\n",
        "    })\n",
        "\n",
        "    error_summary.plot(kind='bar', ax=ax4, alpha=0.7, width=0.8)\n",
        "    ax4.set_title(f'{title_prefix}: Error Magnitude Comparison\\n(Lower = Better)',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "    ax4.set_ylabel('Error (Calls per Day)', fontsize=12)\n",
        "    ax4.set_xlabel('Models', fontsize=12)\n",
        "    ax4.set_xticklabels(ax4.get_xticklabels(), rotation=45, ha='right')\n",
        "    ax4.legend()\n",
        "    ax4.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Overall figure title and layout\n",
        "    plt.suptitle(f'{title_prefix} - Performance Analysis Dashboard',\n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "    plt.tight_layout(rect=[0, 0.02, 1, 0.95])\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nVISUALIZATION SUMMARY:\")  # FIXED: removed double backslash\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    best_model_display = display_results.index[0]\n",
        "    best_mase = display_results.iloc[0]['MASE']\n",
        "    models_below_1 = (display_results['MASE'] < 1.0).sum()\n",
        "    total_models = len(display_results)\n",
        "    avg_mase = display_results['MASE'].mean()\n",
        "\n",
        "    print(f\"Best Performing Model: {best_model_display}\")\n",
        "    print(f\"Best MASE Score: {best_mase:.4f}\")\n",
        "    print(f\"Models Beating Benchmark: {models_below_1}/{total_models}\")\n",
        "    print(f\"Average MASE: {avg_mase:.4f}\")\n",
        "    print(f\"Performance Spread: {display_results['MASE'].min():.3f} - {display_results['MASE'].max():.3f}\")\n",
        "\n",
        "    if models_below_1 > 0:\n",
        "        print(f\"Status: Success - {models_below_1} model(s) beat naive\")\n",
        "    else:\n",
        "        print(\"Status: Challenge - No models beat naive baseline\")\n",
        "        print(\"Recommendation: Consider seasonal patterns or data preprocessing\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "CBlWTlVJ76be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tr3m9T6F76lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL EXECUTION - BASE RUN EVALUATION\n",
        "print(\"INITIATING BASE RUN EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Loading data and executing basic benchmark evaluation...\")\n",
        "\n",
        "# Start timing\n",
        "execution_start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Step 1: Load data\n",
        "    print(\"\\\\nStep 1: Loading call center data...\")\n",
        "    df_raw = load_call_center_data_base_run()\n",
        "\n",
        "    if df_raw is None:\n",
        "        raise Exception(\"Data loading failed - check file path and format\")\n",
        "\n",
        "    # Step 2: Create cross-validation splits\n",
        "    print(\"\\\\nStep 2: Creating time series cross-validation splits...\")\n",
        "    cv_splits = create_time_series_splits_base_run(df_raw, n_splits=1, test_size=7)\n",
        "\n",
        "    if not cv_splits:\n",
        "        raise Exception(\"Cross-validation split creation failed\")\n",
        "\n",
        "    # Step 3: Run base run evaluation\n",
        "    print(\"\\\\nStep 3: Running base run evaluation...\")\n",
        "    print(\"This will evaluate only the 4 core baseline models...\")\n",
        "\n",
        "    results_df, avg_results = run_base_run_evaluation()\n",
        "\n",
        "    if results_df is None or avg_results is None:\n",
        "        raise Exception(\"Base run evaluation failed\")\n",
        "\n",
        "    # Step 4: Generate standardized report\n",
        "    print(\"\\\\nStep 4: Generating performance report...\")\n",
        "    final_report = create_base_run_report(results_df, avg_results, \"BASE RUN - BASIC BENCHMARKS\")\n",
        "\n",
        "    # Step 5: Create visualizations\n",
        "    print(\"\\\\nStep 5: Creating performance visualizations...\")\n",
        "    visualization_fig = create_base_run_visualizations(results_df, avg_results, \"BASE RUN\")\n",
        "\n",
        "    # Step 6: Save results\n",
        "    print(\"\\\\nStep 6: Saving results to files...\")\n",
        "\n",
        "    # Save detailed results\n",
        "    results_filename = 'base_run_detailed_results.csv'\n",
        "    results_df.to_csv(results_filename, index=False)\n",
        "    print(f\"   Detailed results saved: {results_filename}\")\n",
        "\n",
        "    # Save summary report\n",
        "    summary_filename = 'base_run_summary_report.csv'\n",
        "    final_report.to_csv(summary_filename)\n",
        "    print(f\"   Summary report saved: {summary_filename}\")\n",
        "\n",
        "    # Calculate execution time\n",
        "    execution_time = time.time() - execution_start_time\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\\\n\" + \"=\" * 80)\n",
        "    print(\"BASE RUN EVALUATION COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    best_model_name = avg_results.index[0].replace(f'_{MODEL_VERSION}', '').replace('_', ' ').title()\n",
        "    best_mase = avg_results.iloc[0]['MASE']\n",
        "    models_beating_benchmark = (avg_results['MASE'] < 1.0).sum()\n",
        "    total_models = len(avg_results)\n",
        "\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "    print(f\"Models Evaluated: {total_models}\")\n",
        "    print(f\"Champion Model: {best_model_name}\")\n",
        "    print(f\"Champion MASE: {best_mase:.4f}\")\n",
        "    print(f\"Success Rate: {models_beating_benchmark}/{total_models} models beat naive\")\n",
        "    print(f\"Files Generated: {results_filename}, {summary_filename}\")\n",
        "\n",
        "    print(\"\\\\nNEXT STEPS:\")\n",
        "    print(\"1. Review the performance report above\")\n",
        "    print(\"2. Examine the visualization dashboard\")\n",
        "    print(\"3. Check saved CSV files for detailed metrics\")\n",
        "    print(\"4. Use best performing model as baseline for complex models\")\n",
        "    print(\"5. Consider seasonal patterns if no models beat benchmark\")\n",
        "\n",
        "    print(\"\\\\nBASE RUN CHARACTERISTICS:\")\n",
        "    print(\"• No hyperparameter optimization\")\n",
        "    print(\"• No complex feature engineering\")\n",
        "    print(\"• Single cross-validation split\")\n",
        "    print(\"• Default model parameters only\")\n",
        "    print(\"• Focus on interpretability over complexity\")\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "except Exception as e:\n",
        "    execution_time = time.time() - execution_start_time\n",
        "    print(f\"\\\\n*** BASE RUN EXECUTION FAILED ***\")\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    print(f\"Execution time before failure: {execution_time:.2f} seconds\")\n",
        "    print(\"\\\\nTroubleshooting:\")\n",
        "    print(\"1. Check if data file exists and is accessible\")\n",
        "    print(\"2. Verify data format matches expected structure\")\n",
        "    print(\"3. Ensure all required libraries are installed\")\n",
        "    print(\"4. Review error message above for specific issues\")\n",
        "    print(\"\\\\nContact support or check documentation for assistance\")\n",
        "\n",
        "    # Still try to show what we have\n",
        "    if 'df_raw' in locals() and df_raw is not None:\n",
        "        print(f\"\\\\nData was loaded successfully: {len(df_raw)} records\")\n",
        "    if 'cv_splits' in locals() and cv_splits:\n",
        "        print(f\"CV splits were created: {len(cv_splits)} splits\")\n",
        "\n",
        "print(\"\\\\nBase run execution completed.\")"
      ],
      "metadata": {
        "id": "By5pt48ToAJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g0vp5eQQoAMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on your actual results, here's the corrected analysis:\n",
        "\n",
        "## What This Tells You About Your Business\n",
        "\n",
        "**Call Patterns:** Your call center demonstrates strong day-of-week patterns, but the situation is more nuanced than extreme seasonality. The Day-of-Week Average model's clear victory (MASE: 0.700) confirms that Monday patterns differ from Tuesday patterns, but the fact that simple averages also performed well (MASE: 0.95-0.97) indicates your overall call volume is relatively stable.\n",
        "\n",
        "**Forecasting Needs:** Day-of-week awareness provides significant value - the winning model achieved 30% better performance than naive forecasting. However, the strong performance of simple methods (83% success rate) suggests your call patterns are quite predictable and manageable with basic approaches.\n",
        "\n",
        "**Resource Planning:** Your staffing strategy should definitely incorporate day-specific patterns, as evidenced by the Day-of-Week Average model outperforming all others. However, the relatively small gap between methods suggests that even simplified approaches can provide useful guidance.\n",
        "\n",
        "## Base Run Value\n",
        "\n",
        "Rather than proving simple approaches are insufficient, your base run demonstrates that **basic statistical methods work surprisingly well** for your call center:\n",
        "\n",
        "- **Confirming manageable complexity:** 5 out of 6 models beat the naive baseline\n",
        "- **Establishing strong baseline performance:** Best model achieves 14.3% MAPE, which is quite acceptable\n",
        "- **Quantifying day-of-week value:** Day patterns provide 30% improvement over naive forecasting\n",
        "- **Demonstrating forecast stability:** Even simple mean/median approaches achieve ~25% improvement\n",
        "\n",
        "## Key Business Insights\n",
        "\n",
        "Your base run reveals a **forecasting-friendly environment** where:\n",
        "- Day-of-week patterns matter but aren't overwhelming\n",
        "- Simple statistical approaches already provide decent accuracy\n",
        "- The winning approach (Day-of-Week Average) is easy to implement and interpret\n",
        "- You have a solid foundation that more complex models will need to meaningfully beat\n",
        "\n",
        "This suggests your call center has well-established patterns without extreme volatility - a favorable situation for operational planning."
      ],
      "metadata": {
        "id": "fB19cKzKpteP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "piOuMGOrmQhP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}