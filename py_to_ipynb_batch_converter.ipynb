{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/py_to_ipynb_batch_converter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9kKeiVpE9Er"
      },
      "source": [
        "# ðŸ§° Batch-convert Python \"notebooks\" (.py) â†’ real Jupyter notebooks (.ipynb)\n",
        "\n",
        "This notebook scans a directory for `.py` files, **splits them into digestible cells using your house-style section headings**, and writes a proper `.ipynb` next to each source file. Optionally, it can also write back a tidy `.py` with `# %%` cell markers.\n",
        "\n",
        "**How to use**  \n",
        "1. Edit the config cell below (set `SOURCE_DIR`).  \n",
        "2. Run the cells top-to-bottom.  \n",
        "3. Open your new notebooks (`.ipynb`) in the same folder.\n"
      ],
      "id": "z9kKeiVpE9Er"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9ezvglVE9Ev"
      },
      "execution_count": 1,
      "outputs": [],
      "source": [
        "\n",
        "# === Configuration ===\n",
        "SOURCE_DIR = \"/content\"   # change me\n",
        "INCLUDE_GLOB = \"*.py\"      # e.g., \"**/*.py\" for recursive\n",
        "MAX_LINES = 160\n",
        "WRITE_BACK_PY = True\n",
        "WRITE_IPYNB = True\n",
        "MARKDOWN_HEADINGS = True\n",
        "\n",
        "# House-style section regex\n",
        "SECTION_RE = r\"^\\s*#\\s*(?:##+\\s+(.*)|###\\s*(?:Step|Phase)\\s*:\\s*(.*)|[-]{3,}\\s+(.*?)\\s*-*|[=]{3,}\\s+(.*?)\\s*=*|(?:SECTION|STEP|PHASE)\\s*:\\s*(.*))\\s*$\"\n"
      ],
      "id": "z9ezvglVE9Ev"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E166Xnm8GNBn"
      },
      "id": "E166Xnm8GNBn",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH9TVkmjE9Ew"
      },
      "execution_count": 2,
      "outputs": [],
      "source": [
        "import os, re, glob, json, shutil\n",
        "from typing import List, Tuple\n",
        "\n",
        "CHUNK_HEADER_RE = re.compile(r\"^\\s*#\\s*%%\")\n",
        "JUPYTER_IN_RE = re.compile(r\"^\\s*#\\s*In\\[\\d*\\]:\")\n",
        "TOP_LEVEL_DEF_RE = re.compile(r\"^(def |class |@)\", re.ASCII)\n",
        "DOUBLE_BLANK_RE = re.compile(r\"\\n\\s*\\n\\s*\\n\")\n",
        "\n",
        "def split_into_cells(lines: List[str]) -> List[Tuple[str, List[str]]]:\n",
        "    indices = []\n",
        "    for i, line in enumerate(lines):\n",
        "        if CHUNK_HEADER_RE.match(line) or JUPYTER_IN_RE.match(line):\n",
        "            indices.append(i)\n",
        "    if not indices:\n",
        "        return [(\"NO_MARKER\", lines[:])]\n",
        "    cells = []\n",
        "    for idx, start in enumerate(indices):\n",
        "        end = indices[idx + 1] if idx + 1 < len(indices) else len(lines)\n",
        "        header = lines[start].rstrip(\"\\n\")\n",
        "        body = lines[start + 1:end]\n",
        "        cells.append((header, body))\n",
        "    return cells\n",
        "\n",
        "def _extract_title(m: re.Match) -> str:\n",
        "    gd = m.groupdict() if hasattr(m, \"groupdict\") else {}\n",
        "    title = (gd.get(\"title\") or \"\").strip() if gd else \"\"\n",
        "    if title:\n",
        "        return title\n",
        "    for g in m.groups():\n",
        "        if g and str(g).strip():\n",
        "            return str(g).strip()\n",
        "    return \"\"\n",
        "\n",
        "def find_section_indices(text: str, section_re: re.Pattern):\n",
        "    sections = []\n",
        "    for i, ln in enumerate(text.splitlines()):\n",
        "        m = section_re.match(ln)\n",
        "        if m:\n",
        "            title = _extract_title(m)\n",
        "            sections.append((i, title))\n",
        "    return sections\n",
        "\n",
        "def secondary_splits(text: str, max_lines: int):\n",
        "    lines = text.splitlines()\n",
        "    if len(lines) <= max_lines or not text.strip():\n",
        "        return [len(lines)]\n",
        "    candidates = set()\n",
        "    col0_indices = [i for i, ln in enumerate(lines) if ln and not ln.startswith((\" \", \"\\t\"))]\n",
        "    for i in col0_indices:\n",
        "        if TOP_LEVEL_DEF_RE.match(lines[i]):\n",
        "            candidates.add(i)\n",
        "    for m in DOUBLE_BLANK_RE.finditer(\"\\n\".join(lines)):\n",
        "        upto = m.start()\n",
        "        before = text[:upto].splitlines()\n",
        "        candidates.add(len(before))\n",
        "    anchors = sorted([0] + [i for i in candidates if 0 < i < len(lines)] + [len(lines)])\n",
        "    cuts, cur = [], 0\n",
        "    while cur < len(lines):\n",
        "        next_idx = cur + max_lines\n",
        "        valid = [a for a in anchors if cur < a <= min(len(lines), next_idx)]\n",
        "        cut = max(valid) if valid else min(len(lines), cur + max_lines)\n",
        "        cuts.append(cut)\n",
        "        cur = cut\n",
        "    final, prev = [], 0\n",
        "    for cut in cuts:\n",
        "        seg_len = cut - prev\n",
        "        if seg_len <= max_lines:\n",
        "            final.append(cut)\n",
        "        else:\n",
        "            while prev + max_lines < cut:\n",
        "                prev += max_lines\n",
        "                final.append(prev)\n",
        "            final.append(cut)\n",
        "        prev = cut\n",
        "    return final\n",
        "\n",
        "def reassemble_with_sections(py_text: str, max_lines: int, section_re: re.Pattern):\n",
        "    lines = py_text.splitlines()\n",
        "    cells = split_into_cells(lines)\n",
        "    out = []\n",
        "    for (header, body) in cells:\n",
        "        body_text = \"\".join(body)\n",
        "        blines = body_text.splitlines()\n",
        "        sects = find_section_indices(body_text, section_re)\n",
        "        if not sects:\n",
        "            seg_bounds = [(0, len(blines), None)]\n",
        "        else:\n",
        "            anchors = [0] + [i for (i, _t) in sects] + [len(blines)]\n",
        "            seg_bounds = []\n",
        "            for i in range(len(anchors)-1):\n",
        "                start, end = anchors[i], anchors[i+1]\n",
        "                title = None\n",
        "                if i > 0:\n",
        "                    title = sects[i-1][1] if sects[i-1][0] == start else None\n",
        "                if start < len(blines) and section_re.match(blines[start]):\n",
        "                    start += 1\n",
        "                seg_bounds.append((start, end, title))\n",
        "        for (s, e, title) in seg_bounds:\n",
        "            seg_text = \"\\n\".join(blines[s:e]).rstrip()\n",
        "            if not seg_text.strip():\n",
        "                if title:\n",
        "                    out.append(f\"# %% {title} [part 1/1]\\n\")\n",
        "                continue\n",
        "            cuts = secondary_splits(seg_text, max_lines)\n",
        "            prev, parts = 0, []\n",
        "            s_lines = seg_text.splitlines()\n",
        "            for c in cuts:\n",
        "                chunk = \"\\n\".join(s_lines[prev:c]).rstrip()\n",
        "                if chunk.strip():\n",
        "                    parts.append(chunk)\n",
        "                prev = c\n",
        "            total = len(parts) if parts else 1\n",
        "            if not parts:\n",
        "                label = title or \"Auto-split cell\"\n",
        "                out.append(f\"# %% {label} [part 1/1]\\n\")\n",
        "                continue\n",
        "            for i, chunk in enumerate(parts):\n",
        "                label = title or \"Auto-split cell\"\n",
        "                out.append(f\"# %% {label} [part {i+1}/{total}]\\n\")\n",
        "                out.append(chunk + (\"\\n\" if not chunk.endswith(\"\\n\") else \"\"))\n",
        "    return out\n",
        "\n",
        "def to_ipynb(chunks_text, markdown_headings: bool):\n",
        "    nb = {\n",
        "        \"cells\": [],\n",
        "        \"metadata\": {\n",
        "            \"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"},\n",
        "            \"language_info\": {\"name\": \"python\", \"pygments_lexer\": \"ipython3\"},\n",
        "        },\n",
        "        \"nbformat\": 4,\n",
        "        \"nbformat_minor\": 5,\n",
        "    }\n",
        "    header_re = re.compile(r\"^\\s*#\\s*%%\\s*(.*)$\")\n",
        "    current_title = None\n",
        "    current_code = []\n",
        "    def flush_segment():\n",
        "        nonlocal current_title, current_code\n",
        "        if current_title is not None:\n",
        "            if markdown_headings and current_title.strip():\n",
        "                nb[\"cells\"].append({\n",
        "                    \"cell_type\": \"markdown\",\n",
        "                    \"metadata\": {},\n",
        "                    \"source\": f\"## {current_title.strip()}\\n\",\n",
        "                })\n",
        "            nb[\"cells\"].append({\n",
        "                \"cell_type\": \"code\",\n",
        "                \"metadata\": {},\n",
        "                \"execution_count\": None,\n",
        "                \"outputs\": [],\n",
        "                \"source\": \"\\n\".join(current_code).rstrip() + (\"\\n\" if current_code else \"\"),\n",
        "            })\n",
        "            current_title, current_code = None, []\n",
        "    for line in \"\".join(chunks_text).splitlines():\n",
        "        m = header_re.match(line)\n",
        "        if m:\n",
        "            flush_segment()\n",
        "            title = m.group(1)\n",
        "            title = re.sub(r\"\\s*\\[part\\s+\\d+/\\d+\\]\\s*\", \"\", title).strip()\n",
        "            current_title = title\n",
        "            current_code = []\n",
        "        else:\n",
        "            current_code.append(line)\n",
        "    if current_title is not None or current_code:\n",
        "        flush_segment()\n",
        "    return nb\n",
        "\n",
        "def process_one_file(py_path: str, max_lines: int, section_re: re.Pattern, write_back: bool, make_ipynb: bool, markdown_headings: bool):\n",
        "    with open(py_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        original_text = f.read()\n",
        "    new_lines = reassemble_with_sections(original_text, max_lines=max_lines, section_re=section_re)\n",
        "\n",
        "    # Backup original .py once\n",
        "    if write_back:\n",
        "        backup_path = py_path + \".bak\"\n",
        "        if not os.path.exists(backup_path):\n",
        "            with open(backup_path, \"w\", encoding=\"utf-8\") as bf:\n",
        "                bf.write(original_text)\n",
        "        with open(py_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\".join(new_lines))\n",
        "\n",
        "    ipynb_path = None\n",
        "    if make_ipynb:\n",
        "        nb = to_ipynb(new_lines, markdown_headings)\n",
        "        ipynb_path = os.path.splitext(py_path)[0] + \".ipynb\"\n",
        "        with open(ipynb_path, \"w\", encoding=\"utf-8\") as nf:\n",
        "            json.dump(nb, nf, ensure_ascii=False, indent=2)\n",
        "    return {\n",
        "        \"source_py\": py_path,\n",
        "        \"wrote_py\": write_back,\n",
        "        \"ipynb_path\": ipynb_path,\n",
        "        \"chunks\": sum(1 for ln in new_lines if ln.startswith(\"# %%\")),\n",
        "    }"
      ],
      "id": "HH9TVkmjE9Ew"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOMTEeC2E9Ey",
        "outputId": "ae85d96d-d493-4d6b-af8b-441cc23cc1dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 .py file(s) under '/content' matching '*.py'.\n",
            "- cx_basic_model_exploration_run_1.py -> chunks=1, ipynb=yes\n",
            "- cx_basic_model_exploration_run_2.py -> chunks=1, ipynb=yes\n",
            "- cx_basic_model_exploration_run_3.py -> chunks=1, ipynb=yes\n",
            "- cx_basic_model_exploration_run_4.py -> chunks=1, ipynb=yes\n",
            "- cx_basic_model_exploration_run_5.py -> chunks=1, ipynb=yes\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === Run conversion over the directory ===\n",
        "import os, glob, re\n",
        "\n",
        "section_re = re.compile(SECTION_RE)\n",
        "paths = glob.glob(os.path.join(SOURCE_DIR, INCLUDE_GLOB), recursive=True)\n",
        "paths = [p for p in paths if p.lower().endswith(\".py\")]\n",
        "\n",
        "print(f\"Found {len(paths)} .py file(s) under {SOURCE_DIR!r} matching {INCLUDE_GLOB!r}.\")\n",
        "\n",
        "results = []\n",
        "for p in sorted(paths):\n",
        "    res = process_one_file(\n",
        "        py_path=p,\n",
        "        max_lines=MAX_LINES,\n",
        "        section_re=section_re,\n",
        "        write_back=WRITE_BACK_PY,\n",
        "        make_ipynb=WRITE_IPYNB,\n",
        "        markdown_headings=MARKDOWN_HEADINGS,\n",
        "    )\n",
        "    results.append(res)\n",
        "    print(f\"- {os.path.basename(p)} -> chunks={res['chunks']}, ipynb={'yes' if res['ipynb_path'] else 'no'}\")\n",
        "\n",
        "print(\"\\nDone.\")\n"
      ],
      "id": "iOMTEeC2E9Ey"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Download all generated .ipynb files as a zip ===\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Zip file name\n",
        "zip_path = \"/content/converted_notebooks.zip\"\n",
        "\n",
        "# Find all ipynb files in the source dir\n",
        "ipynb_files = glob.glob(os.path.join(SOURCE_DIR, \"*.ipynb\"))\n",
        "\n",
        "# Create zip archive\n",
        "shutil.make_archive(zip_path.replace(\".zip\",\"\"), 'zip', SOURCE_DIR)\n",
        "\n",
        "# Download\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "po9vAApsGQN4"
      },
      "id": "po9vAApsGQN4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjKn5JvpE9Ez"
      },
      "source": [
        "**Tip:** If you want to run only for certain files, change `INCLUDE_GLOB`.  \n",
        "To tighten your headings (e.g., only `# SECTION: ...`), narrow the `SECTION_RE`.\n"
      ],
      "id": "AjKn5JvpE9Ez"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}