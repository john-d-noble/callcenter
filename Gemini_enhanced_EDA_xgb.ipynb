{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU7tsdAbSShNCHhkSto+sw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/Gemini_enhanced_EDA_xgb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "i2z5u72-whc9",
        "outputId": "560d4d9e-36c9-49b6-8cb1-f491b8569e24"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
              "                <p>\n",
              "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
              "                </p>\n",
              "            </div>\n",
              "            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- [STEP 1/8] Starting: Data Collection ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240255686.py:66: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  raw_data = yf.download(TICKERS, start=START_DATE, end=END_DATE, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- [DATA REVIEW & VALIDATION] ---\n",
            "Date Range: 2021-01-01 to 2025-09-04\n",
            "Total Days (Rows) Collected: 1708\n",
            "\n",
            "‚ùå CRITICAL FAILURE: Missing data detected after API call and forward-fill.\n",
            "This indicates a problem with the source data. The script cannot proceed.\n",
            "btc_price     0\n",
            "btc_volume    0\n",
            "eth_price     0\n",
            "eth_volume    0\n",
            "sol_price     0\n",
            "sol_volume    0\n",
            "vix_price     3\n",
            "vix_volume    3\n",
            "dtype: int64\n",
            "--- [STEP 2/8] Starting: Call Volume Simulation ---\n",
            "‚ùå FAILED: [Errno 2] No such file or directory: 'crypto_price_and_volume_data_2021_present.csv'\n",
            "--- [STEP 3/8] Starting: Advanced Feature Engineering ---\n",
            "‚ùå FAILED: [Errno 2] No such file or directory: 'crypto_price_and_volume_data_2021_present.csv'\n",
            "--- [STEP 4/8] Starting: Exploratory Data Analysis ---\n",
            "‚ùå FAILED: [Errno 2] No such file or directory: 'advanced_feature_training_data_2021_present.csv'\n",
            "--- [STEP 5/8] Starting: Automated Model Tuning (this may take several minutes) ---\n",
            "‚ùå FAILED: [Errno 2] No such file or directory: 'advanced_feature_training_data_2021_present.csv'\n",
            "--- [STEP 6/8] Starting: Final Model Training ---\n",
            "‚ùå FAILED: name 'best_params' is not defined\n",
            "--- [STEP 7/8] Starting: Saving the Final Model ---\n",
            "‚ùå FAILED: name 'final_model' is not defined\n",
            "--- [STEP 8/8] Starting: Making a Live Forecast ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240255686.py:262: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  raw_forecast_data = yf.download(TICKERS, start=forecast_start_date, end=forecast_end_date, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå FAILED: name 'X' is not defined\n"
          ]
        }
      ],
      "source": [
        "# course. I've ap B plied all the expert corrections to your script.\n",
        "#his updated version incorporates fixes for data leakage, improves API robustness, significantly boosts performance by removing the slow .apply() call, and ensures reproducibility.\n",
        "# you requested, I've enhanced the hard stop after the data collection step. It now performs an explicit validation check‚Äîif any data is missing after the API pull, it will print a failure message and exit. If the data is complete, it will confirm success and then pause for your manual review before proceeding.\n",
        "## Master Script: Final Version with All Corrections (v5.0)\n",
        "#ere is the complete, production-ready script with all improvements integrated.\n",
        "# =============================================================================\n",
        "# STEP 0: INSTALL REQUIRED LIBRARIES\n",
        "# =============================================================================\n",
        "!pip install yfinance -q\n",
        "!pip install xgboost -q\n",
        "!pip install ydata-profiling -q\n",
        "\n",
        "# =============================================================================\n",
        "# MASTER SCRIPT: From Data Collection to Live Forecasting (Version 5.0 - Production Ready)\n",
        "# This script incorporates robustness checks, data leakage prevention, and performance enhancements.\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
        "from datetime import date, timedelta\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# Set a random seed for reproducible results in simulation and modeling\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- Integrated EDA Function ---\n",
        "def run_comprehensive_eda(df, target_variable=None):\n",
        "    \"\"\"Performs a comprehensive EDA on a pandas DataFrame and generates a report.\"\"\"\n",
        "    print(\"üöÄ Starting Comprehensive Exploratory Data Analysis...\")\n",
        "    profile = ProfileReport(df, title=\"Comprehensive EDA Report\", explorative=True)\n",
        "    profile.to_file(\"training_data_eda_report.html\")\n",
        "    print(\"\\n‚úÖ Success! Detailed report saved as 'training_data_eda_report.html'\")\n",
        "\n",
        "# --- MASTER CONFIGURATION ---\n",
        "# Data Collection & Simulation\n",
        "TICKERS = ['BTC-USD', 'ETH-USD', 'SOL-USD', '^VIX']\n",
        "START_DATE = '2021-01-01'\n",
        "FORECAST_LOOKBACK_DAYS = 45 # Safely larger than the longest feature window (30)\n",
        "\n",
        "# Feature Engineering\n",
        "VOLATILITY_WINDOW = 14\n",
        "SPIKE_WINDOW = 30\n",
        "SPIKE_THRESHOLD = 2.0\n",
        "\n",
        "# Model Tuning\n",
        "TUNING_ITERATIONS = 50\n",
        "\n",
        "# Final Output Files\n",
        "MARKET_DATA_FILE = 'crypto_price_and_volume_data_2021_present.csv'\n",
        "SIMULATED_CALLS_FILE = 'final_simulated_data_with_volatility_2021_present.csv'\n",
        "ADVANCED_TRAINING_FILE = 'advanced_feature_training_data_2021_present.csv'\n",
        "FINAL_MODEL_FILENAME = 'final_advanced_xgboost_model_2021_present.json'\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 1: DATA COLLECTION - Fetch all required price and volume data\n",
        "# ==============================================================================\n",
        "print(\"--- [STEP 1/8] Starting: Data Collection ---\")\n",
        "try:\n",
        "    END_DATE = date.today().strftime('%Y-%m-%d')\n",
        "    raw_data = yf.download(TICKERS, start=START_DATE, end=END_DATE, progress=False)\n",
        "\n",
        "    full_date_range = pd.date_range(start=START_DATE, end=END_DATE, freq='D')\n",
        "    market_data_df = pd.DataFrame(index=full_date_range)\n",
        "\n",
        "    TICKER_MAP = {'BTC-USD': 'btc', 'ETH-USD': 'eth', 'SOL-USD': 'sol', '^VIX': 'vix'}\n",
        "\n",
        "    for ticker, name in TICKER_MAP.items():\n",
        "        price_series = raw_data[('Close', ticker)]\n",
        "        volume_series = raw_data[('Volume', ticker)]\n",
        "        asset_df = pd.DataFrame({f'{name}_price': price_series, f'{name}_volume': volume_series})\n",
        "        # CORRECTION: Use only ffill() to prevent using future data to fill past NaNs\n",
        "        asset_df = asset_df.reindex(full_date_range).ffill()\n",
        "        market_data_df = market_data_df.join(asset_df)\n",
        "\n",
        "    market_data_df.index.name = 'Date'\n",
        "\n",
        "    # --- ENHANCED HARD STOP & VALIDATION ---\n",
        "    print(\"\\n--- [DATA REVIEW & VALIDATION] ---\")\n",
        "    print(f\"Date Range: {market_data_df.index.min().strftime('%Y-%m-%d')} to {market_data_df.index.max().strftime('%Y-%m-%d')}\")\n",
        "    print(f\"Total Days (Rows) Collected: {market_data_df.shape[0]}\")\n",
        "    # CORRECTION: Explicitly check for any nulls after the API call.\n",
        "    if market_data_df.isnull().sum().sum() > 0:\n",
        "        print(\"\\n‚ùå CRITICAL FAILURE: Missing data detected after API call and forward-fill.\")\n",
        "        print(\"This indicates a problem with the source data. The script cannot proceed.\")\n",
        "        print(market_data_df.isnull().sum())\n",
        "        exit()\n",
        "    else:\n",
        "        print(\"\\n‚úÖ Data Integrity Check Passed: No missing values found.\")\n",
        "        market_data_df.to_csv(MARKET_DATA_FILE)\n",
        "        print(f\"‚úÖ Success! Market data saved to '{MARKET_DATA_FILE}'\")\n",
        "        input(\"\\n--- PAUSED --- \\nData is complete. Press Enter to proceed to Step 2 or Ctrl+C to exit.\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå FAILED: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 2: SIMULATION - Create the intelligent, volatility-driven call volume\n",
        "# ==============================================================================\n",
        "print(f\"--- [STEP 2/8] Starting: Call Volume Simulation ---\")\n",
        "try:\n",
        "    df_sim = pd.read_csv(MARKET_DATA_FILE, parse_dates=['Date'])\n",
        "\n",
        "    NUMBER_OF_AGENTS = 3000\n",
        "    AVG_WEEKDAY_CALLS_PER_AGENT = 110\n",
        "    AVG_WEEKEND_CALLS_PER_AGENT = 50\n",
        "    RANDOMNESS_FACTOR = 15000\n",
        "\n",
        "    def simulate_base_volume(d):\n",
        "        mean_vol = NUMBER_OF_AGENTS * (AVG_WEEKDAY_CALLS_PER_AGENT if d.dayofweek < 5 else AVG_WEEKEND_CALLS_PER_AGENT)\n",
        "        return abs(int(np.random.normal(loc=mean_vol, scale=RANDOMNESS_FACTOR)))\n",
        "    df_sim['base_call_volume'] = df_sim['Date'].apply(simulate_base_volume)\n",
        "\n",
        "    VOLATILITY_THRESHOLD = 0.05\n",
        "    VOLATILITY_MULTIPLIER = 1.8\n",
        "    VIX_FEAR_THRESHOLD = 30\n",
        "    VIX_MULTIPLIER = 1.4\n",
        "\n",
        "    for crypto in ['btc', 'eth', 'sol']:\n",
        "        df_sim[f'{crypto}_price_pct_change'] = df_sim[f'{crypto}_price'].pct_change()\n",
        "    df_sim.fillna(0, inplace=True)\n",
        "\n",
        "    # CORRECTION: Replaced slow .apply() with fast, vectorized operations\n",
        "    final_volume = df_sim['base_call_volume'].copy()\n",
        "    is_crypto_volatile = (df_sim['btc_price_pct_change'].abs() > VOLATILITY_THRESHOLD) | \\\n",
        "                         (df_sim['eth_price_pct_change'].abs() > VOLATILITY_THRESHOLD) | \\\n",
        "                         (df_sim['sol_price_pct_change'].abs() > VOLATILITY_THRESHOLD)\n",
        "    is_vix_high = df_sim['vix_price'] > VIX_FEAR_THRESHOLD\n",
        "    final_volume[is_crypto_volatile] *= VOLATILITY_MULTIPLIER\n",
        "    final_volume[is_vix_high] *= VIX_MULTIPLIER\n",
        "    df_sim['adjusted_call_volume'] = final_volume.astype(int)\n",
        "\n",
        "    simulated_df = df_sim[['Date', 'btc_price', 'eth_price', 'sol_price', 'vix_price', 'adjusted_call_volume']]\n",
        "    simulated_df.to_csv(SIMULATED_CALLS_FILE, index=False)\n",
        "    print(f\"‚úÖ Success! Simulated call volume data saved to '{SIMULATED_CALLS_FILE}'\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå FAILED: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: ADVANCED FEATURE ENGINEERING - Create the final training data\n",
        "# ==============================================================================\n",
        "print(f\"--- [STEP 3/8] Starting: Advanced Feature Engineering ---\")\n",
        "try:\n",
        "    df_market = pd.read_csv(MARKET_DATA_FILE, parse_dates=['Date'])\n",
        "    df_calls = pd.read_csv(SIMULATED_CALLS_FILE, parse_dates=['Date'])\n",
        "\n",
        "    for crypto in ['btc', 'eth', 'sol']:\n",
        "        df_market[f'{crypto}_price_pct_change'] = df_market[f'{crypto}_price'].pct_change()\n",
        "        df_market[f'{crypto}_volatility_index'] = df_market[f'{crypto}_price_pct_change'].rolling(window=VOLATILITY_WINDOW).std()\n",
        "        rolling_vol_mean = df_market[f'{crypto}_volume'].rolling(window=SPIKE_WINDOW).mean()\n",
        "        rolling_vol_std = df_market[f'{crypto}_volume'].rolling(window=SPIKE_WINDOW).std()\n",
        "        df_market[f'{crypto}_volume_spike'] = (df_market[f'{crypto}_volume'] > (rolling_vol_mean + SPIKE_THRESHOLD * rolling_vol_std)).astype(int)\n",
        "        rolling_price_mean = df_market[f'{crypto}_price_pct_change'].rolling(window=SPIKE_WINDOW).mean()\n",
        "        rolling_price_std = df_market[f'{crypto}_price_pct_change'].rolling(window=SPIKE_WINDOW).std()\n",
        "        df_market[f'{crypto}_price_shock'] = (abs(df_market[f'{crypto}_price_pct_change']) > (rolling_price_mean + SPIKE_THRESHOLD * rolling_price_std)).astype(int)\n",
        "\n",
        "    df_calls_subset = df_calls[['Date', 'adjusted_call_volume']]\n",
        "    combined_df = pd.merge(df_market, df_calls_subset, on='Date', how='inner')\n",
        "    combined_df.drop(columns=[col for col in combined_df.columns if 'pct_change' in col], inplace=True)\n",
        "    combined_df.fillna(0, inplace=True)\n",
        "\n",
        "    # CORRECTION: Lag all features by 1 day to prevent data leakage.\n",
        "    # This ensures we only use yesterday's data to predict today's volume.\n",
        "    feature_cols = [col for col in combined_df.columns if col not in ['Date', 'adjusted_call_volume']]\n",
        "    combined_df[feature_cols] = combined_df[feature_cols].shift(1)\n",
        "    combined_df.dropna(inplace=True) # Drop the first row which is now NaN\n",
        "\n",
        "    combined_df.to_csv(ADVANCED_TRAINING_FILE, index=False)\n",
        "    print(f\"‚úÖ Success! Advanced training file saved as '{ADVANCED_TRAINING_FILE}'\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå FAILED: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 4: EXPLORATORY DATA ANALYSIS - Analyze the training data\n",
        "# ==============================================================================\n",
        "print(f\"--- [STEP 4/8] Starting: Exploratory Data Analysis ---\")\n",
        "try:\n",
        "    df_eda = pd.read_csv(ADVANCED_TRAINING_FILE, parse_dates=['Date'])\n",
        "    run_comprehensive_eda(df_eda, target_variable='adjusted_call_volume')\n",
        "    input(\"\\n--- PAUSED --- \\nReview the EDA report ('training_data_eda_report.html'). Press Enter to proceed.\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå FAILED: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 5: MODEL TUNING - Find the best hyperparameters\n",
        "# ==============================================================================\n",
        "print(f\"--- [STEP 5/8] Starting: Automated Model Tuning (this may take several minutes) ---\")\n",
        "try:\n",
        "    df_tune = pd.read_csv(ADVANCED_TRAINING_FILE, parse_dates=['Date'])\n",
        "    df_tune.set_index('Date', inplace=True)\n",
        "    df_tune.fillna(0, inplace=True)\n",
        "    X = df_tune.drop('adjusted_call_volume', axis=1)\n",
        "    y = df_tune['adjusted_call_volume']\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 300, 500, 700], 'max_depth': [3, 4, 5, 6, 7],\n",
        "        'learning_rate': [0.01, 0.05, 0.1], 'subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0]\n",
        "    }\n",
        "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='mae')\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, n_iter=TUNING_ITERATIONS,\n",
        "                                   scoring='neg_mean_absolute_error', cv=tscv, n_jobs=-1, verbose=1, random_state=42)\n",
        "    random_search.fit(X, y)\n",
        "\n",
        "    best_params = random_search.best_params_\n",
        "    best_score = -random_search.best_score_\n",
        "\n",
        "    print(\"\\n--- Tuning Complete ---\")\n",
        "    print(f\"‚úÖ Best MAE Score found: {best_score:.2f}\")\n",
        "    print(f\"‚úÖ Best Hyperparameters found: {best_params}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå FAILED: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 6: FINAL MODEL TRAINING - Build the model with the best settings\n",
        "# ==============================================================================\n",
        "print(f\"--- [STEP 6/8] Starting: Final Model Training ---\")\n",
        "try:\n",
        "    final_model = xgb.XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        eval_metric='mae',\n",
        "        **best_params,\n",
        "        random_state=42\n",
        "    )\n",
        "    final_model.fit(X, y)\n",
        "    print(\"‚úÖ Success! Final model trained on all available data.\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå FAILED: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 7: SAVE THE MODEL - Create the final, predictive asset\n",
        "# ==============================================================================\n",
        "print(f\"--- [STEP 7/8] Starting: Saving the Final Model ---\")\n",
        "try:\n",
        "    final_model.save_model(FINAL_MODEL_FILENAME)\n",
        "    print(f\"‚úÖ Success! Final model has been saved as '{FINAL_MODEL_FILENAME}'.\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå FAILED: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 8: MAKE A PREDICTION - Use the model for a live forecast\n",
        "# ==============================================================================\n",
        "print(f\"--- [STEP 8/8] Starting: Making a Live Forecast ---\")\n",
        "try:\n",
        "    # CORRECTION: Use config variable for lookback instead of a \"magic number\"\n",
        "    forecast_start_date = date.today() - timedelta(days=FORECAST_LOOKBACK_DAYS)\n",
        "    forecast_end_date = date.today()\n",
        "    raw_forecast_data = yf.download(TICKERS, start=forecast_start_date, end=forecast_end_date, progress=False)\n",
        "\n",
        "    forecast_full_range = pd.date_range(start=forecast_start_date, end=forecast_end_date, freq='D')\n",
        "    market_df = pd.DataFrame(index=forecast_full_range)\n",
        "\n",
        "    for ticker, name in TICKER_MAP.items():\n",
        "        price_series = raw_forecast_data[('Close', ticker)]\n",
        "        volume_series = raw_forecast_data[('Volume', ticker)]\n",
        "        asset_df = pd.DataFrame({f'{name}_price': price_series, f'{name}_volume': volume_series})\n",
        "        asset_df = asset_df.reindex(forecast_full_range).ffill() # Use ffill only\n",
        "        market_df = market_df.join(asset_df)\n",
        "\n",
        "    market_df.index.name = 'Date'\n",
        "\n",
        "    # Re-create features for the prediction data\n",
        "    for crypto in ['btc', 'eth', 'sol']:\n",
        "        market_df[f'{crypto}_price_pct_change'] = market_df[f'{crypto}_price'].pct_change()\n",
        "        market_df[f'{crypto}_volatility_index'] = market_df[f'{crypto}_price_pct_change'].rolling(window=VOLATILITY_WINDOW).std()\n",
        "        rolling_vol_mean = market_df[f'{crypto}_volume'].rolling(window=SPIKE_WINDOW).mean()\n",
        "        rolling_vol_std = market_df[f'{crypto}_volume'].rolling(window=SPIKE_WINDOW).std()\n",
        "        market_df[f'{crypto}_volume_spike'] = (market_df[f'{crypto}_volume'] > (rolling_vol_mean + SPIKE_THRESHOLD * rolling_vol_std)).astype(int)\n",
        "        rolling_price_mean = market_df[f'{crypto}_price_pct_change'].rolling(window=SPIKE_WINDOW).mean()\n",
        "        rolling_price_std = market_df[f'{crypto}_price_pct_change'].rolling(window=SPIKE_WINDOW).std()\n",
        "        market_df[f'{crypto}_price_shock'] = (abs(market_df[f'{crypto}_price_pct_change']) > (rolling_price_mean + SPIKE_THRESHOLD * rolling_price_std)).astype(int)\n",
        "\n",
        "    # Note: We do NOT lag the prediction input, as we are predicting for the *next* day.\n",
        "    # The final row of features is complete based on today's closing data.\n",
        "    prediction_input = market_df.tail(1)\n",
        "    training_columns = [col for col in X.columns]\n",
        "    prediction_input = prediction_input[training_columns]\n",
        "\n",
        "    loaded_model = xgb.XGBRegressor()\n",
        "    loaded_model.load_model(FINAL_MODEL_FILENAME)\n",
        "\n",
        "    prediction = loaded_model.predict(prediction_input)\n",
        "    predicted_volume = int(prediction[0])\n",
        "    latest_day = prediction_input.index[0]\n",
        "\n",
        "    print(\"\\n--- Forecast Complete ---\")\n",
        "    print(f\"‚úÖ Based on market data from {latest_day.strftime('%Y-%m-%d')}, the model forecasts a call volume for the next day of: {predicted_volume:,}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå FAILED: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8tKeT4xgE-sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIFkk_Ueyye6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dpMJ8olYFCsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgWprVtWFF9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7n6nPPFqFJ43"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}