{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUlt5nFibwACrC8hFYcL3B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/CB_Step_2_Baseline_Models_(Simple_Benchmarks).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Load the updated dataset\n",
        "df = pd.read_csv('updated_final_merged_data.csv', index_col='date', parse_dates=True)\n",
        "\n",
        "# Assume 'Calls' is the target column\n",
        "target = 'calls'\n",
        "\n",
        "# Prepare data: Sort by date if not already\n",
        "df = df.sort_index()\n",
        "\n",
        "# Define forecast horizon (e.g., 7 days for weekly)\n",
        "horizon = 7\n",
        "\n",
        "# Time series cross-validation: 5 splits\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Function to calculate metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100  # As percentage\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
        "\n",
        "# Dictionary to store average metrics for each model\n",
        "model_metrics = {}\n",
        "\n",
        "# 1. Naive Forecast (Last observed value)\n",
        "naive_preds = []\n",
        "naive_trues = []\n",
        "for train_idx, test_idx in tscv.split(df):\n",
        "    train = df.iloc[train_idx]\n",
        "    test = df.iloc[test_idx]\n",
        "\n",
        "    # Predict last train value for all test points\n",
        "    last_value = train[target].iloc[-1]\n",
        "    pred = np.full(len(test), last_value)\n",
        "\n",
        "    naive_preds.extend(pred)\n",
        "    naive_trues.extend(test[target])\n",
        "\n",
        "naive_metrics = calculate_metrics(naive_trues, naive_preds)\n",
        "model_metrics['Naive'] = naive_metrics\n",
        "\n",
        "# 2. Mean Forecast (Overall mean)\n",
        "mean_value = df[target].mean()  # Global mean\n",
        "mean_preds = np.full(len(df), mean_value)\n",
        "mean_metrics = calculate_metrics(df[target], mean_preds)  # Evaluate on full data since it's constant\n",
        "model_metrics['Mean'] = mean_metrics\n",
        "\n",
        "# Median Forecast (Overall median)\n",
        "median_value = df[target].median()\n",
        "median_preds = np.full(len(df), median_value)\n",
        "median_metrics = calculate_metrics(df[target], median_preds)\n",
        "model_metrics['Median'] = median_metrics\n",
        "\n",
        "# 3. Seasonal Naive (Same day last week, lag=7)\n",
        "seasonal_preds = []\n",
        "seasonal_trues = []\n",
        "for train_idx, test_idx in tscv.split(df):\n",
        "    train = df.iloc[train_idx]\n",
        "    test = df.iloc[test_idx]\n",
        "\n",
        "    # For each test point, predict the value from 7 days ago (if available)\n",
        "    pred = []\n",
        "    for i in test_idx:\n",
        "        lag_idx = i - 7\n",
        "        if lag_idx >= 0:\n",
        "            pred.append(df.iloc[lag_idx][target])\n",
        "        else:\n",
        "            pred.append(train[target].mean())  # Fallback if no lag\n",
        "\n",
        "    seasonal_preds.extend(pred)\n",
        "    seasonal_trues.extend(test[target])\n",
        "\n",
        "seasonal_metrics = calculate_metrics(seasonal_trues, seasonal_preds)\n",
        "model_metrics['Seasonal Naive'] = seasonal_metrics\n",
        "\n",
        "# Summarize performance\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "metrics_df = pd.DataFrame(model_metrics).T\n",
        "print(metrics_df)\n",
        "\n",
        "# Pick winner: Lowest MAE (primary metric)\n",
        "winner = metrics_df['MAE'].idxmin()\n",
        "print(f\"\\nChampion Baseline Model: {winner}\")\n",
        "print(f\"Metrics: {metrics_df.loc[winner].to_dict()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOTRK8itfvnK",
        "outputId": "e347ad7d-24d9-4462-bcde-8efc80b1ef5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance Summary:\n",
            "                        MAE         RMSE       MAPE\n",
            "Naive           2706.061350  3385.348788  37.198945\n",
            "Mean            1968.528582  2547.250358  27.218917\n",
            "Median          1960.742331  2553.890108  26.514827\n",
            "Seasonal Naive   857.704294  1322.311704  10.292237\n",
            "\n",
            "Champion Baseline Model: Seasonal Naive\n",
            "Metrics: {'MAE': 857.7042944785276, 'RMSE': 1322.3117036908475, 'MAPE': 10.292236542928933}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Problem\n",
        "\n",
        "You have tested four different forecasting models (Naive, Mean, Median, and Seasonal Naive) and need to determine which one provides the most accurate predictions based on standard performance metrics.\n",
        "\n",
        "The Simple Solution\n",
        "\n",
        "Select the Seasonal Naive model. It is unequivocally the best-performing model of the group, delivering significantly more accurate forecasts than the other alternatives.\n",
        "\n",
        "The Action Plan\n",
        "\n",
        "    Adopt the Champion: Formally designate the Seasonal Naive model as your champion baseline for this forecasting task.\n",
        "\n",
        "    Understand the Metrics:\n",
        "\n",
        "        The model was evaluated using three key error metrics: MAE, RMSE, and MAPE. In all of these, a lower score is better.\n",
        "\n",
        "        The Seasonal Naive model scored the lowest across all three, with a Mean Absolute Percentage Error (MAPE) of just 10.3%. This means, on average, its forecast is off by about 10.3%.\n",
        "\n",
        "        In contrast, the next best models (Mean/Median) had an error rate of over 26%, and the simple Naive model was off by 37%.\n",
        "\n",
        "    Establish as a Benchmark: Use these results as the benchmark for any future, more complex models you develop. A new model is only valuable if it can consistently outperform the Seasonal Naive's scores.\n",
        "\n",
        "The Value & Risk\n",
        "\n",
        "    Value: By using the Seasonal Naive model, you are establishing a reliable and accurate baseline. This model reduces the average forecast error by approximately 65% compared to the next best alternative (Median), leading to better-informed business decisions.\n",
        "\n",
        "    Risk: While it's the champion here, a \"Seasonal Naive\" model is still a simple baseline. It assumes future patterns will repeat past seasonal cycles. It may fail to predict shifts caused by new market trends, promotions, or external factors not present in historical data."
      ],
      "metadata": {
        "id": "ayCYK3FHBPF8"
      }
    }
  ]
}