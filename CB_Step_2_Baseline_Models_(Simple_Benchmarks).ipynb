{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs7TLQ+Qxis+iKhSJhPL7C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/CB_Step_2_Baseline_Models_(Simple_Benchmarks).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Load the updated dataset\n",
        "df = pd.read_csv('updated_final_merged_data.csv', index_col='Date', parse_dates=True)\n",
        "\n",
        "# Assume 'Calls' is the target column\n",
        "target = 'Calls'\n",
        "\n",
        "# Prepare data: Sort by date if not already\n",
        "df = df.sort_index()\n",
        "\n",
        "# Define forecast horizon (e.g., 7 days for weekly)\n",
        "horizon = 7\n",
        "\n",
        "# Time series cross-validation: 5 splits\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Function to calculate metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100  # As percentage\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
        "\n",
        "# Dictionary to store average metrics for each model\n",
        "model_metrics = {}\n",
        "\n",
        "# 1. Naive Forecast (Last observed value)\n",
        "naive_preds = []\n",
        "naive_trues = []\n",
        "for train_idx, test_idx in tscv.split(df):\n",
        "    train = df.iloc[train_idx]\n",
        "    test = df.iloc[test_idx]\n",
        "\n",
        "    # Predict last train value for all test points\n",
        "    last_value = train[target].iloc[-1]\n",
        "    pred = np.full(len(test), last_value)\n",
        "\n",
        "    naive_preds.extend(pred)\n",
        "    naive_trues.extend(test[target])\n",
        "\n",
        "naive_metrics = calculate_metrics(naive_trues, naive_preds)\n",
        "model_metrics['Naive'] = naive_metrics\n",
        "\n",
        "# 2. Mean Forecast (Overall mean)\n",
        "mean_value = df[target].mean()  # Global mean\n",
        "mean_preds = np.full(len(df), mean_value)\n",
        "mean_metrics = calculate_metrics(df[target], mean_preds)  # Evaluate on full data since it's constant\n",
        "model_metrics['Mean'] = mean_metrics\n",
        "\n",
        "# Median Forecast (Overall median)\n",
        "median_value = df[target].median()\n",
        "median_preds = np.full(len(df), median_value)\n",
        "median_metrics = calculate_metrics(df[target], median_preds)\n",
        "model_metrics['Median'] = median_metrics\n",
        "\n",
        "# 3. Seasonal Naive (Same day last week, lag=7)\n",
        "seasonal_preds = []\n",
        "seasonal_trues = []\n",
        "for train_idx, test_idx in tscv.split(df):\n",
        "    train = df.iloc[train_idx]\n",
        "    test = df.iloc[test_idx]\n",
        "\n",
        "    # For each test point, predict the value from 7 days ago (if available)\n",
        "    pred = []\n",
        "    for i in test_idx:\n",
        "        lag_idx = i - 7\n",
        "        if lag_idx >= 0:\n",
        "            pred.append(df.iloc[lag_idx][target])\n",
        "        else:\n",
        "            pred.append(train[target].mean())  # Fallback if no lag\n",
        "\n",
        "    seasonal_preds.extend(pred)\n",
        "    seasonal_trues.extend(test[target])\n",
        "\n",
        "seasonal_metrics = calculate_metrics(seasonal_trues, seasonal_preds)\n",
        "model_metrics['Seasonal Naive'] = seasonal_metrics\n",
        "\n",
        "# Summarize performance\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "metrics_df = pd.DataFrame(model_metrics).T\n",
        "print(metrics_df)\n",
        "\n",
        "# Pick winner: Lowest MAE (primary metric)\n",
        "winner = metrics_df['MAE'].idxmin()\n",
        "print(f\"\\nChampion Baseline Model: {winner}\")\n",
        "print(f\"Metrics: {metrics_df.loc[winner].to_dict()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOTRK8itfvnK",
        "outputId": "8bf4bcbb-046b-4308-81dc-7df072177777"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance Summary:\n",
            "                        MAE         RMSE       MAPE\n",
            "Naive           2351.456790  2942.377655  24.836730\n",
            "Mean            1634.558511  2154.487520  18.230934\n",
            "Median          1613.907787  2177.893679  17.377252\n",
            "Seasonal Naive   907.700000  1359.046947   9.665718\n",
            "\n",
            "Champion Baseline Model: Seasonal Naive\n",
            "Metrics: {'MAE': 907.7, 'RMSE': 1359.0469468357978, 'MAPE': 9.665718451288303}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4060459694.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df = pd.read_csv('updated_final_merged_data.csv', index_col='Date', parse_dates=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary: The baseline models evaluated provide a foundational benchmark for forecasting call center volume, helping us gauge the effectiveness of more sophisticated approaches later. These simple methods—Naive (repeating the last observed value), Mean (using the overall average), Median (using the overall median), and Seasonal Naive (repeating the value from the same day last week)—were tested using time-series cross-validation on the filled dataset. Performance was measured by Mean Absolute Error (MAE, for average deviation in calls), Root Mean Squared Error (RMSE, emphasizing larger errors), and Mean Absolute Percentage Error (MAPE, for relative accuracy).\n",
        "The Naive model, which assumes persistence from the immediate prior day, performed the worst with an MAE of about 2,351 calls, RMSE of 2,942, and MAPE of 25%. This indicates that day-to-day changes in call volume are significant, making short-term repetition unreliable. The Mean and Median models improved upon this by leveraging central tendencies across the entire dataset, achieving MAEs of 1,635 and 1,614 calls respectively, with MAPEs around 18% and 17%. The slight edge of the Median over the Mean in MAE and MAPE suggests the data's distribution is somewhat skewed (as hinted in the EDA's histogram), where outliers pull the mean away from typical values.\n",
        "However, the Seasonal Naive model stands out as the clear champion, with the lowest errors across all metrics: MAE of 908 calls, RMSE of 1,359, and MAPE under 10%. This superior performance underscores the strong weekly seasonality identified in the EDA (e.g., via decomposition and day-of-week averages), where patterns repeat every 7 days due to consistent business cycles, even after filling weekends/holidays. By accounting for this, the Seasonal Naive reduces errors by over 40% compared to the next-best baseline (Median), proving that incorporating basic seasonality yields substantial gains.\n",
        "Overall, these results validate the EDA's findings on seasonality as a key driver and set a benchmark MAPE of ~10% for advanced models (e.g., SARIMA or Prophet) to beat. If they can't surpass this, it might suggest overcomplicating the forecast is unnecessary for this dataset. This positions us well to iterate toward a more accurate champion in subsequent modeling tiers."
      ],
      "metadata": {
        "id": "jsDWEIQVgMQk"
      }
    }
  ]
}