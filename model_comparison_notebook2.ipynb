{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/model_comparison_notebook2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fca9141"
      },
      "source": [
        "# Task\n",
        "Update the script to include ARIMA and XGBoost models, train them on the \"final_merged_data.csv\" dataset, evaluate their performance using RMSE and MAE, and rerun the entire analysis to compare all models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bb399fc"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install `statsmodels` for ARIMA (already imported but good to ensure) and `xgboost` for XGBoost.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cadee8c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the `xgboost` library. This requires using the `pip install` command in a separate cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c19854f6"
      },
      "source": [
        "## Import necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Add import statements for ARIMA and XGBoost models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8101ed0"
      },
      "source": [
        "**Reasoning**:\n",
        "Add the necessary import statements for the ARIMA and XGBoost models as requested in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7abfa029"
      },
      "source": [
        "## Data preparation for arima\n",
        "\n",
        "### Subtask:\n",
        "Ensure the data is in the correct format for ARIMA, handling any potential issues with the time series index or missing values if not already addressed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e690a3ac"
      },
      "source": [
        "**Reasoning**:\n",
        "Check the index and missing values of the dataframe for ARIMA modeling readiness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa4e13d0"
      },
      "source": [
        "## Build, train, and predict with arima\n",
        "\n",
        "### Subtask:\n",
        "Implement the ARIMA model, train it on the training data, and generate predictions on the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7066887d"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the ARIMA model as per the instructions, including instantiation, fitting, and prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb185382"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the trained ARIMA model using RMSE and MAE and store the results in the evaluation dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18b1f090"
      },
      "source": [
        "## Data preparation for xgboost\n",
        "\n",
        "### Subtask:\n",
        "Prepare the data in a suitable format for XGBoost, which typically requires a supervised learning format with features and a target variable. This might involve creating lagged features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d28d7ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Define target and features for XGBoost and create lagged features for the target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29797e27"
      },
      "source": [
        "## Build, train, and predict with xgboost\n",
        "\n",
        "### Subtask:\n",
        "Implement the XGBoost model, train it on the prepared training data, and generate predictions on the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c22c7371"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the XGBoost model, train it on the prepared training data, and generate predictions on the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dadd72fa"
      },
      "source": [
        "## Update evaluation results\n",
        "\n",
        "### Subtask:\n",
        "Add the evaluation metrics for ARIMA and XGBoost to the `evaluation_results` dictionary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02d4fb0c"
      },
      "source": [
        "**Reasoning**:\n",
        "Add the evaluation metrics for ARIMA and XGBoost to the evaluation_results dictionary as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42200fe"
      },
      "source": [
        "## Rerun the entire analysis\n",
        "\n",
        "### Subtask:\n",
        "Execute the updated code cell to run the entire analysis with the new models included.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ab37d32"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to execute the entire updated code cell to run the analysis including the new models. I will use the `code_block` command to run the complete script.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ba7b3d2"
      },
      "source": [
        "## Present updated evaluation and comparison\n",
        "\n",
        "### Subtask:\n",
        "Display the updated evaluation table including the results for ARIMA and XGBoost, and update the model comparison to reflect the new results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18501db3"
      },
      "source": [
        "**Reasoning**:\n",
        "Display the updated evaluation table and provide the model comparison based on the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "244650c4"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The analysis successfully integrated and evaluated ARIMA and XGBoost models alongside existing models (Holt-Winters, SARIMAX, LSTM, GRU, BLSTM, CNN, CNN-LSTM).\n",
        "*   Data preparation for ARIMA involved ensuring a DatetimeIndex and handling missing values (none were found).\n",
        "*   Data preparation for XGBoost included creating lagged features for the target variable (lags 1, 7, and 30) and dropping rows with resulting NaN values.\n",
        "*   All nine models were trained and used to generate predictions on the test set.\n",
        "*   The performance of all models was evaluated using Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n",
        "*   The BLSTM model achieved the lowest RMSE (0.1233) and the lowest MAE (0.0948) among all evaluated models.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The BLSTM model appears to be the most effective for this specific time series forecasting task based on RMSE and MAE. Further tuning of its hyperparameters could potentially yield even better performance.\n",
        "*   Investigate the reasons for the performance differences between models, particularly the neural network models which generally outperformed traditional time series methods like ARIMA and Holt-Winters in this analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efb2f500",
        "outputId": "72f6c923-2de1-4cb0-86f8-ee84bdcc7d65"
      },
      "source": [
        "%pip install yfinance xgboost"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.65)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.4.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (6.32.0)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.28.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7932e257",
        "outputId": "f8c1e6b1-22bb-4567-a5c4-002cd69e2d76"
      },
      "source": [
        "%pip install statsmodels"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.0.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.16.1)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "19be8b64",
        "outputId": "8c01d884-3fd4-44e3-cdfd-e8fb02dc3236"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "try:\n",
        "    df_check = pd.read_csv('/content/final_merged_data.csv')\n",
        "    print(\"File loaded successfully.\")\n",
        "\n",
        "    # Display basic info\n",
        "    print(\"\\n--- File Info ---\")\n",
        "    df_check.info()\n",
        "\n",
        "    # Check for missing values\n",
        "    print(\"\\n--- Missing Values ---\")\n",
        "    print(df_check.isnull().sum())\n",
        "\n",
        "    # Display the first few rows\n",
        "    print(\"\\n--- First 5 Rows ---\")\n",
        "    display(df_check.head())\n",
        "\n",
        "    # Check the format of the date column ('Unnamed: 0')\n",
        "    print(\"\\n--- Date Column Check ---\")\n",
        "    # Attempt to convert to datetime to see if there are parsing errors\n",
        "    try:\n",
        "        pd.to_datetime(df_check['Unnamed: 0'])\n",
        "        print(\"Date column 'Unnamed: 0' can be converted to datetime without errors.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting 'Unnamed: 0' to datetime: {e}\")\n",
        "        print(\"There might be inconsistencies in the date format.\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: final_merged_data.csv not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while checking the file: {e}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded successfully.\n",
            "\n",
            "--- File Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 673 entries, 0 to 672\n",
            "Data columns (total 42 columns):\n",
            " #   Column                                 Non-Null Count  Dtype  \n",
            "---  ------                                 --------------  -----  \n",
            " 0   Unnamed: 0                             673 non-null    object \n",
            " 1   V Cx Contact Volume Template Contacts  673 non-null    int64  \n",
            " 2   ^VIX_Close_^VIX                        673 non-null    float64\n",
            " 3   ^VIX_High_^VIX                         673 non-null    float64\n",
            " 4   ^VIX_Low_^VIX                          673 non-null    float64\n",
            " 5   ^VIX_Open_^VIX                         673 non-null    float64\n",
            " 6   ^VIX_Volume_^VIX                       673 non-null    float64\n",
            " 7   BVOL-USD_Close_BVOL-USD                673 non-null    float64\n",
            " 8   BVOL-USD_High_BVOL-USD                 673 non-null    float64\n",
            " 9   BVOL-USD_Low_BVOL-USD                  673 non-null    float64\n",
            " 10  BVOL-USD_Open_BVOL-USD                 673 non-null    float64\n",
            " 11  BVOL-USD_Volume_BVOL-USD               673 non-null    float64\n",
            " 12  CVOL-USD_Close_CVOL-USD                673 non-null    float64\n",
            " 13  CVOL-USD_High_CVOL-USD                 673 non-null    float64\n",
            " 14  CVOL-USD_Low_CVOL-USD                  673 non-null    float64\n",
            " 15  CVOL-USD_Open_CVOL-USD                 673 non-null    float64\n",
            " 16  CVOL-USD_Volume_CVOL-USD               673 non-null    float64\n",
            " 17  CVX-USD_Close_CVX-USD                  673 non-null    float64\n",
            " 18  CVX-USD_High_CVX-USD                   673 non-null    float64\n",
            " 19  CVX-USD_Low_CVX-USD                    673 non-null    float64\n",
            " 20  CVX-USD_Open_CVX-USD                   673 non-null    float64\n",
            " 21  CVX-USD_Volume_CVX-USD                 673 non-null    int64  \n",
            " 22  SPY_Close_SPY                          673 non-null    float64\n",
            " 23  SPY_High_SPY                           673 non-null    float64\n",
            " 24  SPY_Low_SPY                            673 non-null    float64\n",
            " 25  SPY_Open_SPY                           673 non-null    float64\n",
            " 26  SPY_Volume_SPY                         673 non-null    float64\n",
            " 27  QQQ_Close_QQQ                          673 non-null    float64\n",
            " 28  QQQ_High_QQQ                           673 non-null    float64\n",
            " 29  QQQ_Low_QQQ                            673 non-null    float64\n",
            " 30  QQQ_Open_QQQ                           673 non-null    float64\n",
            " 31  QQQ_Volume_QQQ                         673 non-null    float64\n",
            " 32  DX-Y.NYB_Close_DX-Y.NYB                673 non-null    float64\n",
            " 33  DX-Y.NYB_High_DX-Y.NYB                 673 non-null    float64\n",
            " 34  DX-Y.NYB_Low_DX-Y.NYB                  673 non-null    float64\n",
            " 35  DX-Y.NYB_Open_DX-Y.NYB                 673 non-null    float64\n",
            " 36  DX-Y.NYB_Volume_DX-Y.NYB               673 non-null    float64\n",
            " 37  GC=F_Close_GC=F                        673 non-null    float64\n",
            " 38  GC=F_High_GC=F                         673 non-null    float64\n",
            " 39  GC=F_Low_GC=F                          673 non-null    float64\n",
            " 40  GC=F_Open_GC=F                         673 non-null    float64\n",
            " 41  GC=F_Volume_GC=F                       673 non-null    float64\n",
            "dtypes: float64(39), int64(2), object(1)\n",
            "memory usage: 221.0+ KB\n",
            "\n",
            "--- Missing Values ---\n",
            "Unnamed: 0                               0\n",
            "V Cx Contact Volume Template Contacts    0\n",
            "^VIX_Close_^VIX                          0\n",
            "^VIX_High_^VIX                           0\n",
            "^VIX_Low_^VIX                            0\n",
            "^VIX_Open_^VIX                           0\n",
            "^VIX_Volume_^VIX                         0\n",
            "BVOL-USD_Close_BVOL-USD                  0\n",
            "BVOL-USD_High_BVOL-USD                   0\n",
            "BVOL-USD_Low_BVOL-USD                    0\n",
            "BVOL-USD_Open_BVOL-USD                   0\n",
            "BVOL-USD_Volume_BVOL-USD                 0\n",
            "CVOL-USD_Close_CVOL-USD                  0\n",
            "CVOL-USD_High_CVOL-USD                   0\n",
            "CVOL-USD_Low_CVOL-USD                    0\n",
            "CVOL-USD_Open_CVOL-USD                   0\n",
            "CVOL-USD_Volume_CVOL-USD                 0\n",
            "CVX-USD_Close_CVX-USD                    0\n",
            "CVX-USD_High_CVX-USD                     0\n",
            "CVX-USD_Low_CVX-USD                      0\n",
            "CVX-USD_Open_CVX-USD                     0\n",
            "CVX-USD_Volume_CVX-USD                   0\n",
            "SPY_Close_SPY                            0\n",
            "SPY_High_SPY                             0\n",
            "SPY_Low_SPY                              0\n",
            "SPY_Open_SPY                             0\n",
            "SPY_Volume_SPY                           0\n",
            "QQQ_Close_QQQ                            0\n",
            "QQQ_High_QQQ                             0\n",
            "QQQ_Low_QQQ                              0\n",
            "QQQ_Open_QQQ                             0\n",
            "QQQ_Volume_QQQ                           0\n",
            "DX-Y.NYB_Close_DX-Y.NYB                  0\n",
            "DX-Y.NYB_High_DX-Y.NYB                   0\n",
            "DX-Y.NYB_Low_DX-Y.NYB                    0\n",
            "DX-Y.NYB_Open_DX-Y.NYB                   0\n",
            "DX-Y.NYB_Volume_DX-Y.NYB                 0\n",
            "GC=F_Close_GC=F                          0\n",
            "GC=F_High_GC=F                           0\n",
            "GC=F_Low_GC=F                            0\n",
            "GC=F_Open_GC=F                           0\n",
            "GC=F_Volume_GC=F                         0\n",
            "dtype: int64\n",
            "\n",
            "--- First 5 Rows ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Unnamed: 0  V Cx Contact Volume Template Contacts  ^VIX_Close_^VIX  \\\n",
              "0  2023-01-03                                   6537        22.900000   \n",
              "1  2023-01-04                                   7238        22.010000   \n",
              "2  2023-01-05                                   7302        22.459999   \n",
              "3  2023-01-06                                   6857        21.129999   \n",
              "4  2023-01-09                                   7166        21.969999   \n",
              "\n",
              "   ^VIX_High_^VIX  ^VIX_Low_^VIX  ^VIX_Open_^VIX  ^VIX_Volume_^VIX  \\\n",
              "0           23.76      22.730000       23.090000               0.0   \n",
              "1           23.27      21.940001       22.930000               0.0   \n",
              "2           22.92      21.969999       22.200001               0.0   \n",
              "3           22.90      21.000000       22.690001               0.0   \n",
              "4           21.98      21.270000       21.750000               0.0   \n",
              "\n",
              "   BVOL-USD_Close_BVOL-USD  BVOL-USD_High_BVOL-USD  BVOL-USD_Low_BVOL-USD  \\\n",
              "0                64.984352               64.993263              64.981926   \n",
              "1                64.984573               64.991470              64.981842   \n",
              "2                64.980278               64.993797              64.980118   \n",
              "3                64.982956               64.989983              64.975716   \n",
              "4                64.999962               65.004501              64.993507   \n",
              "\n",
              "   ...  DX-Y.NYB_Close_DX-Y.NYB  DX-Y.NYB_High_DX-Y.NYB  \\\n",
              "0  ...               104.519997              104.860001   \n",
              "1  ...               104.250000              104.599998   \n",
              "2  ...               105.040001              105.269997   \n",
              "3  ...               103.879997              105.629997   \n",
              "4  ...               103.000000              103.940002   \n",
              "\n",
              "   DX-Y.NYB_Low_DX-Y.NYB  DX-Y.NYB_Open_DX-Y.NYB  DX-Y.NYB_Volume_DX-Y.NYB  \\\n",
              "0             103.470001              103.660004                       0.0   \n",
              "1             103.830002              104.580002                       0.0   \n",
              "2             103.989998              104.070000                       0.0   \n",
              "3             103.870003              105.050003                       0.0   \n",
              "4             102.940002              103.910004                       0.0   \n",
              "\n",
              "   GC=F_Close_GC=F  GC=F_High_GC=F  GC=F_Low_GC=F  GC=F_Open_GC=F  \\\n",
              "0      1839.699951     1839.699951    1836.199951     1836.199951   \n",
              "1      1852.800049     1859.099976    1845.599976     1845.599976   \n",
              "2      1834.800049     1855.199951    1834.800049     1855.199951   \n",
              "3      1864.199951     1868.199951    1835.300049     1838.400024   \n",
              "4      1872.699951     1880.000000    1867.000000     1867.000000   \n",
              "\n",
              "   GC=F_Volume_GC=F  \n",
              "0              29.0  \n",
              "1              25.0  \n",
              "2              24.0  \n",
              "3              26.0  \n",
              "4              62.0  \n",
              "\n",
              "[5 rows x 42 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed29d09e-2e77-43ed-98b5-fedf0200510c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>V Cx Contact Volume Template Contacts</th>\n",
              "      <th>^VIX_Close_^VIX</th>\n",
              "      <th>^VIX_High_^VIX</th>\n",
              "      <th>^VIX_Low_^VIX</th>\n",
              "      <th>^VIX_Open_^VIX</th>\n",
              "      <th>^VIX_Volume_^VIX</th>\n",
              "      <th>BVOL-USD_Close_BVOL-USD</th>\n",
              "      <th>BVOL-USD_High_BVOL-USD</th>\n",
              "      <th>BVOL-USD_Low_BVOL-USD</th>\n",
              "      <th>...</th>\n",
              "      <th>DX-Y.NYB_Close_DX-Y.NYB</th>\n",
              "      <th>DX-Y.NYB_High_DX-Y.NYB</th>\n",
              "      <th>DX-Y.NYB_Low_DX-Y.NYB</th>\n",
              "      <th>DX-Y.NYB_Open_DX-Y.NYB</th>\n",
              "      <th>DX-Y.NYB_Volume_DX-Y.NYB</th>\n",
              "      <th>GC=F_Close_GC=F</th>\n",
              "      <th>GC=F_High_GC=F</th>\n",
              "      <th>GC=F_Low_GC=F</th>\n",
              "      <th>GC=F_Open_GC=F</th>\n",
              "      <th>GC=F_Volume_GC=F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>6537</td>\n",
              "      <td>22.900000</td>\n",
              "      <td>23.76</td>\n",
              "      <td>22.730000</td>\n",
              "      <td>23.090000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.984352</td>\n",
              "      <td>64.993263</td>\n",
              "      <td>64.981926</td>\n",
              "      <td>...</td>\n",
              "      <td>104.519997</td>\n",
              "      <td>104.860001</td>\n",
              "      <td>103.470001</td>\n",
              "      <td>103.660004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1839.699951</td>\n",
              "      <td>1839.699951</td>\n",
              "      <td>1836.199951</td>\n",
              "      <td>1836.199951</td>\n",
              "      <td>29.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-01-04</td>\n",
              "      <td>7238</td>\n",
              "      <td>22.010000</td>\n",
              "      <td>23.27</td>\n",
              "      <td>21.940001</td>\n",
              "      <td>22.930000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.984573</td>\n",
              "      <td>64.991470</td>\n",
              "      <td>64.981842</td>\n",
              "      <td>...</td>\n",
              "      <td>104.250000</td>\n",
              "      <td>104.599998</td>\n",
              "      <td>103.830002</td>\n",
              "      <td>104.580002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1852.800049</td>\n",
              "      <td>1859.099976</td>\n",
              "      <td>1845.599976</td>\n",
              "      <td>1845.599976</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-01-05</td>\n",
              "      <td>7302</td>\n",
              "      <td>22.459999</td>\n",
              "      <td>22.92</td>\n",
              "      <td>21.969999</td>\n",
              "      <td>22.200001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.980278</td>\n",
              "      <td>64.993797</td>\n",
              "      <td>64.980118</td>\n",
              "      <td>...</td>\n",
              "      <td>105.040001</td>\n",
              "      <td>105.269997</td>\n",
              "      <td>103.989998</td>\n",
              "      <td>104.070000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1834.800049</td>\n",
              "      <td>1855.199951</td>\n",
              "      <td>1834.800049</td>\n",
              "      <td>1855.199951</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-01-06</td>\n",
              "      <td>6857</td>\n",
              "      <td>21.129999</td>\n",
              "      <td>22.90</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>22.690001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.982956</td>\n",
              "      <td>64.989983</td>\n",
              "      <td>64.975716</td>\n",
              "      <td>...</td>\n",
              "      <td>103.879997</td>\n",
              "      <td>105.629997</td>\n",
              "      <td>103.870003</td>\n",
              "      <td>105.050003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1864.199951</td>\n",
              "      <td>1868.199951</td>\n",
              "      <td>1835.300049</td>\n",
              "      <td>1838.400024</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-01-09</td>\n",
              "      <td>7166</td>\n",
              "      <td>21.969999</td>\n",
              "      <td>21.98</td>\n",
              "      <td>21.270000</td>\n",
              "      <td>21.750000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.999962</td>\n",
              "      <td>65.004501</td>\n",
              "      <td>64.993507</td>\n",
              "      <td>...</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>103.940002</td>\n",
              "      <td>102.940002</td>\n",
              "      <td>103.910004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1872.699951</td>\n",
              "      <td>1880.000000</td>\n",
              "      <td>1867.000000</td>\n",
              "      <td>1867.000000</td>\n",
              "      <td>62.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 42 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed29d09e-2e77-43ed-98b5-fedf0200510c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed29d09e-2e77-43ed-98b5-fedf0200510c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed29d09e-2e77-43ed-98b5-fedf0200510c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b0b58ab7-57f1-42f6-8e7c-d8e0e97a2966\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b0b58ab7-57f1-42f6-8e7c-d8e0e97a2966')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b0b58ab7-57f1-42f6-8e7c-d8e0e97a2966 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Date Column Check ---\n",
            "Date column 'Unnamed: 0' can be converted to datetime without errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "21d54190",
        "outputId": "e616a030-6d3e-44ab-dc4e-8ccbc0b60d84"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, GRU, Bidirectional, Conv1D, MaxPooling1D, Flatten, TimeDistributed, RepeatVector, Reshape\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# --- Data Loading ---\n",
        "\n",
        "# Load call volume data\n",
        "try:\n",
        "    df = pd.read_csv('/content/final_merged_data.csv')\n",
        "    # Correcting the column name for the datetime and setting as index\n",
        "    df['Unnamed: 0'] = pd.to_datetime(df['Unnamed: 0'])\n",
        "    df.set_index('Unnamed: 0', inplace=True)\n",
        "    # Ensure the index is sorted\n",
        "    df.sort_index(inplace=True)\n",
        "    print(\"Call volume data loaded and indexed successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: final_merged_data.csv not found.\")\n",
        "    # Exit or handle the error appropriately if the file is essential\n",
        "    exit()\n",
        "\n",
        "# Assuming final_merged_data.csv already contains all necessary market data\n",
        "# The dynamic fetching and merging sections remain commented out as we are using the pre-merged file\n",
        "\n",
        "# Handle any potential NaNs in the loaded dataframe before splitting\n",
        "initial_nans_before_drop = df.isnull().sum().sum()\n",
        "if initial_nans_before_drop > 0:\n",
        "    df.dropna(inplace=True)\n",
        "    print(f\"Dropped rows with NaN values from the initial dataframe: {initial_nans_before_drop - df.isnull().sum().sum()} NaNs removed.\")\n",
        "else:\n",
        "    print(\"No NaN values found in the initial dataframe.\")\n",
        "\n",
        "\n",
        "# --- Data Preparation for Modeling ---\n",
        "\n",
        "target = df.columns[0] # Assuming the first column is the target\n",
        "exog_cols = [col for col in df.columns if col != target]\n",
        "\n",
        "# Define the split ratio\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Calculate the split point based on the index\n",
        "split_index = int(len(df) * split_ratio)\n",
        "train_index = df.index[:split_index]\n",
        "test_index = df.index[split_index:]\n",
        "\n",
        "# Split the data into training and testing sets using indices\n",
        "target_train = df.loc[train_index, [target]]\n",
        "target_test = df.loc[test_index, [target]]\n",
        "exog_train = df.loc[train_index, exog_cols]\n",
        "exog_test = df.loc[test_index, exog_cols]\n",
        "\n",
        "\n",
        "print(\"\\nData split into training and testing sets.\")\n",
        "print(\"Target data shapes - Train:\", target_train.shape, \"Test:\", target_test.shape)\n",
        "print(\"Exogenous data shapes - Train:\", exog_train.shape, \"Test:\", exog_test.shape)\n",
        "\n",
        "# Check for NaNs in train/test splits AFTER initial drop\n",
        "print(\"\\nChecking for NaNs in train/test splits AFTER initial drop:\")\n",
        "print(\"target_train has NaNs:\", target_train.isnull().sum().sum() > 0)\n",
        "print(\"target_test has NaNs:\", target_test.isnull().sum().sum() > 0)\n",
        "print(\"exog_train has NaNs:\", exog_train.isnull().sum().sum() > 0)\n",
        "print(\"exog_test has NaNs:\", exog_test.isnull().sum().sum() > 0)\n",
        "\n",
        "# Check for columns with zero variance in training data before scaling\n",
        "print(\"\\nChecking for zero variance columns in training data:\")\n",
        "zero_variance_cols = exog_train.columns[exog_train.var() == 0]\n",
        "if not zero_variance_cols.empty:\n",
        "    print(\"Columns with zero variance in exog_train:\", list(zero_variance_cols))\n",
        "    # Drop these columns from both exog_train and exog_test before scaling\n",
        "    exog_train_filtered = exog_train.drop(columns=zero_variance_cols)\n",
        "    exog_test_filtered = exog_test.drop(columns=zero_variance_cols)\n",
        "    print(\"Zero variance columns dropped from exog_train and exog_test.\")\n",
        "else:\n",
        "    exog_train_filtered = exog_train.copy()\n",
        "    exog_test_filtered = exog_test.copy()\n",
        "    print(\"No zero variance columns found in exog_train.\")\n",
        "\n",
        "\n",
        "# --- Model Building, Training, and Prediction ---\n",
        "\n",
        "evaluation_results = {}\n",
        "\n",
        "# 1. Holt-Winters Model\n",
        "print(\"\\nBuilding and training Holt-Winters model...\")\n",
        "try:\n",
        "    # Ensure target_train index has frequency for Holt-Winters\n",
        "    # It should have frequency if the original df had a regular frequency after dropping NaNs\n",
        "    # If not, infer it or set it if known (e.g., 'D' for daily)\n",
        "    if target_train.index.freq is None:\n",
        "        try:\n",
        "            target_train = target_train.asfreq(pd.infer_freq(target_train.index))\n",
        "            print(f\"Inferred frequency for target_train: {target_train.index.freq}\")\n",
        "        except ValueError:\n",
        "            print(\"Could not infer frequency for target_train. Setting to 'D'.\")\n",
        "            target_train = target_train.asfreq('D')\n",
        "\n",
        "\n",
        "    # Drop NaNs from target_train for Holt-Winters (should be minimal after initial drop)\n",
        "    target_train_hw = target_train.dropna()\n",
        "\n",
        "    if not target_train_hw.empty:\n",
        "        # Use the integer index for start and end to avoid issues with date matching\n",
        "        # Predict starting from the end of the training data\n",
        "        holt_winters_model = ExponentialSmoothing(target_train_hw, seasonal='add', seasonal_periods=7).fit()\n",
        "        # Predict over the length of the test set\n",
        "        holt_winters_predictions = holt_winters_model.predict(start=len(target_train_hw), end=len(target_train_hw) + len(target_test) - 1)\n",
        "        # Align predictions to the test set index\n",
        "        holt_winters_predictions.index = target_test.index\n",
        "\n",
        "\n",
        "        evaluation_results['Holt-Winters'] = {'RMSE': np.sqrt(mean_squared_error(target_test, holt_winters_predictions.dropna())),\n",
        "                                           'MAE': mean_absolute_error(target_test, holt_winters_predictions.dropna())}\n",
        "        print(\"Holt-Winters model trained and predictions made.\")\n",
        "    else:\n",
        "         print(\"Error: target_train_hw is empty after dropping NaNs for Holt-Winters.\")\n",
        "         evaluation_results['Holt-Winters'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error with Holt-Winters model: {e}\")\n",
        "    evaluation_results['Holt-Winters'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "\n",
        "# 2. SARIMAX Model\n",
        "print(\"\\nBuilding and training SARIMAX model...\")\n",
        "try:\n",
        "    # Ensure target_train and exog_train filtered index have frequency for SARIMAX\n",
        "    # Use the filtered exogenous data here\n",
        "    if target_train.index.freq is None:\n",
        "         try:\n",
        "            target_train = target_train.asfreq(pd.infer_freq(target_train.index))\n",
        "            print(f\"Inferred frequency for target_train (SARIMAX): {target_train.index.freq}\")\n",
        "         except ValueError:\n",
        "            print(\"Could not infer frequency for target_train (SARIMAX). Setting to 'D'.\")\n",
        "            target_train = target_train.asfreq('D')\n",
        "\n",
        "    if exog_train_filtered.index.freq is None:\n",
        "        try:\n",
        "            exog_train_filtered = exog_train_filtered.asfreq(pd.infer_freq(exog_train_filtered.index))\n",
        "            print(f\"Inferred frequency for exog_train_filtered (SARIMAX): {exog_train_filtered.index.freq}\")\n",
        "        except ValueError:\n",
        "            print(\"Could not infer frequency for exog_train_filtered (SARIMAX). Setting to 'D'.\")\n",
        "            exog_train_filtered = exog_train_filtered.asfreq('D')\n",
        "\n",
        "\n",
        "    # Align target and exogenous data for SARIMAX training and drop any remaining NaNs/inf\n",
        "    train_data_sarimax = pd.concat([target_train, exog_train_filtered], axis=1).dropna()\n",
        "    target_train_sarimax = train_data_sarimax[[target]]\n",
        "    exog_train_sarimax = train_data_sarimax[exog_train_filtered.columns]\n",
        "\n",
        "    # Check for NaNs or inf in train data before fitting SARIMAX\n",
        "    if exog_train_sarimax.isnull().sum().sum() > 0 or np.isinf(exog_train_sarimax).sum().sum() > 0:\n",
        "        print(\"Error: exog_train_sarimax contains NaNs or inf values for SARIMAX after alignment and drop.\")\n",
        "        evaluation_results['SARIMAX'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "    elif target_train_sarimax.isnull().sum().sum() > 0 or np.isinf(target_train_sarimax).sum().sum() > 0:\n",
        "         print(\"Error: target_train_sarimax contains NaNs or inf values for SARIMAX after alignment and drop.\")\n",
        "         evaluation_results['SARIMAX'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "    elif not target_train_sarimax.empty and not exog_train_sarimax.empty:\n",
        "        # Using a basic order (1, 1, 1) and seasonal order (1, 1, 1, 7) as a starting point\n",
        "        sarimax_model = SARIMAX(target_train_sarimax, exog=exog_train_sarimax, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))\n",
        "        sarimax_results = sarimax_model.fit(disp=False)\n",
        "\n",
        "        # Prepare test exogenous data for prediction - ensure frequency and no NaNs/inf\n",
        "        if exog_test_filtered.index.freq is None:\n",
        "             try:\n",
        "                exog_test_filtered = exog_test_filtered.asfreq(pd.infer_freq(exog_test_filtered.index))\n",
        "                print(f\"Inferred frequency for exog_test_filtered (SARIMAX): {exog_test_filtered.index.freq}\")\n",
        "             except ValueError:\n",
        "                print(\"Could not infer frequency for exog_test_filtered (SARIMAX). Setting to 'D'.\")\n",
        "                exog_test_filtered = exog_test_filtered.asfreq('D')\n",
        "\n",
        "        exog_test_sarimax = exog_test_filtered.copy()\n",
        "        # Drop NaNs from test exogenous data before prediction\n",
        "        exog_test_sarimax.dropna(inplace=True)\n",
        "\n",
        "        if exog_test_sarimax.isnull().sum().sum() > 0 or np.isinf(exog_test_sarimax).sum().sum() > 0:\n",
        "             print(\"Error: exog_test_sarimax contains NaNs or inf values for SARIMAX prediction after frequency setting and drop.\")\n",
        "             evaluation_results['SARIMAX'] = {'RMSE': np.nan, 'MAE': np.nan} # Overwrite if test data is bad\n",
        "        elif not exog_test_sarimax.empty:\n",
        "            # Predict using the test set index and the prepared exog_test data\n",
        "            # Use integer indices for start and end to avoid date matching issues\n",
        "            start_pred_index_sarimax = len(target_train_sarimax)\n",
        "            end_pred_index_sarimax = start_pred_index_sarimax + len(exog_test_sarimax) - 1 # Predict over the length of the available test exog data\n",
        "\n",
        "            # Ensure exog_test_sarimax is aligned with the prediction range\n",
        "            sarimax_predictions = sarimax_results.predict(start=start_pred_index_sarimax, end=end_pred_index_sarimax, exog=exog_test_sarimax)\n",
        "            # Align predictions to the original target_test index\n",
        "            sarimax_predictions = sarimax_predictions.reindex(target_test.index)\n",
        "\n",
        "            evaluation_results['SARIMAX'] = {'RMSE': np.sqrt(mean_squared_error(target_test, sarimax_predictions.dropna())),\n",
        "                                             'MAE': mean_absolute_error(target_test, sarimax_predictions.dropna())}\n",
        "            print(\"SARIMAX model trained and predictions made.\")\n",
        "        else:\n",
        "            print(\"Error: Test exogenous data for SARIMAX prediction is empty after dropping NaNs.\")\n",
        "            evaluation_results['SARIMAX'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "    else:\n",
        "        print(\"Error: Training data for SARIMAX is empty after dropping NaNs.\")\n",
        "        evaluation_results['SARIMAX'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error with SARIMAX model: {e}\")\n",
        "    evaluation_results['SARIMAX'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "# 3. ARIMA Model\n",
        "print(\"\\nBuilding and training ARIMA model...\")\n",
        "try:\n",
        "    # Ensure target_train index has frequency for ARIMA\n",
        "    if target_train.index.freq is None:\n",
        "         try:\n",
        "            target_train = target_train.asfreq(pd.infer_freq(target_train.index))\n",
        "            print(f\"Inferred frequency for target_train (ARIMA): {target_train.index.freq}\")\n",
        "         except ValueError:\n",
        "            print(\"Could not infer frequency for target_train (ARIMA). Setting to 'D'.\")\n",
        "            target_train = target_train.asfreq('D')\n",
        "\n",
        "    # Drop NaNs from target_train for ARIMA (should be minimal after initial drop)\n",
        "    target_train_arima = target_train.dropna()\n",
        "\n",
        "    if not target_train_arima.empty:\n",
        "        # Using a basic order (5,1,0) as a starting point\n",
        "        arima_model = ARIMA(target_train_arima, order=(5, 1, 0))\n",
        "        arima_results = arima_model.fit()\n",
        "        # Predict using integer indices relative to the training data end\n",
        "        start_pred_index_arima = len(target_train_arima)\n",
        "        end_pred_index_arima = start_pred_index_arima + len(target_test) - 1 # Predict over the length of the test set\n",
        "\n",
        "        arima_predictions = arima_results.predict(start=start_pred_index_arima, end=end_pred_index_arima)\n",
        "        # Align predictions to the test set index\n",
        "        arima_predictions.index = target_test.index\n",
        "\n",
        "        evaluation_results['ARIMA'] = {'RMSE': np.sqrt(mean_squared_error(target_test, arima_predictions.dropna())),\n",
        "                                   'MAE': mean_absolute_error(target_test, arima_predictions.dropna())}\n",
        "        print(\"ARIMA model trained and predictions made.\")\n",
        "    else:\n",
        "         print(\"Error: target_train_arima is empty after dropping NaNs for ARIMA.\")\n",
        "         evaluation_results['ARIMA'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error with ARIMA model: {e}\")\n",
        "    evaluation_results['ARIMA'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "\n",
        "# Prepare data for Neural Network Models (LSTM, GRU, BLSTM, CNN, CNN-LSTM)\n",
        "print(\"\\nPreparing data for Neural Network models...\")\n",
        "target_scaler = MinMaxScaler()\n",
        "exog_scaler = MinMaxScaler()\n",
        "\n",
        "# Use the filtered exogenous data (zero variance columns dropped)\n",
        "# Ensure no NaNs before scaling by dropping them from the split data\n",
        "target_train_nn = target_train.dropna()\n",
        "exog_train_nn = exog_train_filtered.loc[target_train_nn.index].dropna() # Align exog with target after dropping NaNs\n",
        "target_test_nn = target_test.dropna()\n",
        "exog_test_nn = exog_test_filtered.loc[target_test_nn.index].dropna() # Align exog with target after dropping NaNs\n",
        "\n",
        "\n",
        "# Ensure that the indices of target and exogenous data for NN match after dropping NaNs\n",
        "# This step is crucial to ensure correct pairing before scaling\n",
        "common_train_index_nn = target_train_nn.index.intersection(exog_train_nn.index)\n",
        "target_train_nn = target_train_nn.loc[common_train_index_nn]\n",
        "exog_train_nn = exog_train_nn.loc[common_train_index_nn]\n",
        "\n",
        "common_test_index_nn = target_test_nn.index.intersection(exog_test_nn.index)\n",
        "target_test_nn = target_test_nn.loc[common_test_index_nn]\n",
        "exog_test_nn = exog_test_nn.loc[common_test_index_nn]\n",
        "\n",
        "\n",
        "# Check for NaNs in train/test splits before scaling (NN) - Should be False now\n",
        "print(\"\\nChecking for NaNs in train/test splits BEFORE SCALING (NN):\")\n",
        "print(\"target_train_nn has NaNs:\", target_train_nn.isnull().sum().sum() > 0)\n",
        "print(\"target_test_nn has NaNs:\", target_test_nn.isnull().sum().sum() > 0)\n",
        "print(\"exog_train_nn has NaNs:\", exog_train_nn.isnull().sum().sum() > 0)\n",
        "print(\"exog_test_nn has NaNs:\", exog_test_nn.isnull().sum().sum() > 0)\n",
        "\n",
        "\n",
        "# Apply scaling\n",
        "if not target_train_nn.empty and not exog_train_nn.empty:\n",
        "    target_train_scaled = target_scaler.fit_transform(target_train_nn)\n",
        "    exog_train_scaled = exog_scaler.fit_transform(exog_train_nn) # Use filtered and dropped data for scaling\n",
        "\n",
        "    if not target_test_nn.empty and not exog_test_nn.empty:\n",
        "        target_test_scaled = target_scaler.transform(target_test_nn)\n",
        "        exog_test_scaled = exog_scaler.transform(exog_test_nn) # Use filtered and dropped data for scaling\n",
        "\n",
        "        # Check for NaNs after scaling (NN) - Should be False now\n",
        "        print(\"\\nChecking for NaNs after scaling (NN):\")\n",
        "        print(\"target_train_scaled has NaNs:\", np.isnan(target_train_scaled).sum() > 0)\n",
        "        print(\"target_test_scaled has NaNs:\", np.isnan(target_test_scaled).sum() > 0)\n",
        "        print(\"exog_train_scaled has NaNs:\", np.isnan(exog_train_scaled).sum() > 0)\n",
        "        print(\"exog_test_scaled has NaNs:\", np.isnan(exog_test_scaled).sum() > 0)\n",
        "\n",
        "\n",
        "        def create_sequences(X, y, time_step=1):\n",
        "            Xs, ys = [], []\n",
        "            for i in range(len(X) - time_step):\n",
        "                v = X[i:(i + time_step)]\n",
        "                Xs.append(v)\n",
        "                ys.append(y[i + time_step])\n",
        "            return np.array(Xs), np.array(ys)\n",
        "\n",
        "        time_step = 7\n",
        "        X_train, y_train = create_sequences(exog_train_scaled, target_train_scaled, time_step)\n",
        "        X_test, y_test = create_sequences(exog_test_scaled, target_test_scaled, time_step)\n",
        "\n",
        "        # Check for NaNs after creating sequences (NN) - Should be False now\n",
        "        print(\"\\nChecking for NaNs after creating sequences (NN):\")\n",
        "        print(\"X_train has NaNs:\", np.isnan(X_train).sum() > 0)\n",
        "        print(\"y_train has NaNs:\", np.isnan(y_train).sum() > 0)\n",
        "        print(\"X_test has NaNs:\", np.isnan(X_test).sum() > 0)\n",
        "        print(\"y_test has NaNs:\", np.isnan(y_test).sum() > 0)\n",
        "\n",
        "        n_features = X_train.shape[2]\n",
        "\n",
        "        print(\"Neural Network data prepared.\")\n",
        "        print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
        "        print(\"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)\n",
        "\n",
        "        # Adjust target_test_scaled for evaluation with neural networks - Use y_test\n",
        "        target_test_scaled_nn = y_test\n",
        "\n",
        "        # 4. LSTM Model\n",
        "        print(\"\\nBuilding and training LSTM model...\")\n",
        "        try:\n",
        "            if X_train.shape[0] > 0 and X_test.shape[0] > 0: # Check if training and test data are not empty after sequencing\n",
        "                lstm_model = Sequential()\n",
        "                lstm_model.add(LSTM(50, activation='relu', input_shape=(time_step, n_features)))\n",
        "                lstm_model.add(Dense(1))\n",
        "                lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "                # Use validation_split to monitor performance during training if needed\n",
        "                lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0) # Increased epochs and batch size\n",
        "                lstm_predictions_scaled = lstm_model.predict(X_test)\n",
        "                lstm_predictions = target_scaler.inverse_transform(lstm_predictions_scaled)\n",
        "                evaluation_results['LSTM'] = {'RMSE': np.sqrt(mean_squared_error(target_test_scaled_nn, lstm_predictions_scaled)),\n",
        "                                              'MAE': mean_absolute_error(target_test_scaled_nn, lstm_predictions_scaled)}\n",
        "                print(\"LSTM model trained and predictions made.\")\n",
        "            else:\n",
        "                print(\"Error: Not enough data to train/test LSTM after sequencing.\")\n",
        "                evaluation_results['LSTM'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "        except Exception as e:\n",
        "            print(f\"Error with LSTM model: {e}\")\n",
        "            evaluation_results['LSTM'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "\n",
        "        # 5. GRU Model\n",
        "        print(\"\\nBuilding and training GRU model...\")\n",
        "        try:\n",
        "            if X_train.shape[0] > 0 and X_test.shape[0] > 0: # Check if training and test data are not empty after sequencing\n",
        "                gru_model = Sequential()\n",
        "                gru_model.add(GRU(50, activation='relu', input_shape=(time_step, n_features)))\n",
        "                gru_model.add(Dense(1))\n",
        "                gru_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "                gru_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0) # Increased epochs and batch size\n",
        "                gru_predictions_scaled = gru_model.predict(X_test)\n",
        "                gru_predictions = target_scaler.inverse_transform(gru_predictions_scaled)\n",
        "                evaluation_results['GRU'] = {'RMSE': np.sqrt(mean_squared_error(target_test_scaled_nn, gru_predictions_scaled)),\n",
        "                                             'MAE': mean_absolute_error(target_test_scaled_nn, gru_predictions_scaled)}\n",
        "                print(\"GRU model trained and predictions made.\")\n",
        "            else:\n",
        "                print(\"Error: Not enough data to train/test GRU after sequencing.\")\n",
        "                evaluation_results['GRU'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with GRU model: {e}\")\n",
        "            evaluation_results['GRU'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "\n",
        "        # 6. BLSTM Model\n",
        "        print(\"\\nBuilding and training BLSTM model...\")\n",
        "        try:\n",
        "            if X_train.shape[0] > 0 and X_test.shape[0] > 0: # Check if training and test data are not empty after sequencing\n",
        "                blstm_model = Sequential()\n",
        "                blstm_model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(time_step, n_features)))\n",
        "                blstm_model.add(Dense(1))\n",
        "                blstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "                blstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0) # Increased epochs and batch size\n",
        "                blstm_predictions_scaled = blstm_model.predict(X_test)\n",
        "                blstm_predictions = target_scaler.inverse_transform(blstm_predictions_scaled)\n",
        "                evaluation_results['BLSTM'] = {'RMSE': np.sqrt(mean_squared_error(target_test_scaled_nn, blstm_predictions_scaled)),\n",
        "                                           'MAE': mean_absolute_error(target_test_scaled_nn, blstm_predictions_scaled)}\n",
        "                print(\"BLSTM model trained and predictions made.\")\n",
        "            else:\n",
        "                print(\"Error: Not enough data to train/test BLSTM after sequencing.\")\n",
        "                evaluation_results['BLSTM'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with BLSTM model: {e}\")\n",
        "            evaluation_results['BLSTM'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "\n",
        "        # 7. CNN Model\n",
        "        print(\"\\nBuilding and training CNN model...\")\n",
        "        try:\n",
        "            if X_train.shape[0] > 0 and X_test.shape[0] > 0: # Check if training and test data are not empty after sequencing\n",
        "                cnn_model = Sequential()\n",
        "                cnn_model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(time_step, n_features)))\n",
        "                cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "                cnn_model.add(Flatten())\n",
        "                cnn_model.add(Dense(50, activation='relu'))\n",
        "                cnn_model.add(Dense(1))\n",
        "                cnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "                cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0) # Increased epochs and batch size\n",
        "                cnn_predictions_scaled = cnn_model.predict(X_test)\n",
        "                cnn_predictions = target_scaler.inverse_transform(cnn_predictions_scaled)\n",
        "                evaluation_results['CNN'] = {'RMSE': np.sqrt(mean_squared_error(target_test_scaled_nn, cnn_predictions_scaled)),\n",
        "                                         'MAE': mean_absolute_error(target_test_scaled_nn, cnn_predictions_scaled)}\n",
        "                print(\"CNN model trained and predictions made.\")\n",
        "            else:\n",
        "                print(\"Error: Not enough data to train/test CNN after sequencing.\")\n",
        "                evaluation_results['CNN'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with CNN model: {e}\")\n",
        "            evaluation_results['CNN'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "\n",
        "        # 8. CNN-LSTM Model\n",
        "        print(\"\\nBuilding and training CNN-LSTM model...\")\n",
        "        try:\n",
        "            if X_train.shape[0] > 0 and X_test.shape[0] > 0: # Check if training and test data are not empty after sequencing\n",
        "                # Reshape input for CNN-LSTM (samples, subsequences, timesteps_per_subsequence, features)\n",
        "                n_seq_cnn_lstm = 1\n",
        "                n_steps_cnn_lstm = time_step\n",
        "\n",
        "                X_train_cnn_lstm = X_train.reshape((X_train.shape[0], n_seq_cnn_lstm, n_steps_cnn_lstm, n_features))\n",
        "                X_test_cnn_lstm = X_test.reshape((X_test.shape[0], n_seq_cnn_lstm, n_steps_cnn_lstm, n_features))\n",
        "\n",
        "\n",
        "                cnn_lstm_model = Sequential()\n",
        "                cnn_lstm_model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps_cnn_lstm, n_features)))\n",
        "                cnn_lstm_model.add(TimeDistributed(MaxPooling1D(pool_size=1)))\n",
        "                cnn_lstm_model.add(TimeDistributed(Flatten()))\n",
        "                cnn_lstm_model.add(LSTM(50, activation='relu'))\n",
        "                cnn_lstm_model.add(Dense(1))\n",
        "\n",
        "                cnn_lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "                cnn_lstm_model.fit(X_train_cnn_lstm, y_train, epochs=5, batch_size=32, verbose=0) # Increased epochs and batch size\n",
        "                cnn_lstm_predictions_scaled = cnn_lstm_model.predict(X_test_cnn_lstm)\n",
        "                cnn_lstm_predictions = target_scaler.inverse_transform(cnn_lstm_predictions_scaled)\n",
        "                evaluation_results['CNN-LSTM'] = {'RMSE': np.sqrt(mean_squared_error(target_test_scaled_nn, cnn_lstm_predictions_scaled)),\n",
        "                                              'MAE': mean_absolute_error(target_test_scaled_nn, cnn_lstm_predictions_scaled)}\n",
        "                print(\"CNN-LSTM model trained and predictions made.\")\n",
        "            else:\n",
        "                print(\"Error: Not enough data to train/test CNN-LSTM after sequencing.\")\n",
        "                evaluation_results['CNN-LSTM'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with CNN-LSTM model: {e}\")\n",
        "            evaluation_results['CNN-LSTM'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "    else:\n",
        "        print(\"\\nError: Not enough data in test set after dropping NaNs for Neural Networks.\")\n",
        "        # Set all NN model results to NaN if test data is insufficient\n",
        "        nn_models = ['LSTM', 'GRU', 'BLSTM', 'CNN', 'CNN-LSTM']\n",
        "        for model_name in nn_models:\n",
        "             evaluation_results[model_name] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "else:\n",
        "    print(\"\\nError: Not enough data in training set after dropping NaNs for Neural Networks.\")\n",
        "    # Set all NN model results to NaN if training data is insufficient\n",
        "    nn_models = ['LSTM', 'GRU', 'BLSTM', 'CNN', 'CNN-LSTM']\n",
        "    for model_name in nn_models:\n",
        "        evaluation_results[model_name] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "print(\"\\nPreparing data for XGBoost...\")\n",
        "# 1. Define the target variable y and features X - Use original df\n",
        "y = df[target]\n",
        "exog_cols_xgb = [col for col in df.columns if col != target]\n",
        "X = df[exog_cols_xgb]\n",
        "\n",
        "\n",
        "# 2. Create lagged features for the target variable\n",
        "lag_values = [1, 7, 30]\n",
        "lagged_y = pd.DataFrame(index=df.index) # Use original df index\n",
        "\n",
        "for lag in lag_values:\n",
        "    lagged_y[f'{target}_lag_{lag}'] = y.shift(lag)\n",
        "\n",
        "# 3. Combine the original features (X) with the newly created lagged features\n",
        "X_xgb = pd.concat([X, lagged_y], axis=1)\n",
        "\n",
        "# 4. Split the data into training and testing sets for XGBoost using original indices\n",
        "X_train_xgb = X_xgb.loc[train_index]\n",
        "X_test_xgb = X_xgb.loc[test_index]\n",
        "y_train_xgb = y.loc[train_index]\n",
        "y_test_xgb = y.loc[test_index]\n",
        "\n",
        "\n",
        "# 5. Now drop NaNs from the split XGBoost data (introduced by lagging)\n",
        "initial_nans_train_xgb = X_train_xgb.isnull().sum().sum() + y_train_xgb.isnull().sum()\n",
        "initial_nans_test_xgb = X_test_xgb.isnull().sum().sum() + y_test_xgb.isnull().sum()\n",
        "\n",
        "train_data_xgb = pd.concat([X_train_xgb, y_train_xgb], axis=1).dropna()\n",
        "X_train_xgb = train_data_xgb[X_train_xgb.columns]\n",
        "y_train_xgb = train_data_xgb[target]\n",
        "\n",
        "test_data_xgb = pd.concat([X_test_xgb, y_test_xgb], axis=1).dropna()\n",
        "X_test_xgb = test_data_xgb[X_test_xgb.columns]\n",
        "y_test_xgb = test_data_xgb[target]\n",
        "\n",
        "\n",
        "print(f\"\\nDropped NaNs from XGBoost train split: {initial_nans_train_xgb - (X_train_xgb.isnull().sum().sum() + y_train_xgb.isnull().sum())} NaNs\")\n",
        "print(f\"Dropped NaNs from XGBoost test split: {initial_nans_test_xgb - (X_test_xgb.isnull().sum().sum() + y_test_xgb.isnull().sum())} NaNs\")\n",
        "\n",
        "\n",
        "# Check for NaNs in XGBoost train/test splits after dropping - Should be False now\n",
        "print(\"\\nChecking for NaNs in XGBoost train/test splits AFTER DROPPING:\")\n",
        "print(\"X_train_xgb has NaNs:\", X_train_xgb.isnull().sum().sum() > 0)\n",
        "print(\"X_test_xgb has NaNs:\", X_test_xgb.isnull().sum().sum() > 0)\n",
        "print(\"y_train_xgb has NaNs:\", y_train_xgb.isnull().sum() > 0)\n",
        "print(\"y_test_xgb has NaNs:\", y_test_xgb.isnull().sum() > 0)\n",
        "\n",
        "\n",
        "print(\"\\nXGBoost data prepared.\")\n",
        "print(\"X_train_xgb shape:\", X_train_xgb.shape, \"y_train_xgb shape:\", y_train_xgb.shape)\n",
        "print(\"X_test_xgb shape:\", X_test_xgb.shape, \"y_test_xgb shape:\", y_test_xgb.shape)\n",
        "\n",
        "\n",
        "# 9. XGBoost Model\n",
        "print(\"\\nBuilding and training XGBoost model...\")\n",
        "try:\n",
        "    if X_train_xgb.shape[0] > 0 and X_test_xgb.shape[0] > 0: # Check if training and test data are not empty after dropping NaNs\n",
        "        # Create an instance of the XGBoost Regressor model\n",
        "        # Using common parameters for regression\n",
        "        xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
        "\n",
        "        # Train the XGBoost model\n",
        "        xgb_model.fit(X_train_xgb, y_train_xgb)\n",
        "        print(\"XGBoost model trained.\")\n",
        "\n",
        "        # Generate predictions on the test set\n",
        "        xgb_predictions = xgb_model.predict(X_test_xgb)\n",
        "        print(\"XGBoost predictions made.\")\n",
        "\n",
        "        # Add XGBoost evaluation results\n",
        "        evaluation_results['XGBoost'] = {'RMSE': np.sqrt(mean_squared_error(y_test_xgb, xgb_predictions)),\n",
        "                                         'MAE': mean_absolute_error(y_test_xgb, xgb_predictions)}\n",
        "    else:\n",
        "        print(\"Error: Not enough data to train/test XGBoost after dropping NaNs.\")\n",
        "        evaluation_results['XGBoost'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error with XGBoost model: {e}\")\n",
        "    evaluation_results['XGBoost'] = {'RMSE': np.nan, 'MAE': np.nan}\n",
        "\n",
        "\n",
        "# --- Evaluation and Comparison ---\n",
        "\n",
        "print(\"\\n--- Model Evaluation Results ---\")\n",
        "evaluation_table = pd.DataFrame(evaluation_results).T\n",
        "display(evaluation_table)\n",
        "\n",
        "print(\"\\n--- Model Comparison ---\")\n",
        "# Filter out models with NaN RMSE or MAE for comparison\n",
        "comparable_models = evaluation_table.dropna()\n",
        "\n",
        "if not comparable_models.empty:\n",
        "    winning_model_rmse = comparable_models['RMSE'].idxmin()\n",
        "    winning_model_mae = comparable_models['MAE'].idxmin()\n",
        "\n",
        "    print(f\"Model with lowest RMSE: {winning_model_rmse} (RMSE: {comparable_models.loc[winning_model_rmse, 'RMSE']:.4f})\")\n",
        "    print(f\"Model with lowest MAE: {winning_model_mae} (MAE: {comparable_models.loc[winning_model_mae, 'MAE']:.4f})\")\n",
        "\n",
        "    if winning_model_rmse == winning_model_mae:\n",
        "        print(f\"The winning model is {winning_model_rmse} as it has the lowest RMSE and MAE among comparable models.\")\n",
        "        print(\"Rationale: This model's predictions are closest to the actual values based on these common evaluation metrics for forecasting.\")\n",
        "    else:\n",
        "        print(\"The winning models for RMSE and MAE are different among comparable models.\")\n",
        "        print(\"Rationale: The choice of the 'best' model depends on which metric is considered more important for your specific application.\")\n",
        "else:\n",
        "    print(\"No models with valid evaluation results to compare.\")\n",
        "\n",
        "\n",
        "# --- Naive Forecast Calculation ---\n",
        "print(\"\\n--- Naive Forecast Analysis ---\")\n",
        "\n",
        "# Implement naive forecast (using prior day's volume)\n",
        "# Use the original df for naive forecast before dropping rows for lagged features\n",
        "naive_predictions = df[target].shift(1)\n",
        "\n",
        "# Calculate the error for each day\n",
        "# We need to align the actual values and naive predictions,\n",
        "# dropping the first row which will have a NaN prediction\n",
        "actual_values = df[target][1:]\n",
        "naive_predictions = naive_predictions[1:]\n",
        "\n",
        "# Calculate the absolute forecast error\n",
        "absolute_forecast_errors = abs(actual_values - naive_predictions)\n",
        "\n",
        "# Calculate the average absolute forecast error\n",
        "naive_average_absolute_error = absolute_forecast_errors.mean()\n",
        "\n",
        "print(f\"Average forecast error using naive assumption (prior day's volume): {naive_average_absolute_error:.4f}\")\n",
        "\n",
        "# Calculate the average actual contact volume over the same period as the naive forecast evaluation\n",
        "average_actual_volume = actual_values.mean()\n",
        "\n",
        "# Calculate the average percentage error for the naive forecast\n",
        "naive_average_percentage_error = (naive_average_absolute_error / average_actual_volume) * 100\n",
        "\n",
        "print(f\"Average percentage forecast error using naive assumption: {naive_average_percentage_error:.2f}%\")\n",
        "\n",
        "\n",
        "# --- Percentage Improvement over Naive Forecast ---\n",
        "\n",
        "print(\"\\n--- Percentage Improvement over Naive Forecast ---\")\n",
        "\n",
        "# Iterate through the evaluation results of the trained models\n",
        "for model_name, metrics in evaluation_results.items():\n",
        "    rmse = metrics.get('RMSE')\n",
        "    mae = metrics.get('MAE')\n",
        "\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "\n",
        "    # Calculate and print percentage improvement\n",
        "    # Using MAE for comparison with naive forecast as it's also an absolute error\n",
        "    if pd.notna(mae) and naive_average_absolute_error != 0:\n",
        "        improvement_value = ((naive_average_absolute_error - mae) / naive_average_absolute_error) * 100\n",
        "        print(f\"  MAE Improvement over Naive: {improvement_value:.2f}%\")\n",
        "    else:\n",
        "        print(\"  MAE Improvement over Naive: N/A (Model MAE is NaN or Naive error is zero)\")\n",
        "\n",
        "    if pd.notna(rmse) and naive_average_absolute_error != 0:\n",
        "         # Can also show RMSE improvement if desired, but MAE is a more direct comparison\n",
        "         rmse_improvement = ((naive_average_absolute_error - rmse) / naive_average_absolute_error) * 100\n",
        "         print(f\"  RMSE Improvement over Naive: {rmse_improvement:.2f}%\")\n",
        "    else:\n",
        "         print(\"  RMSE Improvement over Naive: N/A (Model RMSE is NaN or Naive error is zero)\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Call volume data loaded and indexed successfully.\n",
            "No NaN values found in the initial dataframe.\n",
            "\n",
            "Data split into training and testing sets.\n",
            "Target data shapes - Train: (538, 1) Test: (135, 1)\n",
            "Exogenous data shapes - Train: (538, 40) Test: (135, 40)\n",
            "\n",
            "Checking for NaNs in train/test splits AFTER initial drop:\n",
            "target_train has NaNs: False\n",
            "target_test has NaNs: False\n",
            "exog_train has NaNs: False\n",
            "exog_test has NaNs: False\n",
            "\n",
            "Checking for zero variance columns in training data:\n",
            "Columns with zero variance in exog_train: ['^VIX_Volume_^VIX', 'DX-Y.NYB_Volume_DX-Y.NYB']\n",
            "Zero variance columns dropped from exog_train and exog_test.\n",
            "\n",
            "Building and training Holt-Winters model...\n",
            "Inferred frequency for target_train: <Day>\n",
            "Holt-Winters model trained and predictions made.\n",
            "\n",
            "Building and training SARIMAX model...\n",
            "Inferred frequency for exog_train_filtered (SARIMAX): <Day>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
            "  warn('Non-stationary starting autoregressive parameters'\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
            "  warn('Non-invertible starting MA parameters found.'\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inferred frequency for exog_test_filtered (SARIMAX): <Day>\n",
            "Error with SARIMAX model: Found input variables with inconsistent numbers of samples: [135, 0]\n",
            "\n",
            "Building and training ARIMA model...\n",
            "ARIMA model trained and predictions made.\n",
            "\n",
            "Preparing data for Neural Network models...\n",
            "\n",
            "Checking for NaNs in train/test splits BEFORE SCALING (NN):\n",
            "target_train_nn has NaNs: False\n",
            "target_test_nn has NaNs: False\n",
            "exog_train_nn has NaNs: False\n",
            "exog_test_nn has NaNs: False\n",
            "\n",
            "Checking for NaNs after scaling (NN):\n",
            "target_train_scaled has NaNs: False\n",
            "target_test_scaled has NaNs: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
            "  return get_prediction_index(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exog_train_scaled has NaNs: False\n",
            "exog_test_scaled has NaNs: False\n",
            "\n",
            "Checking for NaNs after creating sequences (NN):\n",
            "X_train has NaNs: False\n",
            "y_train has NaNs: False\n",
            "X_test has NaNs: False\n",
            "y_test has NaNs: False\n",
            "Neural Network data prepared.\n",
            "X_train shape: (531, 7, 38) y_train shape: (531, 1)\n",
            "X_test shape: (128, 7, 38) y_test shape: (128, 1)\n",
            "\n",
            "Building and training LSTM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step  \n",
            "LSTM model trained and predictions made.\n",
            "\n",
            "Building and training GRU model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
            "GRU model trained and predictions made.\n",
            "\n",
            "Building and training BLSTM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
            "BLSTM model trained and predictions made.\n",
            "\n",
            "Building and training CNN model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "CNN model trained and predictions made.\n",
            "\n",
            "Building and training CNN-LSTM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step  \n",
            "CNN-LSTM model trained and predictions made.\n",
            "\n",
            "Preparing data for XGBoost...\n",
            "\n",
            "Dropped NaNs from XGBoost train split: 38 NaNs\n",
            "Dropped NaNs from XGBoost test split: 0 NaNs\n",
            "\n",
            "Checking for NaNs in XGBoost train/test splits AFTER DROPPING:\n",
            "X_train_xgb has NaNs: False\n",
            "X_test_xgb has NaNs: False\n",
            "y_train_xgb has NaNs: False\n",
            "y_test_xgb has NaNs: False\n",
            "\n",
            "XGBoost data prepared.\n",
            "X_train_xgb shape: (508, 43) y_train_xgb shape: (508,)\n",
            "X_test_xgb shape: (135, 43) y_test_xgb shape: (135,)\n",
            "\n",
            "Building and training XGBoost model...\n",
            "XGBoost model trained.\n",
            "XGBoost predictions made.\n",
            "\n",
            "--- Model Evaluation Results ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                     RMSE          MAE\n",
              "Holt-Winters  1628.228773  1351.152857\n",
              "SARIMAX               NaN          NaN\n",
              "ARIMA         1623.239152  1351.783768\n",
              "LSTM             0.168335     0.137981\n",
              "GRU              0.099596     0.082494\n",
              "BLSTM            0.261334     0.241564\n",
              "CNN              0.173565     0.158742\n",
              "CNN-LSTM         0.247274     0.226829\n",
              "XGBoost       1378.123180  1021.364929"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47d703f5-bcf5-4df6-a4d6-afd71d01e211\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Holt-Winters</th>\n",
              "      <td>1628.228773</td>\n",
              "      <td>1351.152857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SARIMAX</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ARIMA</th>\n",
              "      <td>1623.239152</td>\n",
              "      <td>1351.783768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LSTM</th>\n",
              "      <td>0.168335</td>\n",
              "      <td>0.137981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GRU</th>\n",
              "      <td>0.099596</td>\n",
              "      <td>0.082494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BLSTM</th>\n",
              "      <td>0.261334</td>\n",
              "      <td>0.241564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CNN</th>\n",
              "      <td>0.173565</td>\n",
              "      <td>0.158742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CNN-LSTM</th>\n",
              "      <td>0.247274</td>\n",
              "      <td>0.226829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost</th>\n",
              "      <td>1378.123180</td>\n",
              "      <td>1021.364929</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47d703f5-bcf5-4df6-a4d6-afd71d01e211')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-47d703f5-bcf5-4df6-a4d6-afd71d01e211 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-47d703f5-bcf5-4df6-a4d6-afd71d01e211');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c24bd5a4-e665-4a9b-9c77-7e0efc7f84d7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c24bd5a4-e665-4a9b-9c77-7e0efc7f84d7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c24bd5a4-e665-4a9b-9c77-7e0efc7f84d7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_a698e834-b494-499e-8d86-5768a7bd117a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('evaluation_table')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a698e834-b494-499e-8d86-5768a7bd117a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('evaluation_table');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "evaluation_table",
              "summary": "{\n  \"name\": \"evaluation_table\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 802.230728608655,\n        \"min\": 0.09959609187447306,\n        \"max\": 1628.2287734002987,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1623.239151553004,\n          0.17356467798763764,\n          1628.2287734002987\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 650.4424694056153,\n        \"min\": 0.08249368801897536,\n        \"max\": 1351.783768329576,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1351.783768329576,\n          0.1587419809547063,\n          1351.1528572212578\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Comparison ---\n",
            "Model with lowest RMSE: GRU (RMSE: 0.0996)\n",
            "Model with lowest MAE: GRU (MAE: 0.0825)\n",
            "The winning model is GRU as it has the lowest RMSE and MAE among comparable models.\n",
            "Rationale: This model's predictions are closest to the actual values based on these common evaluation metrics for forecasting.\n",
            "\n",
            "--- Naive Forecast Analysis ---\n",
            "Average forecast error using naive assumption (prior day's volume): 610.0045\n",
            "Average percentage forecast error using naive assumption: 6.58%\n",
            "\n",
            "--- Percentage Improvement over Naive Forecast ---\n",
            "\n",
            "Model: Holt-Winters\n",
            "  MAE Improvement over Naive: -121.50%\n",
            "  RMSE Improvement over Naive: -166.92%\n",
            "\n",
            "Model: SARIMAX\n",
            "  MAE Improvement over Naive: N/A (Model MAE is NaN or Naive error is zero)\n",
            "  RMSE Improvement over Naive: N/A (Model RMSE is NaN or Naive error is zero)\n",
            "\n",
            "Model: ARIMA\n",
            "  MAE Improvement over Naive: -121.60%\n",
            "  RMSE Improvement over Naive: -166.10%\n",
            "\n",
            "Model: LSTM\n",
            "  MAE Improvement over Naive: 99.98%\n",
            "  RMSE Improvement over Naive: 99.97%\n",
            "\n",
            "Model: GRU\n",
            "  MAE Improvement over Naive: 99.99%\n",
            "  RMSE Improvement over Naive: 99.98%\n",
            "\n",
            "Model: BLSTM\n",
            "  MAE Improvement over Naive: 99.96%\n",
            "  RMSE Improvement over Naive: 99.96%\n",
            "\n",
            "Model: CNN\n",
            "  MAE Improvement over Naive: 99.97%\n",
            "  RMSE Improvement over Naive: 99.97%\n",
            "\n",
            "Model: CNN-LSTM\n",
            "  MAE Improvement over Naive: 99.96%\n",
            "  RMSE Improvement over Naive: 99.96%\n",
            "\n",
            "Model: XGBoost\n",
            "  MAE Improvement over Naive: -67.44%\n",
            "  RMSE Improvement over Naive: -125.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b7f7587"
      },
      "source": [
        "The necessary libraries are now installed. You can now run the code cell to execute the analysis."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}