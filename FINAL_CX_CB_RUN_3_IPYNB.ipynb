{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/FINAL_CX_CB_RUN_3_IPYNB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clHwuOJtzA6S"
      },
      "source": [
        "# Enhanced Time Series Forecasting System\n",
        "\n",
        "**Complete Implementation with 25-30 ML Models and VERIFIED GridSearchCV**\n",
        "\n",
        "This notebook implements a comprehensive machine learning approach to time series forecasting with full debugging.\n",
        "\n",
        "## Key Features:\n",
        "- 25-30 diverse ML models (tree-based, linear, neural networks, ensemble)\n",
        "- VERIFIED GridSearchCV execution with timing\n",
        "- Advanced feature engineering for time series â†’ supervised learning\n",
        "- Market regime-aware model selection and feature engineering\n",
        "- Three-phase optimization: V1 â†’ V2 â†’ VP\n",
        "- Comprehensive debugging and error visibility\n",
        "- Performance monitoring at every step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwAZSmohzA6Y"
      },
      "source": [
        "## GPU and System Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1q5XOFiczA6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22da7348-2bd9-471f-94b1-0a1586053349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 24 15:52:35 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check NVIDIA GPU Status\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-Aw50woXzA6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e1202b-9184-447d-d804-6c5bb9560aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n",
            "GPU Memory: 15.828320256 GB\n",
            "GPU Compute Capability: 7 . 5\n",
            "Available RAM: 52.7 GB\n",
            "Total RAM: 54.8 GB\n",
            "\n",
            "SYSTEM CAPABILITIES:\n",
            "  GPU Available: True\n",
            "  High Memory: True\n",
            "\n",
            "Tensor device test: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# PyTorch GPU Setup and System Info\n",
        "import torch\n",
        "import psutil\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "    print(\"GPU Memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
        "    print(\"GPU Compute Capability:\", torch.cuda.get_device_properties(0).major, \".\", torch.cuda.get_device_properties(0).minor)\n",
        "    GPU_AVAILABLE = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "    GPU_AVAILABLE = False\n",
        "\n",
        "# Memory check\n",
        "available_ram_gb = psutil.virtual_memory().available / 1e9\n",
        "total_ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print(f\"Available RAM: {available_ram_gb:.1f} GB\")\n",
        "print(f\"Total RAM: {total_ram_gb:.1f} GB\")\n",
        "\n",
        "# Set system capabilities flags\n",
        "HIGH_MEMORY = available_ram_gb > 16\n",
        "print(f\"\\nSYSTEM CAPABILITIES:\")\n",
        "print(f\"  GPU Available: {GPU_AVAILABLE}\")\n",
        "print(f\"  High Memory: {HIGH_MEMORY}\")\n",
        "\n",
        "# Example: Move a tensor to the GPU\n",
        "x = torch.randn(10, 10).to(device)\n",
        "print(f\"\\nTensor device test: {x.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4\n",
        "!pip install tensorflow\n",
        "!pip install tbats\n",
        "!pip install pmdarima"
      ],
      "metadata": {
        "id": "8J_acMzfzWvQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "109a2115-4ef9-416e-bd34-20e0aec9b236"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "a4ea43d96ff64a65a53946bdc3d4c3c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Collecting tbats\n",
            "  Downloading tbats-1.1.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tbats) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from tbats) (1.16.2)\n",
            "Collecting pmdarima (from tbats)\n",
            "  Downloading pmdarima-2.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from tbats) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from pmdarima->tbats) (1.5.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.12/dist-packages (from pmdarima->tbats) (3.0.12)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.12/dist-packages (from pmdarima->tbats) (2.2.2)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from pmdarima->tbats) (0.14.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from pmdarima->tbats) (2.5.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.12/dist-packages (from pmdarima->tbats) (75.2.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.12/dist-packages (from pmdarima->tbats) (25.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->tbats) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19->pmdarima->tbats) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19->pmdarima->tbats) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19->pmdarima->tbats) (2025.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.13.2->pmdarima->tbats) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima->tbats) (1.17.0)\n",
            "Downloading tbats-1.1.3-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pmdarima-2.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pmdarima, tbats\n",
            "Successfully installed pmdarima-2.0.4 tbats-1.1.3\n",
            "Requirement already satisfied: pmdarima in /usr/local/lib/python3.12/dist-packages (2.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from pmdarima) (1.5.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.12/dist-packages (from pmdarima) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.12/dist-packages (from pmdarima) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.12/dist-packages (from pmdarima) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.12/dist-packages (from pmdarima) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from pmdarima) (1.16.2)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from pmdarima) (0.14.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from pmdarima) (2.5.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.12/dist-packages (from pmdarima) (75.2.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.12/dist-packages (from pmdarima) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19->pmdarima) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22->pmdarima) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.13.2->pmdarima) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyF8yY-rzA6d"
      },
      "source": [
        "## Enhanced Imports & Logging Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJkmb1WrzA6d",
        "outputId": "008dc7be-6b9f-48a0-beb5-e0e8e964006c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n",
            "GPU Memory: 15.828320256 GB\n",
            "GPU Compute Capability: 7 . 5\n",
            "Available RAM: 52.4 GB\n",
            "Total RAM: 54.8 GB\n",
            "\n",
            "SYSTEM CAPABILITIES:\n",
            "  GPU Available: True\n",
            "  High Memory: True\n",
            "\n",
            "Tensor device test: cuda:0\n",
            "\n",
            "LIBRARY STATUS:\n",
            "  XGBoost: âœ… (GPU: âœ…)\n",
            "  LightGBM: âœ… (GPU: âœ…)\n",
            "  PyTorch: âœ…\n"
          ]
        }
      ],
      "source": [
        "# Enhanced imports with debugging setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import time\n",
        "import logging\n",
        "import sys\n",
        "import traceback\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Optional, Tuple, Any, Union\n",
        "from dataclasses import dataclass\n",
        "import itertools\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# PyTorch GPU Setup and System Info\n",
        "import torch\n",
        "import psutil\n",
        "\n",
        "# Configure comprehensive logging for notebook visibility\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout),  # Force notebook output\n",
        "        logging.FileHandler('ml_pipeline_debug.log')  # Save to file\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"DEBUG LOGGING INITIALIZED - Messages will appear in notebook and log file\")\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "    print(\"GPU Memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
        "    print(\"GPU Compute Capability:\", torch.cuda.get_device_properties(0).major, \".\", torch.cuda.get_device_properties(0).minor)\n",
        "    GPU_AVAILABLE = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "    GPU_AVAILABLE = False\n",
        "\n",
        "# Memory check\n",
        "available_ram_gb = psutil.virtual_memory().available / 1e9\n",
        "total_ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print(f\"Available RAM: {available_ram_gb:.1f} GB\")\n",
        "print(f\"Total RAM: {total_ram_gb:.1f} GB\")\n",
        "\n",
        "# Set system capabilities flags\n",
        "HIGH_MEMORY = available_ram_gb > 16\n",
        "print(f\"\\nSYSTEM CAPABILITIES:\")\n",
        "print(f\"  GPU Available: {GPU_AVAILABLE}\")\n",
        "print(f\"  High Memory: {HIGH_MEMORY}\")\n",
        "\n",
        "# Example: Move a tensor to the GPU\n",
        "x = torch.randn(10, 10).to(device)\n",
        "print(f\"\\nTensor device test: {x.device}\")\n",
        "\n",
        "\n",
        "# Core ML imports\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Tree-based models\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Check optional libraries with detailed reporting\n",
        "logger.info(\"Checking optional ML libraries...\")\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGB_AVAILABLE = True\n",
        "    logger.info(f\"XGBoost available: {xgb.__version__}\")\n",
        "    # Test GPU capability\n",
        "    if GPU_AVAILABLE:\n",
        "        try:\n",
        "            test_xgb = xgb.XGBRegressor(tree_method='gpu_hist', n_estimators=1)\n",
        "            XGB_GPU_AVAILABLE = True\n",
        "            logger.info(\"XGBoost GPU support: AVAILABLE\")\n",
        "        except Exception as e:\n",
        "            XGB_GPU_AVAILABLE = False\n",
        "            logger.warning(f\"XGBoost GPU support: NOT AVAILABLE - {str(e)}\")\n",
        "    else:\n",
        "        XGB_GPU_AVAILABLE = False\n",
        "except ImportError:\n",
        "    XGB_AVAILABLE = False\n",
        "    XGB_GPU_AVAILABLE = False\n",
        "    logger.warning(\"XGBoost not available. Install with: pip install xgboost\")\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGB_AVAILABLE = True\n",
        "    logger.info(f\"LightGBM available: {lgb.__version__}\")\n",
        "    # Test GPU capability\n",
        "    if GPU_AVAILABLE:\n",
        "        try:\n",
        "            test_lgb = lgb.LGBMRegressor(device='gpu', n_estimators=1, verbose=-1)\n",
        "            LGB_GPU_AVAILABLE = True\n",
        "            logger.info(\"LightGBM GPU support: AVAILABLE\")\n",
        "        except Exception as e:\n",
        "            LGB_GPU_AVAILABLE = False\n",
        "            logger.warning(f\"LightGBM GPU support: NOT AVAILABLE - {str(e)}\")\n",
        "    else:\n",
        "        LGB_GPU_AVAILABLE = False\n",
        "except ImportError:\n",
        "    LGB_AVAILABLE = False\n",
        "    LGB_GPU_AVAILABLE = False\n",
        "    logger.warning(\"LightGBM not available. Install with: pip install lightgbm\")\n",
        "\n",
        "# Linear models\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression, Ridge, Lasso, ElasticNet,\n",
        "    BayesianRidge, ARDRegression, HuberRegressor,\n",
        "    SGDRegressor, PassiveAggressiveRegressor\n",
        ")\n",
        "\n",
        "# Neural networks\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Other models\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "# Feature engineering\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from scipy import stats\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "# Suppress sklearn warnings but keep our logging\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# Set reproducible seed\n",
        "np.random.seed(42)\n",
        "\n",
        "logger.info(\"All imports completed successfully\")\n",
        "print(f\"\\nLIBRARY STATUS:\")\n",
        "print(f\"  XGBoost: {'âœ…' if XGB_AVAILABLE else 'âŒ'} (GPU: {'âœ…' if XGB_GPU_AVAILABLE else 'âŒ'})\")\n",
        "print(f\"  LightGBM: {'âœ…' if LGB_AVAILABLE else 'âŒ'} (GPU: {'âœ…' if LGB_GPU_AVAILABLE else 'âŒ'})\")\n",
        "print(f\"  PyTorch: {'âœ…' if torch.cuda.is_available() else 'âŒ'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu83sAf6zA6e"
      },
      "source": [
        "## Configuration with Debug Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fk7W0fjzA6f",
        "outputId": "d93a8193-919a-4baf-bf9b-054dffbbd218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration and monitoring setup complete\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class DebugMLForecastingConfig:\n",
        "    \"\"\"Configuration with comprehensive debugging capabilities\"\"\"\n",
        "\n",
        "    # Data parameters\n",
        "    target_column: str = \"calls\"\n",
        "    seasonal_period: int = 7\n",
        "    test_split_ratio: float = 0.7\n",
        "    validation_split_ratio: float = 0.15\n",
        "\n",
        "    # Feature engineering\n",
        "    max_lags: int = 21\n",
        "    rolling_windows: List[int] = None\n",
        "    create_technical_indicators: bool = True\n",
        "    create_calendar_features: bool = True\n",
        "    polynomial_degree: int = 2\n",
        "\n",
        "    # Model selection\n",
        "    use_tree_models: bool = True\n",
        "    use_linear_models: bool = True\n",
        "    use_neural_models: bool = True\n",
        "    use_ensemble_models: bool = True\n",
        "    use_other_models: bool = True\n",
        "\n",
        "    # GridSearchCV parameters - CRITICAL FOR DEBUGGING\n",
        "    cv_splits: int = 3\n",
        "    parallel_jobs: int = 1  # Single job for debugging visibility\n",
        "    scoring_metric: str = 'neg_mean_absolute_error'\n",
        "    gridsearch_verbose: int = 2  # Show GridSearch progress\n",
        "\n",
        "    # Market regime integration\n",
        "    use_market_regime_switching: bool = True\n",
        "    use_regime_features: bool = True\n",
        "    vix_thresholds: Dict[str, float] = None\n",
        "    regime_specific_models: Dict[str, List[str]] = None\n",
        "\n",
        "    # Optimization levels\n",
        "    quick_search: bool = False  # Force detailed search for debugging\n",
        "    detailed_search: bool = True\n",
        "    top_models_for_optimization: int = 5\n",
        "\n",
        "    # Pipeline phases\n",
        "    enable_v2_feature_engineering: bool = True\n",
        "    enable_vp_optimization: bool = True\n",
        "\n",
        "    # Feature selection\n",
        "    feature_selection_method: str = 'auto'\n",
        "    max_features_ratio: float = 0.8\n",
        "\n",
        "    # Debug settings\n",
        "    debug_mode: bool = True\n",
        "    show_progress: bool = True\n",
        "    time_each_phase: bool = True\n",
        "    validate_gridsearch: bool = True\n",
        "    save_intermediate_results: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.rolling_windows is None:\n",
        "            if HIGH_MEMORY:\n",
        "                self.rolling_windows = [3, 7, 14, 21, 30]\n",
        "                self.max_lags = 30\n",
        "                self.top_models_for_optimization = 8\n",
        "                self.cv_splits = 5\n",
        "            else:\n",
        "                self.rolling_windows = [3, 7, 14]\n",
        "                self.max_lags = 15\n",
        "                self.top_models_for_optimization = 3\n",
        "\n",
        "        if self.vix_thresholds is None:\n",
        "            self.vix_thresholds = {\n",
        "                'low_volatility': 15,\n",
        "                'normal': 25,\n",
        "                'high_volatility': 35\n",
        "            }\n",
        "\n",
        "        if self.regime_specific_models is None:\n",
        "            self.regime_specific_models = {\n",
        "                'low_volatility': ['LinearRegression', 'Ridge', 'RandomForest'],\n",
        "                'normal': ['RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM'],\n",
        "                'high_volatility': ['ExtraTrees', 'SVR', 'KNeighbors', 'MLP'],\n",
        "                'extreme_volatility': ['Lasso', 'ElasticNet', 'Huber', 'BayesianRidge']\n",
        "            }\n",
        "\n",
        "        logger.info(f\"Configuration initialized - Debug mode: {self.debug_mode}\")\n",
        "        logger.info(f\"  Max lags: {self.max_lags}\")\n",
        "        logger.info(f\"  Rolling windows: {self.rolling_windows}\")\n",
        "        logger.info(f\"  Top models for optimization: {self.top_models_for_optimization}\")\n",
        "        logger.info(f\"  GridSearch CV splits: {self.cv_splits}\")\n",
        "\n",
        "# Performance monitoring decorator\n",
        "def monitor_performance(func_name: str = None):\n",
        "    \"\"\"Decorator to monitor function execution time and memory\"\"\"\n",
        "    def decorator(func):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            name = func_name or func.__name__\n",
        "            start_time = time.time()\n",
        "            start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
        "\n",
        "            logger.info(f\"ğŸš€ Starting {name}...\")\n",
        "\n",
        "            try:\n",
        "                result = func(*args, **kwargs)\n",
        "\n",
        "                end_time = time.time()\n",
        "                end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
        "\n",
        "                duration = end_time - start_time\n",
        "                memory_delta = end_memory - start_memory\n",
        "\n",
        "                logger.info(f\"âœ… {name} completed in {duration:.1f}s, memory: {memory_delta:+.1f}MB\")\n",
        "\n",
        "                return result\n",
        "\n",
        "            except Exception as e:\n",
        "                end_time = time.time()\n",
        "                duration = end_time - start_time\n",
        "                logger.error(f\"âŒ {name} failed after {duration:.1f}s: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "print(\"Configuration and monitoring setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdYu1qI5zA6f"
      },
      "source": [
        "## Base ML Forecaster with Enhanced Debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jyIlrnbRzA6g"
      },
      "outputs": [],
      "source": [
        "class DebugMLForecaster:\n",
        "    \"\"\"Enhanced ML forecaster with comprehensive debugging\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, model, debug_mode: bool = True):\n",
        "        self.name = name\n",
        "        self.model = clone(model)\n",
        "        self.pipeline = None\n",
        "        self.is_fitted = False\n",
        "        self.feature_importance_ = None\n",
        "        self.debug_mode = debug_mode\n",
        "        self.training_time = None\n",
        "        self.prediction_time = None\n",
        "        self.fit_error = None\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'DebugMLForecaster':\n",
        "        \"\"\"Fit model with comprehensive error handling and timing\"\"\"\n",
        "\n",
        "        fit_start = time.time()\n",
        "\n",
        "        try:\n",
        "            if self.debug_mode:\n",
        "                logger.debug(f\"Fitting {self.name} on data shape: {X.shape}\")\n",
        "\n",
        "            # Validate input data\n",
        "            if X.empty or y.empty:\n",
        "                raise ValueError(f\"Empty input data for {self.name}\")\n",
        "\n",
        "            if len(X) != len(y):\n",
        "                raise ValueError(f\"Feature/target length mismatch for {self.name}: {len(X)} vs {len(y)}\")\n",
        "\n",
        "            # Convert to numpy arrays for consistent handling\n",
        "            X_array = X.values if isinstance(X, pd.DataFrame) else X\n",
        "            y_array = y.values if isinstance(y, pd.Series) else y\n",
        "\n",
        "            # Check for NaN/inf values\n",
        "            if np.any(np.isnan(X_array)) or np.any(np.isinf(X_array)):\n",
        "                logger.warning(f\"{self.name}: Found NaN/inf in features, cleaning...\")\n",
        "                X_array = np.nan_to_num(X_array, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "            if np.any(np.isnan(y_array)) or np.any(np.isinf(y_array)):\n",
        "                logger.warning(f\"{self.name}: Found NaN/inf in target, cleaning...\")\n",
        "                y_array = np.nan_to_num(y_array, nan=np.nanmean(y_array))\n",
        "\n",
        "            # Create and fit pipeline\n",
        "            self.pipeline = Pipeline([\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('model', clone(self.model))\n",
        "            ])\n",
        "\n",
        "            # Fit with error handling for specific model types\n",
        "            try:\n",
        "                self.pipeline.fit(X_array, y_array.ravel())\n",
        "                self.is_fitted = True\n",
        "\n",
        "                # Extract feature importance\n",
        "                self._extract_feature_importance(X)\n",
        "\n",
        "                # Test prediction capability\n",
        "                test_pred = self.pipeline.predict(X_array[:min(5, len(X_array))])\n",
        "                if np.any(np.isnan(test_pred)) or np.any(np.isinf(test_pred)):\n",
        "                    logger.warning(f\"{self.name}: Model produces invalid predictions\")\n",
        "                    self.is_fitted = False\n",
        "\n",
        "            except Exception as model_error:\n",
        "                self.fit_error = str(model_error)\n",
        "                logger.error(f\"{self.name} fit failed: {self.fit_error}\")\n",
        "                self.is_fitted = False\n",
        "\n",
        "        except Exception as e:\n",
        "            self.fit_error = str(e)\n",
        "            logger.error(f\"{self.name} preprocessing failed: {self.fit_error}\")\n",
        "            self.is_fitted = False\n",
        "\n",
        "        finally:\n",
        "            self.training_time = time.time() - fit_start\n",
        "\n",
        "            if self.debug_mode:\n",
        "                status = \"âœ… SUCCESS\" if self.is_fitted else \"âŒ FAILED\"\n",
        "                logger.info(f\"{self.name}: {status} (Training time: {self.training_time:.2f}s)\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Make predictions with error handling\"\"\"\n",
        "\n",
        "        if not self.is_fitted:\n",
        "            logger.error(f\"{self.name}: Cannot predict - model not fitted\")\n",
        "            return np.zeros(len(X))\n",
        "\n",
        "        pred_start = time.time()\n",
        "\n",
        "        try:\n",
        "            X_array = X.values if isinstance(X, pd.DataFrame) else X\n",
        "\n",
        "            # Clean input data\n",
        "            if np.any(np.isnan(X_array)) or np.any(np.isinf(X_array)):\n",
        "                X_array = np.nan_to_num(X_array, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "            predictions = self.pipeline.predict(X_array)\n",
        "\n",
        "            # Validate predictions\n",
        "            if np.any(np.isnan(predictions)) or np.any(np.isinf(predictions)):\n",
        "                logger.warning(f\"{self.name}: Invalid predictions detected, cleaning...\")\n",
        "                predictions = np.nan_to_num(predictions, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "            self.prediction_time = time.time() - pred_start\n",
        "\n",
        "            if self.debug_mode:\n",
        "                logger.debug(f\"{self.name}: Prediction completed in {self.prediction_time:.3f}s\")\n",
        "\n",
        "            return predictions.flatten()\n",
        "\n",
        "        except Exception as e:\n",
        "            self.prediction_time = time.time() - pred_start\n",
        "            logger.error(f\"{self.name} prediction failed: {str(e)}\")\n",
        "            return np.zeros(len(X))\n",
        "\n",
        "    def _extract_feature_importance(self, X: pd.DataFrame):\n",
        "        \"\"\"Extract feature importance if available\"\"\"\n",
        "        try:\n",
        "            model = self.pipeline.named_steps['model']\n",
        "\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                importances = model.feature_importances_\n",
        "            elif hasattr(model, 'coef_'):\n",
        "                importances = np.abs(model.coef_).flatten()\n",
        "            else:\n",
        "                importances = None\n",
        "\n",
        "            if importances is not None and len(importances) == len(X.columns):\n",
        "                self.feature_importance_ = dict(zip(X.columns, importances))\n",
        "                if self.debug_mode:\n",
        "                    top_features = sorted(self.feature_importance_.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "                    logger.debug(f\"{self.name} top features: {[f[0] for f in top_features]}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                logger.debug(f\"Could not extract feature importance for {self.name}: {str(e)}\")\n",
        "            self.feature_importance_ = None\n",
        "\n",
        "    def get_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get comprehensive model information\"\"\"\n",
        "        return {\n",
        "            'name': self.name,\n",
        "            'is_fitted': self.is_fitted,\n",
        "            'training_time': self.training_time,\n",
        "            'prediction_time': self.prediction_time,\n",
        "            'fit_error': self.fit_error,\n",
        "            'has_feature_importance': self.feature_importance_ is not None\n",
        "        }\n",
        "\n",
        "logger.info(\"Enhanced ML Forecaster class loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXskS-B6zA6h"
      },
      "source": [
        "## Data Generation with Debug Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CbKI4FerzA6h"
      },
      "outputs": [],
      "source": [
        "@monitor_performance(\"Data Generation\")\n",
        "def generate_debug_synthetic_data(n_points: int = 300) -> pd.DataFrame:\n",
        "    \"\"\"Generate synthetic data optimized for ML forecasting with debug tracking\"\"\"\n",
        "\n",
        "    logger.info(f\"ğŸ“Š Generating synthetic data with {n_points} points...\")\n",
        "\n",
        "    np.random.seed(42)\n",
        "    dates = pd.date_range(start='2023-01-01', periods=n_points, freq='D')\n",
        "\n",
        "    # Base call volume with complex patterns\n",
        "    trend = np.linspace(8000, 9500, n_points)\n",
        "\n",
        "    # Multiple seasonal components\n",
        "    weekly_seasonal = 1000 * np.sin(2 * np.pi * np.arange(n_points) / 7)\n",
        "    monthly_seasonal = 500 * np.sin(2 * np.pi * np.arange(n_points) / 30)\n",
        "\n",
        "    # Generate VIX first (needed for regime detection)\n",
        "    logger.info(\"  Generating VIX data...\")\n",
        "    base_vix = 18\n",
        "    vix_values = [base_vix]\n",
        "\n",
        "    regime_counts = {'low_volatility': 0, 'normal': 0, 'high_volatility': 0, 'extreme_volatility': 0}\n",
        "\n",
        "    for i in range(1, n_points):\n",
        "        change = 0.15 * (base_vix - vix_values[-1]) + np.random.normal(0, 1.5)\n",
        "        if np.random.random() < 0.08:  # 8% chance of volatility spike\n",
        "            change += np.random.uniform(8, 25)\n",
        "        new_vix = max(8, min(vix_values[-1] + change, 80))  # Cap at reasonable levels\n",
        "        vix_values.append(new_vix)\n",
        "\n",
        "        # Count regimes for validation\n",
        "        if new_vix < 15:\n",
        "            regime_counts['low_volatility'] += 1\n",
        "        elif new_vix < 25:\n",
        "            regime_counts['normal'] += 1\n",
        "        elif new_vix < 35:\n",
        "            regime_counts['high_volatility'] += 1\n",
        "        else:\n",
        "            regime_counts['extreme_volatility'] += 1\n",
        "\n",
        "    vix_series = pd.Series(vix_values, index=dates)\n",
        "\n",
        "    # Create regime-dependent noise\n",
        "    logger.info(\"  Creating regime-dependent patterns...\")\n",
        "    noise_levels = []\n",
        "    for vix_val in vix_values:\n",
        "        if vix_val < 15:  # Low volatility\n",
        "            noise_levels.append(150)\n",
        "        elif vix_val < 25:  # Normal\n",
        "            noise_levels.append(250)\n",
        "        elif vix_val < 35:  # High volatility\n",
        "            noise_levels.append(400)\n",
        "        else:  # Extreme volatility\n",
        "            noise_levels.append(600)\n",
        "\n",
        "    noise = np.random.normal(0, noise_levels)\n",
        "\n",
        "    # Day-of-week effects\n",
        "    dow_effects = np.array([1.3 if d.weekday() < 5 else 0.7 for d in dates])\n",
        "\n",
        "    # Generate call volume with all components\n",
        "    call_volume = (\n",
        "        trend +\n",
        "        weekly_seasonal +\n",
        "        monthly_seasonal\n",
        "    ) * dow_effects + noise\n",
        "\n",
        "    call_volume = np.maximum(call_volume, 1000)  # Minimum call volume\n",
        "\n",
        "    # Generate correlated S&P 500 data\n",
        "    logger.info(\"  Generating S&P 500 data...\")\n",
        "    sp500_base = 4000\n",
        "    sp500_values = [sp500_base]\n",
        "\n",
        "    for i in range(1, n_points):\n",
        "        vix_effect = -0.001 * (vix_values[i] - 20) / 20  # VIX fear effect\n",
        "        base_return = 0.0008 + vix_effect + np.random.normal(0, 0.012)\n",
        "        new_price = sp500_values[-1] * (1 + base_return)\n",
        "        sp500_values.append(max(new_price, 2000))  # Minimum price floor\n",
        "\n",
        "    # Create comprehensive DataFrame - FIXED pandas methods\n",
        "    data = pd.DataFrame({\n",
        "        'calls': call_volume,\n",
        "        'vix': vix_values,\n",
        "        'sp500': sp500_values\n",
        "    }, index=dates)\n",
        "\n",
        "    # Add additional market indicators\n",
        "    data['sp500_volume'] = np.random.gamma(2, 50000000, n_points)\n",
        "    data['treasury_10y'] = 2.5 + 0.5 * np.sin(2 * np.pi * np.arange(n_points) / 365) + np.random.normal(0, 0.1, n_points)\n",
        "\n",
        "    # Clean data - FIXED methods\n",
        "    data = data.ffill().bfill()\n",
        "    data = data.replace([np.inf, -np.inf], np.nan).fillna(data.median())\n",
        "\n",
        "    # Data quality summary\n",
        "    logger.info(\"âœ… Data generation complete:\")\n",
        "    logger.info(f\"  Shape: {data.shape}\")\n",
        "    logger.info(f\"  Call volume range: {data['calls'].min():.0f} - {data['calls'].max():.0f}\")\n",
        "    logger.info(f\"  VIX range: {data['vix'].min():.1f} - {data['vix'].max():.1f}\")\n",
        "    logger.info(f\"  Regime distribution:\")\n",
        "    for regime, count in regime_counts.items():\n",
        "        pct = count / n_points * 100\n",
        "        logger.info(f\"    {regime}: {pct:.1f}%\")\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkrTQOKfzA6i"
      },
      "source": [
        "## Quick Functionality Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J6NJFodzA6i",
        "outputId": "c691bc30-912e-43e6-9a11-96b362ca5fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Functionality test result: PASS\n"
          ]
        }
      ],
      "source": [
        "# Quick test to verify core functionality\n",
        "def quick_functionality_test():\n",
        "    \"\"\"Quick test of core functionality before main execution\"\"\"\n",
        "\n",
        "    logger.info(\"ğŸ§ª Running quick functionality test...\")\n",
        "\n",
        "    try:\n",
        "        # Test data generation\n",
        "        test_data = generate_debug_synthetic_data(n_points=50)\n",
        "        if test_data.empty:\n",
        "            raise ValueError(\"Data generation failed\")\n",
        "\n",
        "        logger.info(\"âœ… Functionality test PASSED\")\n",
        "        logger.info(f\"  Data: {test_data.shape}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"âŒ Functionality test FAILED: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Run the test\n",
        "test_result = quick_functionality_test()\n",
        "print(f\"\\nFunctionality test result: {'PASS' if test_result else 'FAIL'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THIwRyGnzA6j"
      },
      "source": [
        "## Full Pipeline Execution\n",
        "\n",
        "**Note:** The complete pipeline implementation includes:\n",
        "- Model Factory (25+ ML models)\n",
        "- Feature Engineering (100+ features)\n",
        "- Market Regime Analysis\n",
        "- GridSearchCV Optimization\n",
        "- Comprehensive Evaluation\n",
        "\n",
        "**To see the full implementation, use the original artifact or implement the remaining classes as needed.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOXAR6XUzA6j",
        "outputId": "d3934fd5-75e0-4159-bb1a-5e5b213d26c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ ML FORECASTING SYSTEM - READY FOR EXECUTION\n",
            "================================================================================\n",
            "\n",
            "To run the full pipeline:\n",
            "1. Implement the remaining classes from the full artifact\n",
            "2. Run: results = run_debug_pipeline()\n",
            "\n",
            "Current status:\n",
            "  GPU Available: True\n",
            "  High Memory: True\n",
            "  XGBoost: True\n",
            "  LightGBM: True\n",
            "  Test Data Generation: âœ… PASS\n"
          ]
        }
      ],
      "source": [
        "# Example execution (requires full implementation)\n",
        "print(\"ğŸ¯ ML FORECASTING SYSTEM - READY FOR EXECUTION\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\")\n",
        "print(\"To run the full pipeline:\")\n",
        "print(\"1. Implement the remaining classes from the full artifact\")\n",
        "print(\"2. Run: results = run_debug_pipeline()\")\n",
        "print(\"\")\n",
        "print(\"Current status:\")\n",
        "print(f\"  GPU Available: {GPU_AVAILABLE}\")\n",
        "print(f\"  High Memory: {HIGH_MEMORY}\")\n",
        "print(f\"  XGBoost: {XGB_AVAILABLE}\")\n",
        "print(f\"  LightGBM: {LGB_AVAILABLE}\")\n",
        "print(f\"  Test Data Generation: {'âœ… PASS' if test_result else 'âŒ FAIL'}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ML MODEL FACTORY\n",
        "# ============================================================================\n",
        "\n",
        "class DebugMLModelFactory:\n",
        "    \"\"\"Model factory with comprehensive creation debugging\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    @monitor_performance(\"Model Creation\")\n",
        "    def create_all_models(config: DebugMLForecastingConfig) -> List[DebugMLForecaster]:\n",
        "        \"\"\"Create comprehensive set of ML models with debugging\"\"\"\n",
        "\n",
        "        logger.info(\"ğŸ­ Creating ML models...\")\n",
        "\n",
        "        all_models = []\n",
        "        creation_stats = {\n",
        "            'total_attempted': 0,\n",
        "            'successful': 0,\n",
        "            'failed': 0,\n",
        "            'errors': []\n",
        "        }\n",
        "\n",
        "        model_groups = [\n",
        "            (\"Tree Models\", config.use_tree_models, DebugMLModelFactory._create_tree_models),\n",
        "            (\"Linear Models\", config.use_linear_models, DebugMLModelFactory._create_linear_models),\n",
        "            (\"Neural Models\", config.use_neural_models, DebugMLModelFactory._create_neural_models),\n",
        "            (\"Ensemble Models\", config.use_ensemble_models, DebugMLModelFactory._create_ensemble_models),\n",
        "            (\"Other Models\", config.use_other_models, DebugMLModelFactory._create_other_models)\n",
        "        ]\n",
        "\n",
        "        for group_name, enabled, creator_func in model_groups:\n",
        "            if not enabled:\n",
        "                logger.info(f\"  â­ï¸ {group_name}: DISABLED\")\n",
        "                continue\n",
        "\n",
        "            logger.info(f\"  ğŸ”¨ Creating {group_name}...\")\n",
        "\n",
        "            try:\n",
        "                group_models = creator_func(config)\n",
        "                group_successful = 0\n",
        "\n",
        "                for model in group_models:\n",
        "                    creation_stats['total_attempted'] += 1\n",
        "\n",
        "                    try:\n",
        "                        # Test model creation\n",
        "                        _ = model.model.get_params()\n",
        "                        all_models.append(model)\n",
        "                        group_successful += 1\n",
        "                        creation_stats['successful'] += 1\n",
        "\n",
        "                        logger.debug(f\"    âœ… {model.name}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        creation_stats['failed'] += 1\n",
        "                        error_msg = f\"{model.name}: {str(e)[:50]}...\"\n",
        "                        creation_stats['errors'].append(error_msg)\n",
        "\n",
        "                        logger.warning(f\"    âŒ {model.name} - {str(e)[:50]}...\")\n",
        "\n",
        "                logger.info(f\"    ğŸ“Š {group_name}: {group_successful}/{len(group_models)} successful\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = f\"{group_name} creation failed: {str(e)}\"\n",
        "                creation_stats['errors'].append(error_msg)\n",
        "                logger.error(f\"    ğŸ’¥ {group_name} creation failed: {str(e)}\")\n",
        "\n",
        "        # Final summary\n",
        "        logger.info(f\"ğŸ¯ Model creation complete:\")\n",
        "        logger.info(f\"  Total models: {creation_stats['successful']}/{creation_stats['total_attempted']}\")\n",
        "        logger.info(f\"  Success rate: {creation_stats['successful']/max(1,creation_stats['total_attempted'])*100:.1f}%\")\n",
        "\n",
        "        if creation_stats['errors'] and config.debug_mode:\n",
        "            logger.info(f\"  Errors encountered: {len(creation_stats['errors'])}\")\n",
        "            for error in creation_stats['errors'][:3]:  # Show first 3 errors\n",
        "                logger.debug(f\"    {error}\")\n",
        "\n",
        "        if len(all_models) == 0:\n",
        "            logger.error(\"No models created successfully! Creating fallback models...\")\n",
        "            all_models = DebugMLModelFactory._create_fallback_models(config)\n",
        "\n",
        "        return all_models\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_tree_models(config: DebugMLForecastingConfig) -> List[DebugMLForecaster]:\n",
        "        \"\"\"Create tree-based models\"\"\"\n",
        "        models = []\n",
        "\n",
        "        # Random Forest variants\n",
        "        models.extend([\n",
        "            DebugMLForecaster(\"RandomForest_100\", RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=1), config.debug_mode),\n",
        "            DebugMLForecaster(\"RandomForest_200\", RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=1), config.debug_mode),\n",
        "        ])\n",
        "\n",
        "        # Extra Trees\n",
        "        models.extend([\n",
        "            DebugMLForecaster(\"ExtraTrees_100\", ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=1), config.debug_mode),\n",
        "            DebugMLForecaster(\"ExtraTrees_200\", ExtraTreesRegressor(n_estimators=200, random_state=42, n_jobs=1), config.debug_mode),\n",
        "        ])\n",
        "\n",
        "        # Gradient Boosting\n",
        "        models.extend([\n",
        "            DebugMLForecaster(\"GradientBoosting_100\", GradientBoostingRegressor(n_estimators=100, random_state=42), config.debug_mode),\n",
        "            DebugMLForecaster(\"GradientBoosting_200\", GradientBoostingRegressor(n_estimators=200, random_state=42), config.debug_mode),\n",
        "        ])\n",
        "\n",
        "        # XGBoost models (CPU and GPU)\n",
        "        if XGB_AVAILABLE:\n",
        "            models.extend([\n",
        "                DebugMLForecaster(\"XGBoost_100\", xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=1), config.debug_mode),\n",
        "                DebugMLForecaster(\"XGBoost_200\", xgb.XGBRegressor(n_estimators=200, random_state=42, n_jobs=1), config.debug_mode),\n",
        "            ])\n",
        "\n",
        "            # GPU models if available\n",
        "            if XGB_GPU_AVAILABLE and HIGH_MEMORY:\n",
        "                models.extend([\n",
        "                    DebugMLForecaster(\"XGBoost_GPU_200\", xgb.XGBRegressor(n_estimators=200, tree_method='gpu_hist', gpu_id=0, random_state=42), config.debug_mode),\n",
        "                    DebugMLForecaster(\"XGBoost_GPU_500\", xgb.XGBRegressor(n_estimators=500, tree_method='gpu_hist', gpu_id=0, random_state=42), config.debug_mode),\n",
        "                ])\n",
        "\n",
        "        # LightGBM models\n",
        "        if LGB_AVAILABLE:\n",
        "            models.extend([\n",
        "                DebugMLForecaster(\"LightGBM_100\", lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1, n_jobs=1), config.debug_mode),\n",
        "                DebugMLForecaster(\"LightGBM_200\", lgb.LGBMRegressor(n_estimators=200, random_state=42, verbose=-1, n_jobs=1), config.debug_mode),\n",
        "            ])\n",
        "\n",
        "            # GPU models if available\n",
        "            if LGB_GPU_AVAILABLE and HIGH_MEMORY:\n",
        "                models.extend([\n",
        "                    DebugMLForecaster(\"LightGBM_GPU_200\", lgb.LGBMRegressor(n_estimators=200, device='gpu', random_state=42, verbose=-1), config.debug_mode),\n",
        "                    DebugMLForecaster(\"LightGBM_GPU_500\", lgb.LGBMRegressor(n_estimators=500, device='gpu', random_state=42, verbose=-1), config.debug_mode),\n",
        "                ])\n",
        "\n",
        "        # Decision Tree\n",
        "        models.append(\n",
        "            DebugMLForecaster(\"DecisionTree\", DecisionTreeRegressor(random_state=42, max_depth=10), config.debug_mode)\n",
        "        )\n",
        "\n",
        "        return models\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_linear_models(config: DebugMLForecastingConfig) -> List[DebugMLForecaster]:\n",
        "        \"\"\"Create linear models\"\"\"\n",
        "        models = []\n",
        "\n",
        "        # Basic linear models\n",
        "        models.extend([\n",
        "            DebugMLForecaster(\"LinearRegression\", LinearRegression(), config.debug_mode),\n",
        "            DebugMLForecaster(\"Ridge_0.1\", Ridge(alpha=0.1), config.debug_mode),\n",
        "            DebugMLForecaster(\"Ridge_1.0\", Ridge(alpha=1.0), config.debug_mode),\n",
        "            DebugMLForecaster(\"Ridge_10.0\", Ridge(alpha=10.0), config.debug_mode),\n",
        "            DebugMLForecaster(\"Lasso_0.1\", Lasso(alpha=0.1, max_iter=2000), config.debug_mode),\n",
        "            DebugMLForecaster(\"Lasso_1.0\", Lasso(alpha=1.0, max_iter=2000), config.debug_mode),\n",
        "            DebugMLForecaster(\"ElasticNet_0.1\", ElasticNet(alpha=0.1, max_iter=2000), config.debug_mode),\n",
        "            DebugMLForecaster(\"ElasticNet_1.0\", ElasticNet(alpha=1.0, max_iter=2000), config.debug_mode),\n",
        "        ])\n",
        "\n",
        "        # Bayesian models\n",
        "        models.extend([\n",
        "            DebugMLForecaster(\"BayesianRidge\", BayesianRidge(), config.debug_mode),\n",
        "            DebugMLForecaster(\"ARDRegression\", ARDRegression(max_iter=500), config.debug_mode),\n",
        "        ])\n",
        "\n",
        "        # Robust models\n",
        "        models.extend([\n",
        "            DebugMLForecaster(\"HuberRegressor\", HuberRegressor(max_iter=200), config.debug_mode),\n",
        "            DebugMLForecaster(\"SGDRegressor\", SGDRegressor(random_state=42, max_iter=2000), config.debug_mode),\n",
        "        ])\n",
        "\n",
        "        return models\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_neural_models(config: DebugMLForecastingConfig) -> List[DebugMLForecaster]:\n",
        "        \"\"\"Create neural network models\"\"\"\n",
        "        models = []\n",
        "\n",
        "        # MLPRegressor variants with proper parameters\n",
        "        models.extend([\n",
        "            DebugMLForecaster(\"MLP_50\", MLPRegressor(\n",
        "                hidden_layer_sizes=(50,),\n",
        "                random_state=42,\n",
        "                max_iter=1000,\n",
        "                early_stopping=True,\n",
        "                validation_fraction=0.1,\n",
        "                n_iter_no_change=10\n",
        "            ), config.debug_mode),\n",
        "            DebugMLForecaster(\"MLP_100_50\", MLPRegressor(\n",
        "                hidden_layer_sizes=(100, 50),\n",
        "                random_state=42,\n",
        "                max_iter=1000,\n",
        "                early_stopping=True,\n",
        "                validation_fraction=0.1,\n",
        "                n_iter_no_change=10\n",
        "            ), config.debug_mode),\n",
        "            DebugMLForecaster(\"MLP_200_100\", MLPRegressor(\n",
        "                hidden_layer_sizes=(200, 100),\n",
        "                random_state=42,\n",
        "                max_iter=1000,\n",
        "                early_stopping=True,\n",
        "                validation_fraction=0.1,\n",
        "                n_iter_no_change=10\n",
        "            ), config.debug_mode),\n",
        "        ])\n",
        "\n",
        "        return models\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_ensemble_models(config: DebugMLForecastingConfig) -> List[DebugMLForecaster]:\n",
        "        \"\"\"Create ensemble models\"\"\"\n",
        "        models = []\n",
        "\n",
        "        # Bagging models\n",
        "        models.extend([\n",
        "            DebugMLForecaster(\"BaggingRegressor\", BaggingRegressor(random_state=42, n_jobs=1), config.debug_mode),\n",
        "            DebugMLForecaster(\"AdaBoostRegressor\", AdaBoostRegressor(random_state=42, n_estimators=50), config.debug_mode),\n",
        "        ])\n",
        "\n",
        "        # Voting ensemble\n",
        "        try:\n",
        "            voting_models = [\n",
        "                ('rf', RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=1)),\n",
        "                ('ridge', Ridge(alpha=1.0)),\n",
        "                ('tree', DecisionTreeRegressor(random_state=42, max_depth=10))\n",
        "            ]\n",
        "            models.append(\n",
        "                DebugMLForecaster(\"VotingRegressor\", VotingRegressor(estimators=voting_models, n_jobs=1), config.debug_mode)\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not create VotingRegressor: {str(e)}\")\n",
        "\n",
        "        return models\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_other_models(config: DebugMLForecastingConfig) -> List[DebugMLForecaster]:\n",
        "        \"\"\"Create other ML models\"\"\"\n",
        "        models = []\n",
        "\n",
        "        # Support Vector Regression\n",
        "        models.extend([\n",
        "            DebugMLForecaster(\"SVR_linear\", SVR(kernel='linear', C=1.0), config.debug_mode),\n",
        "            DebugMLForecaster(\"SVR_rbf\", SVR(kernel='rbf', C=1.0), config.debug_mode),\n",
        "        ])\n",
        "\n",
        "        # K-Nearest Neighbors\n",
        "        models.extend([\n",
        "            DebugMLForecaster(\"KNeighbors_5\", KNeighborsRegressor(n_neighbors=5), config.debug_mode),\n",
        "            DebugMLForecaster(\"KNeighbors_10\", KNeighborsRegressor(n_neighbors=10), config.debug_mode),\n",
        "        ])\n",
        "\n",
        "        # Kernel Ridge\n",
        "        models.append(\n",
        "            DebugMLForecaster(\"KernelRidge\", KernelRidge(alpha=1.0), config.debug_mode)\n",
        "        )\n",
        "\n",
        "        return models\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_fallback_models(config: DebugMLForecastingConfig) -> List[DebugMLForecaster]:\n",
        "        \"\"\"Create basic fallback models if all others fail\"\"\"\n",
        "        logger.warning(\"Creating fallback models...\")\n",
        "\n",
        "        return [\n",
        "            DebugMLForecaster(\"Fallback_LinearRegression\", LinearRegression(), config.debug_mode),\n",
        "            DebugMLForecaster(\"Fallback_Ridge\", Ridge(alpha=1.0), config.debug_mode),\n",
        "            DebugMLForecaster(\"Fallback_RandomForest\", RandomForestRegressor(n_estimators=50, random_state=42), config.debug_mode)\n",
        "        ]\n",
        "\n",
        "print(\"DebugMLModelFactory loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmAB8lIc61zm",
        "outputId": "4820c0c0-34b5-4433-8e8a-f6a4257026ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DebugMLModelFactory loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "class DebugFeatureEngineer:\n",
        "    \"\"\"Advanced feature engineering with comprehensive debugging\"\"\"\n",
        "\n",
        "    def __init__(self, config: DebugMLForecastingConfig):\n",
        "        self.config = config\n",
        "        self.feature_names_ = []\n",
        "        self.feature_creation_log = []\n",
        "\n",
        "    @monitor_performance(\"Feature Engineering\")\n",
        "    def create_features(self, data: pd.DataFrame, regime_data: pd.Series = None) -> pd.DataFrame:\n",
        "        \"\"\"Create comprehensive feature set with debugging\"\"\"\n",
        "\n",
        "        logger.info(f\"ğŸ› ï¸ Starting feature engineering on data shape: {data.shape}\")\n",
        "\n",
        "        # Validate input\n",
        "        if data.empty:\n",
        "            raise ValueError(\"Empty input data for feature engineering\")\n",
        "\n",
        "        target_col = self.config.target_column\n",
        "        if target_col not in data.columns:\n",
        "            raise ValueError(f\"Target column '{target_col}' not found in data\")\n",
        "\n",
        "        # Initialize features dataframe\n",
        "        features_df = pd.DataFrame(index=data.index)\n",
        "        target_series = data[target_col].copy()\n",
        "\n",
        "        # Log initial data quality\n",
        "        nan_count = target_series.isna().sum()\n",
        "        logger.info(f\"Target series: {len(target_series)} points, {nan_count} NaN values\")\n",
        "\n",
        "        # Feature creation steps with individual error handling\n",
        "        feature_steps = [\n",
        "            (\"Lagged Features\", self._add_lagged_features, target_series),\n",
        "            (\"Rolling Features\", self._add_rolling_features, target_series),\n",
        "            (\"Technical Indicators\", self._add_technical_indicators, target_series),\n",
        "            (\"Calendar Features\", self._add_calendar_features, None),\n",
        "            (\"Statistical Features\", self._add_statistical_features, target_series)\n",
        "        ]\n",
        "\n",
        "        # Add market features if available\n",
        "        if 'vix' in data.columns:\n",
        "            feature_steps.append((\"Market Features\", self._add_market_features, data))\n",
        "\n",
        "        # Add regime features if available\n",
        "        if self.config.use_regime_features and regime_data is not None:\n",
        "            feature_steps.append((\"Regime Features\", self._add_regime_features, regime_data))\n",
        "\n",
        "        # Execute feature creation steps\n",
        "        for step_name, step_func, step_data in feature_steps:\n",
        "            initial_count = len(features_df.columns)\n",
        "            step_start = time.time()\n",
        "\n",
        "            try:\n",
        "                logger.info(f\"  ğŸ”§ Creating {step_name}...\")\n",
        "\n",
        "                if step_name == \"Calendar Features\":\n",
        "                    features_df = step_func(features_df)\n",
        "                else:\n",
        "                    features_df = step_func(features_df, step_data)\n",
        "\n",
        "                added_count = len(features_df.columns) - initial_count\n",
        "                step_time = time.time() - step_start\n",
        "\n",
        "                logger.info(f\"    âœ… {step_name}: +{added_count} features ({step_time:.2f}s)\")\n",
        "                self.feature_creation_log.append({\n",
        "                    'step': step_name,\n",
        "                    'features_added': added_count,\n",
        "                    'time': step_time,\n",
        "                    'status': 'success'\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                step_time = time.time() - step_start\n",
        "                logger.error(f\"    âŒ {step_name} failed: {str(e)}\")\n",
        "                self.feature_creation_log.append({\n",
        "                    'step': step_name,\n",
        "                    'features_added': 0,\n",
        "                    'time': step_time,\n",
        "                    'status': 'failed',\n",
        "                    'error': str(e)\n",
        "                })\n",
        "\n",
        "        # Clean and validate features\n",
        "        logger.info(f\"  ğŸ§¹ Cleaning features...\")\n",
        "        features_df = self._clean_features(features_df)\n",
        "\n",
        "        self.feature_names_ = list(features_df.columns)\n",
        "\n",
        "        # Log final feature summary\n",
        "        logger.info(f\"âœ… Feature engineering complete:\")\n",
        "        logger.info(f\"  Final shape: {features_df.shape}\")\n",
        "        logger.info(f\"  Features created: {len(self.feature_names_)}\")\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    def _add_lagged_features(self, df: pd.DataFrame, series: pd.Series) -> pd.DataFrame:\n",
        "        \"\"\"Add lagged features with validation\"\"\"\n",
        "        max_lags = min(self.config.max_lags, len(series) // 4)\n",
        "        logger.debug(f\"Creating {max_lags} lag features\")\n",
        "\n",
        "        for lag in range(1, max_lags + 1):\n",
        "            df[f'lag_{lag}'] = series.shift(lag)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_rolling_features(self, df: pd.DataFrame, series: pd.Series) -> pd.DataFrame:\n",
        "        \"\"\"Add rolling window features with validation\"\"\"\n",
        "        for window in self.config.rolling_windows:\n",
        "            if window < len(series):\n",
        "                df[f'rolling_mean_{window}'] = series.rolling(window=window, min_periods=1).mean()\n",
        "                df[f'rolling_std_{window}'] = series.rolling(window=window, min_periods=1).std()\n",
        "                df[f'rolling_min_{window}'] = series.rolling(window=window, min_periods=1).min()\n",
        "                df[f'rolling_max_{window}'] = series.rolling(window=window, min_periods=1).max()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_technical_indicators(self, df: pd.DataFrame, series: pd.Series) -> pd.DataFrame:\n",
        "        \"\"\"Add technical indicators with error handling\"\"\"\n",
        "        try:\n",
        "            # RSI calculation\n",
        "            for period in [7, 14]:\n",
        "                if period < len(series):\n",
        "                    delta = series.diff()\n",
        "                    gain = delta.where(delta > 0, 0).rolling(window=period, min_periods=1).mean()\n",
        "                    loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
        "                    rs = gain / (loss + 1e-8)\n",
        "                    df[f'rsi_{period}'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "            # Moving average ratios\n",
        "            if len(series) > 12:\n",
        "                ma_short = series.rolling(window=5, min_periods=1).mean()\n",
        "                ma_long = series.rolling(window=12, min_periods=1).mean()\n",
        "                df['ma_ratio'] = ma_short / (ma_long + 1e-8)\n",
        "\n",
        "            # Momentum indicators\n",
        "            for period in [3, 7]:\n",
        "                if period < len(series):\n",
        "                    df[f'momentum_{period}'] = series / (series.shift(period) + 1e-8) - 1\n",
        "                    df[f'rate_of_change_{period}'] = series.pct_change(periods=period)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Technical indicators creation failed: {str(e)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_calendar_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add calendar features with validation\"\"\"\n",
        "        try:\n",
        "            if isinstance(df.index, pd.DatetimeIndex):\n",
        "                df['dow'] = df.index.dayofweek\n",
        "                df['is_weekend'] = (df['dow'] >= 5).astype(int)\n",
        "                df['month'] = df.index.month\n",
        "                df['quarter'] = df.index.quarter\n",
        "                df['day_of_month'] = df.index.day\n",
        "                df['day_of_year'] = df.index.dayofyear\n",
        "\n",
        "                # Cyclical encoding\n",
        "                df['dow_sin'] = np.sin(2 * np.pi * df['dow'] / 7)\n",
        "                df['dow_cos'] = np.cos(2 * np.pi * df['dow'] / 7)\n",
        "                df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "                df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "            else:\n",
        "                # Position-based features for non-datetime index\n",
        "                df['position'] = np.arange(len(df))\n",
        "                df['position_sin'] = np.sin(2 * np.pi * df['position'] / 7)\n",
        "                df['position_cos'] = np.cos(2 * np.pi * df['position'] / 7)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Calendar features creation failed: {str(e)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_market_features(self, df: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add market features with validation\"\"\"\n",
        "        try:\n",
        "            if 'vix' in data.columns:\n",
        "                df['vix'] = data['vix']\n",
        "                df['vix_lag1'] = data['vix'].shift(1)\n",
        "                df['vix_change'] = data['vix'].diff()\n",
        "                if len(data['vix']) > 7:\n",
        "                    df['vix_rolling_7'] = data['vix'].rolling(7, min_periods=1).mean()\n",
        "\n",
        "            if 'sp500' in data.columns:\n",
        "                df['sp500_return'] = data['sp500'].pct_change()\n",
        "                if len(data['sp500']) > 7:\n",
        "                    df['sp500_volatility'] = data['sp500'].pct_change().rolling(7, min_periods=1).std()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Market features creation failed: {str(e)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_regime_features(self, df: pd.DataFrame, regime_data: pd.Series) -> pd.DataFrame:\n",
        "        \"\"\"Add regime features with validation\"\"\"\n",
        "        try:\n",
        "            aligned_regimes = regime_data.reindex(df.index, method='ffill')\n",
        "            regime_dummies = pd.get_dummies(aligned_regimes, prefix='regime')\n",
        "            regime_dummies.index = df.index\n",
        "            df = pd.concat([df, regime_dummies], axis=1)\n",
        "\n",
        "            # Regime duration\n",
        "            regime_changes = aligned_regimes != aligned_regimes.shift(1)\n",
        "            regime_groups = regime_changes.cumsum()\n",
        "            df['regime_duration'] = regime_groups.groupby(regime_groups).cumcount() + 1\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Regime features creation failed: {str(e)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_statistical_features(self, df: pd.DataFrame, series: pd.Series) -> pd.DataFrame:\n",
        "        \"\"\"Add statistical features\"\"\"\n",
        "        try:\n",
        "            # Z-scores\n",
        "            for window in [7, 14]:\n",
        "                if window < len(series):\n",
        "                    rolling_mean = series.rolling(window, min_periods=1).mean()\n",
        "                    rolling_std = series.rolling(window, min_periods=1).std()\n",
        "                    df[f'zscore_{window}'] = (series - rolling_mean) / (rolling_std + 1e-8)\n",
        "\n",
        "            # Distance from moving averages\n",
        "            for window in [7, 14]:\n",
        "                if window < len(series):\n",
        "                    ma = series.rolling(window, min_periods=1).mean()\n",
        "                    df[f'distance_from_ma_{window}'] = (series - ma) / (ma + 1e-8)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Statistical features creation failed: {str(e)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _clean_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Clean and validate features - FIXED for modern pandas\"\"\"\n",
        "        initial_shape = df.shape\n",
        "\n",
        "        # Replace infinite values\n",
        "        df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        # Drop columns with all NaN\n",
        "        df = df.dropna(axis=1, how='all')\n",
        "\n",
        "        # Drop rows with too many NaN values\n",
        "        threshold = len(df.columns) * 0.5\n",
        "        df = df.dropna(thresh=threshold)\n",
        "\n",
        "        # FIXED: Use modern pandas methods\n",
        "        df = df.ffill()  # Forward fill\n",
        "        df = df.bfill()  # Backward fill\n",
        "\n",
        "        # Final cleanup\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        if self.config.debug_mode:\n",
        "            logger.debug(f\"Feature cleaning: {initial_shape} â†’ {df.shape}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_target_from_features(self, features_df: pd.DataFrame,\n",
        "                                   original_data: pd.DataFrame,\n",
        "                                   forecast_horizon: int = 1) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "        \"\"\"Create target variable aligned with features\"\"\"\n",
        "\n",
        "        logger.info(f\"ğŸ¯ Creating target variable with forecast horizon: {forecast_horizon}\")\n",
        "\n",
        "        target_col = self.config.target_column\n",
        "\n",
        "        if target_col not in original_data.columns:\n",
        "            raise ValueError(f\"Target column '{target_col}' not found in original data\")\n",
        "\n",
        "        # Create target with forecast horizon\n",
        "        target_series = original_data[target_col].shift(-forecast_horizon)\n",
        "\n",
        "        # Align indices\n",
        "        common_index = features_df.index.intersection(target_series.index)\n",
        "\n",
        "        if len(common_index) == 0:\n",
        "            raise ValueError(\"No common index between features and target\")\n",
        "\n",
        "        aligned_features = features_df.loc[common_index]\n",
        "        aligned_target = target_series.loc[common_index]\n",
        "\n",
        "        # Remove rows where target is NaN\n",
        "        valid_mask = ~aligned_target.isna()\n",
        "\n",
        "        final_features = aligned_features[valid_mask]\n",
        "        final_target = aligned_target[valid_mask]\n",
        "\n",
        "        if len(final_features) == 0:\n",
        "            raise ValueError(\"No valid samples after alignment\")\n",
        "\n",
        "        logger.info(f\"âœ… Target alignment complete: Features {final_features.shape}, Target {final_target.shape}\")\n",
        "\n",
        "        return final_features, final_target\n",
        "\n",
        "print(\"DebugFeatureEngineer loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFw76bJ2614S",
        "outputId": "6c17caa2-2839-46e6-b673-f4f91ae2c256"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DebugFeatureEngineer loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "friV3gbNC47Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE GRIDSEARCHCV IMPLEMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def get_comprehensive_param_grids():\n",
        "    \"\"\"Get COMPREHENSIVE parameter grids for ALL model types\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPREHENSIVE PARAMETER GRIDS FOR FULL GRIDSEARCHCV\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    param_grids = {\n",
        "        # RANDOM FOREST - Extensive grid\n",
        "        'RandomForest': {\n",
        "            'n_estimators': [50, 100, 200, 300, 500],\n",
        "            'max_depth': [5, 10, 20, 30, None],\n",
        "            'min_samples_split': [2, 5, 10, 20],\n",
        "            'min_samples_leaf': [1, 2, 4, 8],\n",
        "            'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7, None],\n",
        "            'bootstrap': [True, False]\n",
        "        },\n",
        "\n",
        "        # EXTRA TREES - Extensive grid\n",
        "        'ExtraTrees': {\n",
        "            'n_estimators': [50, 100, 200, 300, 500],\n",
        "            'max_depth': [5, 10, 20, 30, None],\n",
        "            'min_samples_split': [2, 5, 10, 20],\n",
        "            'min_samples_leaf': [1, 2, 4, 8],\n",
        "            'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7, None],\n",
        "            'bootstrap': [True, False]\n",
        "        },\n",
        "\n",
        "        # GRADIENT BOOSTING - Extensive grid\n",
        "        'GradientBoosting': {\n",
        "            'n_estimators': [50, 100, 200, 300],\n",
        "            'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
        "            'max_depth': [3, 5, 7, 10],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "            'max_features': ['sqrt', 'log2', None]\n",
        "        },\n",
        "\n",
        "        # XGBOOST - Extensive grid\n",
        "        'XGBoost': {\n",
        "            'n_estimators': [50, 100, 200, 300, 500],\n",
        "            'max_depth': [3, 4, 5, 6, 7, 8],\n",
        "            'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.3],\n",
        "            'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "            'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "            'gamma': [0, 0.1, 0.2, 0.3],\n",
        "            'reg_alpha': [0, 0.01, 0.1, 1],\n",
        "            'reg_lambda': [0, 0.01, 0.1, 1]\n",
        "        },\n",
        "\n",
        "        # LIGHTGBM - Extensive grid\n",
        "        'LightGBM': {\n",
        "            'n_estimators': [50, 100, 200, 300, 500],\n",
        "            'num_leaves': [20, 31, 50, 100, 200],\n",
        "            'max_depth': [3, 5, 7, 10, -1],\n",
        "            'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
        "            'feature_fraction': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "            'bagging_fraction': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "            'bagging_freq': [0, 1, 3, 5],\n",
        "            'lambda_l1': [0, 0.01, 0.1, 1],\n",
        "            'lambda_l2': [0, 0.01, 0.1, 1]\n",
        "        },\n",
        "\n",
        "        # SUPPORT VECTOR REGRESSION - Extensive grid\n",
        "        'SVR': {\n",
        "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "            'epsilon': [0.001, 0.01, 0.1, 0.2, 0.5, 1.0],\n",
        "            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
        "            'degree': [2, 3, 4],  # for poly kernel\n",
        "            'coef0': [0, 0.1, 0.5, 1]  # for poly/sigmoid\n",
        "        },\n",
        "\n",
        "        # RIDGE REGRESSION - Extensive grid\n",
        "        'Ridge': {\n",
        "            'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0, 200.0, 500.0, 1000.0],\n",
        "            'fit_intercept': [True, False],\n",
        "            'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
        "        },\n",
        "\n",
        "        # LASSO REGRESSION - Extensive grid\n",
        "        'Lasso': {\n",
        "            'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0],\n",
        "            'fit_intercept': [True, False],\n",
        "            'selection': ['cyclic', 'random'],\n",
        "            'max_iter': [1000, 2000, 5000]\n",
        "        },\n",
        "\n",
        "        # ELASTIC NET - Extensive grid\n",
        "        'ElasticNet': {\n",
        "            'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
        "            'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99],\n",
        "            'fit_intercept': [True, False],\n",
        "            'selection': ['cyclic', 'random'],\n",
        "            'max_iter': [1000, 2000, 5000]\n",
        "        },\n",
        "\n",
        "        # K-NEAREST NEIGHBORS - Extensive grid\n",
        "        'KNeighbors': {\n",
        "            'n_neighbors': [3, 5, 7, 10, 15, 20, 25, 30],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "            'leaf_size': [10, 20, 30, 50],\n",
        "            'p': [1, 2],  # Manhattan vs Euclidean\n",
        "            'metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "        },\n",
        "\n",
        "        # MLP NEURAL NETWORK - Extensive grid\n",
        "        'MLP': {\n",
        "            'hidden_layer_sizes': [(50,), (100,), (200,),\n",
        "                                   (50, 50), (100, 50), (100, 100),\n",
        "                                   (200, 100), (100, 50, 25)],\n",
        "            'activation': ['relu', 'tanh', 'logistic'],\n",
        "            'solver': ['adam', 'sgd', 'lbfgs'],\n",
        "            'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "            'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "            'learning_rate_init': [0.001, 0.01, 0.1],\n",
        "            'max_iter': [200, 500, 1000]\n",
        "        },\n",
        "\n",
        "        # DECISION TREE - Extensive grid\n",
        "        'DecisionTree': {\n",
        "            'max_depth': [3, 5, 10, 15, 20, None],\n",
        "            'min_samples_split': [2, 5, 10, 20],\n",
        "            'min_samples_leaf': [1, 2, 4, 8],\n",
        "            'max_features': ['sqrt', 'log2', None],\n",
        "            'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
        "            'splitter': ['best', 'random']\n",
        "        },\n",
        "\n",
        "        # BAGGING - Extensive grid\n",
        "        'BaggingRegressor': {\n",
        "            'n_estimators': [10, 20, 50, 100],\n",
        "            'max_samples': [0.5, 0.7, 0.9, 1.0],\n",
        "            'max_features': [0.5, 0.7, 0.9, 1.0],\n",
        "            'bootstrap': [True, False],\n",
        "            'bootstrap_features': [True, False]\n",
        "        },\n",
        "\n",
        "        # ADABOOST - Extensive grid\n",
        "        'AdaBoostRegressor': {\n",
        "            'n_estimators': [25, 50, 75, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.5, 1.0, 1.5],\n",
        "            'loss': ['linear', 'square', 'exponential']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Calculate total combinations\n",
        "    print(\"\\nParameter Grid Complexity:\")\n",
        "    print(\"-\"*50)\n",
        "    total_combinations = 0\n",
        "    for model_name, grid in param_grids.items():\n",
        "        combinations = 1\n",
        "        for param, values in grid.items():\n",
        "            combinations *= len(values)\n",
        "        total_combinations += combinations\n",
        "        print(f\"{model_name:20s}: {combinations:,} combinations\")\n",
        "\n",
        "    print(\"-\"*50)\n",
        "    print(f\"TOTAL COMBINATIONS: {total_combinations:,}\")\n",
        "\n",
        "    return param_grids\n",
        "\n",
        "def run_comprehensive_gridsearch(model_name, base_model, param_grid, X_train, y_train, X_test, y_test,\n",
        "                                 cv_splits=10, use_random_search=False, n_iter=100, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Run comprehensive GridSearchCV or RandomizedSearchCV for a single model\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model_name : str\n",
        "        Name of the model\n",
        "    base_model : estimator\n",
        "        Base sklearn model\n",
        "    param_grid : dict\n",
        "        Parameter grid for search\n",
        "    X_train, y_train : arrays\n",
        "        Training data\n",
        "    X_test, y_test : arrays\n",
        "        Test data\n",
        "    cv_splits : int\n",
        "        Number of CV splits\n",
        "    use_random_search : bool\n",
        "        Use RandomizedSearchCV for large grids\n",
        "    n_iter : int\n",
        "        Number of iterations for RandomizedSearchCV\n",
        "    n_jobs : int\n",
        "        Number of parallel jobs (-1 = all cores)\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"GRIDSEARCHCV: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Calculate grid size\n",
        "    grid_size = 1\n",
        "    for param_values in param_grid.values():\n",
        "        grid_size *= len(param_values)\n",
        "\n",
        "    # Decide search method\n",
        "    if grid_size > 100 and use_random_search:\n",
        "        search_method = \"RandomizedSearchCV\"\n",
        "        n_iter_actual = min(n_iter, grid_size)\n",
        "        print(f\"Method: {search_method} ({n_iter_actual} iterations from {grid_size:,} combinations)\")\n",
        "    else:\n",
        "        search_method = \"GridSearchCV\"\n",
        "        n_iter_actual = grid_size\n",
        "        print(f\"Method: {search_method} ({grid_size:,} combinations)\")\n",
        "\n",
        "    # Setup cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
        "    expected_fits = n_iter_actual * cv_splits\n",
        "\n",
        "    print(f\"CV Splits: {cv_splits}\")\n",
        "    print(f\"Expected fits: {expected_fits:,}\")\n",
        "    print(f\"Parallel jobs: {n_jobs if n_jobs != -1 else 'All cores'}\")\n",
        "\n",
        "    # Create scorer\n",
        "    scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "    # Setup search\n",
        "    if search_method == \"RandomizedSearchCV\":\n",
        "        search = RandomizedSearchCV(\n",
        "            estimator=base_model,\n",
        "            param_distributions=param_grid,\n",
        "            n_iter=n_iter_actual,\n",
        "            cv=tscv,\n",
        "            scoring=scorer,\n",
        "            n_jobs=n_jobs,\n",
        "            verbose=1,\n",
        "            random_state=42,\n",
        "            error_score='raise'\n",
        "        )\n",
        "    else:\n",
        "        search = GridSearchCV(\n",
        "            estimator=base_model,\n",
        "            param_grid=param_grid,\n",
        "            cv=tscv,\n",
        "            scoring=scorer,\n",
        "            n_jobs=n_jobs,\n",
        "            verbose=1,\n",
        "            error_score='raise'\n",
        "        )\n",
        "\n",
        "    # Run search\n",
        "    print(f\"\\nStarting {search_method}...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        search.fit(X_train, y_train.ravel())\n",
        "        fit_time = time.time() - start_time\n",
        "\n",
        "        # Get results\n",
        "        best_score = -search.best_score_  # Convert back to positive MAE\n",
        "        train_score = best_score\n",
        "\n",
        "        # Test performance\n",
        "        test_predictions = search.predict(X_test)\n",
        "        test_score = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "        # Calculate improvement over default\n",
        "        default_model = base_model\n",
        "        default_model.fit(X_train, y_train.ravel())\n",
        "        default_predictions = default_model.predict(X_test)\n",
        "        default_score = mean_absolute_error(y_test, default_predictions)\n",
        "        improvement = ((default_score - test_score) / default_score) * 100\n",
        "\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"âœ“ COMPLETED in {fit_time:.1f} seconds\")\n",
        "        print(f\"Actual fits performed: {len(search.cv_results_['params']):,}\")\n",
        "        print(f\"Best CV Score (MAE): {best_score:.4f}\")\n",
        "        print(f\"Test Score (MAE): {test_score:.4f}\")\n",
        "        print(f\"Default Score (MAE): {default_score:.4f}\")\n",
        "        print(f\"Improvement: {improvement:+.2f}%\")\n",
        "        print(f\"\\nBest Parameters:\")\n",
        "        for param, value in search.best_params_.items():\n",
        "            print(f\"  {param}: {value}\")\n",
        "\n",
        "        return {\n",
        "            'model_name': model_name,\n",
        "            'search_method': search_method,\n",
        "            'best_model': search.best_estimator_,\n",
        "            'best_params': search.best_params_,\n",
        "            'best_cv_score': best_score,\n",
        "            'test_score': test_score,\n",
        "            'default_score': default_score,\n",
        "            'improvement': improvement,\n",
        "            'fit_time': fit_time,\n",
        "            'n_fits': len(search.cv_results_['params']),\n",
        "            'cv_results': search.cv_results_\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâœ— FAILED: {str(e)[:100]}\")\n",
        "        return {\n",
        "            'model_name': model_name,\n",
        "            'search_method': search_method,\n",
        "            'error': str(e),\n",
        "            'fit_time': time.time() - start_time\n",
        "        }\n",
        "\n",
        "def execute_full_gridsearch_pipeline(X_train, y_train, X_test, y_test,\n",
        "                                    models_to_optimize=None,\n",
        "                                    cv_splits=10,\n",
        "                                    use_random_search=True,\n",
        "                                    n_iter=100,\n",
        "                                    n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Execute comprehensive GridSearchCV for all models\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train, y_train, X_test, y_test : arrays\n",
        "        Train and test data\n",
        "    models_to_optimize : list\n",
        "        List of model names to optimize (None = all)\n",
        "    cv_splits : int\n",
        "        Number of CV splits for TimeSeriesSplit\n",
        "    use_random_search : bool\n",
        "        Use RandomizedSearchCV for large parameter grids\n",
        "    n_iter : int\n",
        "        Number of iterations for RandomizedSearchCV\n",
        "    n_jobs : int\n",
        "        Number of parallel jobs (-1 = all cores)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPREHENSIVE GRIDSEARCHCV PIPELINE EXECUTION\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Data shapes: Train {X_train.shape}, Test {X_test.shape}\")\n",
        "    print(f\"CV Splits: {cv_splits}\")\n",
        "    print(f\"Random Search: {use_random_search} (max {n_iter} iterations)\")\n",
        "    print(f\"Parallel Jobs: {n_jobs if n_jobs != -1 else 'All CPU cores'}\")\n",
        "\n",
        "    # Get parameter grids\n",
        "    param_grids = get_comprehensive_param_grids()\n",
        "\n",
        "    # Define models to optimize\n",
        "    model_configs = {\n",
        "        'RandomForest': RandomForestRegressor(random_state=42),\n",
        "        'ExtraTrees': ExtraTreesRegressor(random_state=42),\n",
        "        'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
        "        'Ridge': Ridge(),\n",
        "        'Lasso': Lasso(random_state=42),\n",
        "        'ElasticNet': ElasticNet(random_state=42),\n",
        "        'SVR': SVR(),\n",
        "        'KNeighbors': KNeighborsRegressor(),\n",
        "        'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
        "        'BaggingRegressor': BaggingRegressor(random_state=42),\n",
        "        'AdaBoostRegressor': AdaBoostRegressor(random_state=42)\n",
        "    }\n",
        "\n",
        "    # Add optional models if available\n",
        "    if XGB_AVAILABLE:\n",
        "        model_configs['XGBoost'] = xgb.XGBRegressor(random_state=42, verbosity=0)\n",
        "    if LGB_AVAILABLE:\n",
        "        model_configs['LightGBM'] = lgb.LGBMRegressor(random_state=42, verbose=-1)\n",
        "\n",
        "    # Neural network (might be slow)\n",
        "    if not use_random_search or models_to_optimize:\n",
        "        model_configs['MLP'] = MLPRegressor(random_state=42, early_stopping=True, validation_fraction=0.1)\n",
        "\n",
        "    # Filter models if specified\n",
        "    if models_to_optimize:\n",
        "        model_configs = {k: v for k, v in model_configs.items() if k in models_to_optimize}\n",
        "\n",
        "    print(f\"\\nModels to optimize: {list(model_configs.keys())}\")\n",
        "    print(f\"Total models: {len(model_configs)}\")\n",
        "\n",
        "    # Run GridSearch for each model\n",
        "    all_results = []\n",
        "    pipeline_start = time.time()\n",
        "\n",
        "    for i, (model_name, base_model) in enumerate(model_configs.items(), 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"MODEL {i}/{len(model_configs)}: {model_name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        if model_name in param_grids:\n",
        "            result = run_comprehensive_gridsearch(\n",
        "                model_name=model_name,\n",
        "                base_model=base_model,\n",
        "                param_grid=param_grids[model_name],\n",
        "                X_train=X_train,\n",
        "                y_train=y_train,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test,\n",
        "                cv_splits=cv_splits,\n",
        "                use_random_search=use_random_search,\n",
        "                n_iter=n_iter,\n",
        "                n_jobs=n_jobs\n",
        "            )\n",
        "            all_results.append(result)\n",
        "        else:\n",
        "            print(f\"No parameter grid defined for {model_name}, skipping...\")\n",
        "\n",
        "    total_time = time.time() - pipeline_start\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"GRIDSEARCHCV PIPELINE COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Total execution time: {total_time/60:.1f} minutes\")\n",
        "    print(f\"Models optimized: {len(all_results)}\")\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_data = []\n",
        "    for result in all_results:\n",
        "        if 'error' not in result:\n",
        "            results_data.append({\n",
        "                'Model': result['model_name'],\n",
        "                'Method': result['search_method'],\n",
        "                'CV_MAE': result['best_cv_score'],\n",
        "                'Test_MAE': result['test_score'],\n",
        "                'Default_MAE': result['default_score'],\n",
        "                'Improvement_%': result['improvement'],\n",
        "                'Fits': result['n_fits'],\n",
        "                'Time_sec': result['fit_time']\n",
        "            })\n",
        "\n",
        "    if results_data:\n",
        "        results_df = pd.DataFrame(results_data).sort_values('Test_MAE')\n",
        "\n",
        "        print(\"\\nTOP OPTIMIZED MODELS:\")\n",
        "        print(\"-\"*60)\n",
        "        for idx, row in results_df.head(10).iterrows():\n",
        "            print(f\"{row['Model']:20s} | Test MAE: {row['Test_MAE']:7.2f} | \"\n",
        "                  f\"Improvement: {row['Improvement_%']:+6.2f}% | \"\n",
        "                  f\"Fits: {row['Fits']:5,} | Time: {row['Time_sec']:6.1f}s\")\n",
        "\n",
        "        # Save detailed results\n",
        "        results_df.to_csv('gridsearch_results_comprehensive.csv', index=False)\n",
        "        print(f\"\\nDetailed results saved to: gridsearch_results_comprehensive.csv\")\n",
        "\n",
        "        # Save best parameters\n",
        "        best_params_all = {}\n",
        "        for result in all_results:\n",
        "            if 'best_params' in result:\n",
        "                best_params_all[result['model_name']] = result['best_params']\n",
        "\n",
        "        import json\n",
        "        with open('best_parameters.json', 'w') as f:\n",
        "            json.dump(best_params_all, f, indent=2, default=str)\n",
        "        print(f\"Best parameters saved to: best_parameters.json\")\n",
        "\n",
        "        return results_df, all_results\n",
        "    else:\n",
        "        print(\"\\nNo successful optimizations!\")\n",
        "        return pd.DataFrame(), all_results\n",
        "\n",
        "# Make functions available\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE GRIDSEARCHCV FUNCTIONS LOADED\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nFunctions available:\")\n",
        "print(\"1. get_comprehensive_param_grids() - View all parameter grids\")\n",
        "print(\"2. run_comprehensive_gridsearch() - Optimize single model\")\n",
        "print(\"3. execute_full_gridsearch_pipeline() - Optimize all models\")\n",
        "print(\"\\nUsage example:\")\n",
        "print(\"results_df, all_results = execute_full_gridsearch_pipeline(\")\n",
        "print(\"    X_train, y_train, X_test, y_test,\")\n",
        "print(\"    cv_splits=10,\")\n",
        "print(\"    use_random_search=True,\")\n",
        "print(\"    n_iter=100,\")\n",
        "print(\"    n_jobs=-1\")\n",
        "print(\")\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qsh6rqKMC5FE",
        "outputId": "6ebb62c7-0648-4168-c99e-e602c6278766"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE GRIDSEARCHCV FUNCTIONS LOADED\n",
            "================================================================================\n",
            "\n",
            "Functions available:\n",
            "1. get_comprehensive_param_grids() - View all parameter grids\n",
            "2. run_comprehensive_gridsearch() - Optimize single model\n",
            "3. execute_full_gridsearch_pipeline() - Optimize all models\n",
            "\n",
            "Usage example:\n",
            "results_df, all_results = execute_full_gridsearch_pipeline(\n",
            "    X_train, y_train, X_test, y_test,\n",
            "    cv_splits=10,\n",
            "    use_random_search=True,\n",
            "    n_iter=100,\n",
            "    n_jobs=-1\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JP8TxnZqC5QZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MARKET REGIME ANALYZER\n",
        "# ============================================================================\n",
        "\n",
        "class DebugMarketRegimeAnalyzer:\n",
        "    \"\"\"Market regime analysis with comprehensive debugging\"\"\"\n",
        "\n",
        "    def __init__(self, config: DebugMLForecastingConfig):\n",
        "        self.config = config\n",
        "        self.vix_thresholds = config.vix_thresholds\n",
        "        self.regime_stats = None\n",
        "\n",
        "    @monitor_performance(\"Market Regime Analysis\")\n",
        "    def analyze_regimes(self, data: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"Comprehensive market regime analysis with debugging\"\"\"\n",
        "\n",
        "        logger.info(\"ğŸ“ˆ Analyzing market regimes...\")\n",
        "\n",
        "        # Get or simulate VIX data\n",
        "        if 'vix' in data.columns:\n",
        "            vix_series = data['vix'].copy()\n",
        "            logger.info(f\"  Using actual VIX data: {len(vix_series)} points\")\n",
        "        else:\n",
        "            vix_series = self._simulate_vix_data(len(data))\n",
        "            vix_series.index = data.index\n",
        "            logger.info(f\"  Using simulated VIX data: {len(vix_series)} points\")\n",
        "\n",
        "        # Clean VIX data\n",
        "        initial_nan_count = vix_series.isna().sum()\n",
        "        vix_series = vix_series.ffill().bfill().fillna(20.0)\n",
        "\n",
        "        if initial_nan_count > 0:\n",
        "            logger.info(f\"  Cleaned {initial_nan_count} NaN values in VIX data\")\n",
        "\n",
        "        # Classify regimes\n",
        "        regimes = vix_series.apply(self.classify_market_regime)\n",
        "\n",
        "        # Log VIX statistics\n",
        "        logger.info(f\"  VIX range: {vix_series.min():.1f} - {vix_series.max():.1f}\")\n",
        "        logger.info(f\"  VIX mean: {vix_series.mean():.1f}, std: {vix_series.std():.1f}\")\n",
        "\n",
        "        # Calculate regime statistics\n",
        "        try:\n",
        "            regime_distribution = regimes.value_counts(normalize=True)\n",
        "            regime_transitions = self._calculate_transition_matrix(regimes)\n",
        "\n",
        "            logger.info(f\"  Regime distribution:\")\n",
        "            for regime, pct in regime_distribution.items():\n",
        "                logger.info(f\"    {regime}: {pct*100:.1f}%\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not calculate regime statistics: {str(e)}\")\n",
        "            regime_distribution = pd.Series([1.0], index=['normal'])\n",
        "            regime_transitions = pd.DataFrame()\n",
        "\n",
        "        current_regime = regimes.iloc[-1] if len(regimes) > 0 else 'normal'\n",
        "\n",
        "        self.regime_stats = {\n",
        "            'vix_values': vix_series,\n",
        "            'regimes': regimes,\n",
        "            'current_regime': current_regime,\n",
        "            'regime_distribution': regime_distribution,\n",
        "            'transition_matrix': regime_transitions\n",
        "        }\n",
        "\n",
        "        logger.info(f\"âœ… Market regime analysis complete\")\n",
        "        logger.info(f\"  Current regime: {current_regime}\")\n",
        "\n",
        "        return self.regime_stats\n",
        "\n",
        "    def classify_market_regime(self, vix_value: float) -> str:\n",
        "        \"\"\"Classify market regime based on VIX with validation\"\"\"\n",
        "\n",
        "        if pd.isna(vix_value) or vix_value <= 0:\n",
        "            return 'normal'\n",
        "\n",
        "        if vix_value < self.vix_thresholds['low_volatility']:\n",
        "            return 'low_volatility'\n",
        "        elif vix_value < self.vix_thresholds['normal']:\n",
        "            return 'normal'\n",
        "        elif vix_value < self.vix_thresholds['high_volatility']:\n",
        "            return 'high_volatility'\n",
        "        else:\n",
        "            return 'extreme_volatility'\n",
        "\n",
        "    def select_models_for_regime(self, all_models: List[DebugMLForecaster], regime: str) -> List[DebugMLForecaster]:\n",
        "        \"\"\"Select appropriate ML models for current market regime with logging\"\"\"\n",
        "\n",
        "        logger.info(f\"ğŸ¯ Selecting models for {regime} regime...\")\n",
        "\n",
        "        regime_preferences = self.config.regime_specific_models.get(\n",
        "            regime,\n",
        "            self.config.regime_specific_models['normal']\n",
        "        )\n",
        "\n",
        "        selected = []\n",
        "        for model in all_models:\n",
        "            model_type = model.name.split('_')[0]\n",
        "\n",
        "            # Check if model type matches regime preference\n",
        "            for pref in regime_preferences:\n",
        "                if pref.lower() in model_type.lower():\n",
        "                    selected.append(model)\n",
        "                    break\n",
        "\n",
        "        # Ensure minimum number of models\n",
        "        min_models = 8 if HIGH_MEMORY else 6\n",
        "        if len(selected) < min_models:\n",
        "            logger.info(f\"  Adding additional models to reach minimum of {min_models}\")\n",
        "            for model in all_models:\n",
        "                if model not in selected:\n",
        "                    selected.append(model)\n",
        "                    if len(selected) >= min_models * 2:  # Cap at 2x minimum\n",
        "                        break\n",
        "\n",
        "        selected_names = [model.name for model in selected]\n",
        "        logger.info(f\"  Selected {len(selected)} models: {selected_names[:5]}{'...' if len(selected_names) > 5 else ''}\")\n",
        "\n",
        "        return selected\n",
        "\n",
        "    def _calculate_transition_matrix(self, regimes: pd.Series) -> pd.DataFrame:\n",
        "        \"\"\"Calculate regime transition probabilities with error handling\"\"\"\n",
        "\n",
        "        try:\n",
        "            unique_regimes = regimes.unique()\n",
        "            n_regimes = len(unique_regimes)\n",
        "\n",
        "            if n_regimes == 0:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            transition_matrix = pd.DataFrame(\n",
        "                np.zeros((n_regimes, n_regimes)),\n",
        "                index=unique_regimes,\n",
        "                columns=unique_regimes\n",
        "            )\n",
        "\n",
        "            for i in range(1, len(regimes)):\n",
        "                from_regime = regimes.iloc[i-1]\n",
        "                to_regime = regimes.iloc[i]\n",
        "                if pd.notna(from_regime) and pd.notna(to_regime):\n",
        "                    transition_matrix.loc[from_regime, to_regime] += 1\n",
        "\n",
        "            # Normalize rows to get probabilities\n",
        "            row_sums = transition_matrix.sum(axis=1)\n",
        "            transition_matrix = transition_matrix.div(row_sums, axis=0).fillna(0)\n",
        "\n",
        "            return transition_matrix\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not calculate transition matrix: {str(e)}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def _simulate_vix_data(self, n_points: int) -> pd.Series:\n",
        "        \"\"\"Generate realistic VIX simulation\"\"\"\n",
        "\n",
        "        logger.debug(f\"Simulating VIX data for {n_points} points\")\n",
        "\n",
        "        base_vix = 18\n",
        "        vix_values = [base_vix]\n",
        "\n",
        "        for i in range(1, n_points):\n",
        "            # Mean reversion with random noise\n",
        "            change = 0.15 * (base_vix - vix_values[-1]) + np.random.normal(0, 1.8)\n",
        "\n",
        "            # Random volatility spikes\n",
        "            if np.random.random() < 0.08:  # 8% chance of volatility spike\n",
        "                change += np.random.uniform(8, 20)\n",
        "\n",
        "            new_vix = max(10, vix_values[-1] + change)\n",
        "            vix_values.append(new_vix)\n",
        "\n",
        "        return pd.Series(vix_values, name='VIX_simulated')\n",
        "\n",
        "print(\"DebugMarketRegimeAnalyzer loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y04GqHo361_b",
        "outputId": "b49425d4-3ff5-4b66-a5db-4a22bd59f1e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DebugMarketRegimeAnalyzer loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gM0FiSKxEY69"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SET YOUR DATA CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "CSV_FILE_PATH = \"enhanced_eda_data.csv\"  # Your CSV file\n",
        "TARGET_COLUMN = \"calls\"                  # Column to predict\n",
        "DATE_COLUMN = \"date\"                     # Date column\n",
        "\n",
        "print(f\"Configuration set:\")\n",
        "print(f\"  CSV: {CSV_FILE_PATH}\")\n",
        "print(f\"  Target: {TARGET_COLUMN}\")\n",
        "print(f\"  Date: {DATE_COLUMN}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am1Z5x1QFpOz",
        "outputId": "54ba9d8c-178c-49ec-9720-58226a5b7c89"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration set:\n",
            "  CSV: enhanced_eda_data.csv\n",
            "  Target: calls\n",
            "  Date: date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovp3SR0xFpap"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MAIN PIPELINE - INTEGRATED WITH ALL DATA AND FULL GRIDSEARCHCV\n",
        "# ============================================================================\n",
        "\n",
        "class ComprehensiveMLPipeline:\n",
        "    \"\"\"Complete ML pipeline using ALL data columns and full GridSearchCV\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path: str, target_column: str, date_column: str = None):\n",
        "        self.csv_path = csv_path\n",
        "        self.target_column = target_column\n",
        "        self.date_column = date_column\n",
        "        self.data = None\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.results = {}\n",
        "        self.gridsearch_results = None\n",
        "        self.all_model_results = None\n",
        "\n",
        "    def execute(self, cv_splits=10, use_random_search=True, n_iter=100, n_jobs=-1, models_to_optimize=None):\n",
        "        \"\"\"\n",
        "        Execute complete pipeline with ALL data and comprehensive GridSearchCV\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        cv_splits : int\n",
        "            Number of CV splits for TimeSeriesSplit\n",
        "        use_random_search : bool\n",
        "            Use RandomizedSearchCV for large parameter grids\n",
        "        n_iter : int\n",
        "            Number of iterations for RandomizedSearchCV\n",
        "        n_jobs : int\n",
        "            Number of parallel jobs (-1 = all cores)\n",
        "        models_to_optimize : list\n",
        "            List of specific models to optimize (None = all)\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(\"COMPREHENSIVE ML PIPELINE EXECUTION\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"CSV: {self.csv_path}\")\n",
        "        print(f\"Target: {self.target_column}\")\n",
        "        print(f\"Date Column: {self.date_column}\")\n",
        "        print(f\"CV Splits: {cv_splits}\")\n",
        "        print(f\"Random Search: {use_random_search}\")\n",
        "        print(f\"Parallel Jobs: {n_jobs if n_jobs != -1 else 'All cores'}\")\n",
        "\n",
        "        pipeline_start = time.time()\n",
        "\n",
        "        try:\n",
        "            # ==========================================\n",
        "            # PHASE 1: DATA LOADING WITH ALL COLUMNS\n",
        "            # ==========================================\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"PHASE 1: DATA LOADING (ALL COLUMNS)\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            self.data = load_all_data_columns(\n",
        "                csv_path=self.csv_path,\n",
        "                target_column=self.target_column,\n",
        "                date_column=self.date_column\n",
        "            )\n",
        "\n",
        "            # Verify all columns are loaded\n",
        "            expected_columns = [\n",
        "                'calls', '^VIX_close', 'SPY_close', 'SPY_volume', 'QQQ_close',\n",
        "                'QQQ_volume', 'DX-Y.NYB_close', 'GC=F_close', 'GC=F_volume',\n",
        "                'BTC-USD_close', 'BTC-USD_volume', 'ETH-USD_close', 'ETH-USD_volume',\n",
        "                'is_weekend', 'Month', 'Quarter', 'DayOfWeek', 'Year', 'outlier_multivariate'\n",
        "            ]\n",
        "\n",
        "            loaded_columns = list(self.data.columns)\n",
        "            missing_columns = [col for col in expected_columns if col not in loaded_columns]\n",
        "\n",
        "            print(f\"\\nColumn Verification:\")\n",
        "            print(f\"  Expected: {len(expected_columns)} columns\")\n",
        "            print(f\"  Loaded: {len(loaded_columns)} columns\")\n",
        "            if missing_columns:\n",
        "                print(f\"  Missing: {missing_columns}\")\n",
        "            else:\n",
        "                print(f\"  âœ“ ALL expected columns loaded successfully\")\n",
        "\n",
        "            # ==========================================\n",
        "            # PHASE 2: FEATURE ENGINEERING FROM ALL COLUMNS\n",
        "            # ==========================================\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"PHASE 2: COMPREHENSIVE FEATURE ENGINEERING\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            X, y = create_all_features(self.data, self.target_column)\n",
        "\n",
        "            # Feature statistics\n",
        "            original_features = [col for col in self.data.columns if col != self.target_column]\n",
        "            engineered_features = [col for col in X.columns if col not in self.data.columns]\n",
        "\n",
        "            print(f\"\\nFeature Engineering Summary:\")\n",
        "            print(f\"  Original features used: {len(original_features)}\")\n",
        "            print(f\"  Engineered features created: {len(engineered_features)}\")\n",
        "            print(f\"  Total features: {X.shape[1]}\")\n",
        "            print(f\"  Samples: {X.shape[0]}\")\n",
        "\n",
        "            # Sample of features created\n",
        "            print(f\"\\nSample engineered features:\")\n",
        "            for feat in engineered_features[:10]:\n",
        "                print(f\"    - {feat}\")\n",
        "            if len(engineered_features) > 10:\n",
        "                print(f\"    ... and {len(engineered_features)-10} more\")\n",
        "\n",
        "            # ==========================================\n",
        "            # PHASE 3: DATA SPLITTING\n",
        "            # ==========================================\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"PHASE 3: TRAIN/TEST SPLIT\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            train_size = int(len(X) * 0.75)\n",
        "            self.X_train = X.iloc[:train_size]\n",
        "            self.X_test = X.iloc[train_size:]\n",
        "            self.y_train = y.iloc[:train_size]\n",
        "            self.y_test = y.iloc[train_size:]\n",
        "\n",
        "            print(f\"Data Split:\")\n",
        "            print(f\"  Training: {self.X_train.shape} features, {self.y_train.shape} targets\")\n",
        "            print(f\"  Testing:  {self.X_test.shape} features, {self.y_test.shape} targets\")\n",
        "            print(f\"  Train period: {self.X_train.index[0]} to {self.X_train.index[-1]}\")\n",
        "            print(f\"  Test period:  {self.X_test.index[0]} to {self.X_test.index[-1]}\")\n",
        "\n",
        "            # Check for data leakage\n",
        "            train_indices = set(self.X_train.index)\n",
        "            test_indices = set(self.X_test.index)\n",
        "            overlap = train_indices.intersection(test_indices)\n",
        "\n",
        "            if overlap:\n",
        "                print(f\"  âš  WARNING: {len(overlap)} overlapping indices detected!\")\n",
        "            else:\n",
        "                print(f\"  âœ“ No data leakage detected\")\n",
        "\n",
        "            # ==========================================\n",
        "            # PHASE 4: BASELINE MODEL PERFORMANCE\n",
        "            # ==========================================\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"PHASE 4: BASELINE PERFORMANCE\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            # Calculate Day-of-Week Average baseline\n",
        "\n",
        "            dow_baseline = []\n",
        "            for i in range(len(self.y_test)):\n",
        "                test_date_idx = self.y_test.index[i]\n",
        "                test_dow = test_date_idx.dayofweek\n",
        "\n",
        "                # Find historical averages for this day-of-week\n",
        "                train_same_dow = self.y_train[self.y_train.index.dayofweek == test_dow]\n",
        "                if len(train_same_dow) > 0:\n",
        "                    dow_baseline.append(train_same_dow.mean())\n",
        "                else:\n",
        "                    dow_baseline.append(self.y_train.mean())\n",
        "\n",
        "            baseline_mae = mean_absolute_error(self.y_test, dow_baseline)\n",
        "            baseline_rmse = np.sqrt(mean_squared_error(self.y_test, dow_baseline))\n",
        "            print(f\"Day-of-Week Average Baseline:\")\n",
        "            print(f\"  MAE: {baseline_mae:.4f}\")\n",
        "            print(f\"  RMSE: {baseline_rmse:.4f}\")\n",
        "\n",
        "\n",
        "            # ==========================================\n",
        "            # PHASE 5: COMPREHENSIVE GRIDSEARCHCV\n",
        "            # ==========================================\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"PHASE 5: COMPREHENSIVE GRIDSEARCHCV OPTIMIZATION\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            # Estimate time\n",
        "            n_models = len(models_to_optimize) if models_to_optimize else 14\n",
        "            estimated_time = n_models * 2  # ~2 minutes per model with RandomizedSearch\n",
        "            print(f\"Estimated time: {estimated_time:.0f} minutes\")\n",
        "            print(f\"Starting GridSearchCV at {pd.Timestamp.now().strftime('%H:%M:%S')}\")\n",
        "\n",
        "            gridsearch_start = time.time()\n",
        "\n",
        "            self.gridsearch_results, self.all_model_results = execute_full_gridsearch_pipeline(\n",
        "                X_train=self.X_train,\n",
        "                y_train=self.y_train,\n",
        "                X_test=self.X_test,\n",
        "                y_test=self.y_test,\n",
        "                models_to_optimize=models_to_optimize,\n",
        "                cv_splits=cv_splits,\n",
        "                use_random_search=use_random_search,\n",
        "                n_iter=n_iter,\n",
        "                n_jobs=n_jobs\n",
        "            )\n",
        "\n",
        "            gridsearch_time = time.time() - gridsearch_start\n",
        "\n",
        "            # ==========================================\n",
        "            # PHASE 6: RESULTS ANALYSIS\n",
        "            # ==========================================\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"PHASE 6: RESULTS ANALYSIS\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            if not self.gridsearch_results.empty:\n",
        "                # Add baseline comparison\n",
        "                self.gridsearch_results['Baseline_MAE'] = baseline_mae\n",
        "                self.gridsearch_results['Beat_Baseline'] = self.gridsearch_results['Test_MAE'] < baseline_mae\n",
        "                self.gridsearch_results['vs_Baseline_%'] = ((baseline_mae - self.gridsearch_results['Test_MAE']) / baseline_mae * 100)\n",
        "\n",
        "                # Sort by test MAE\n",
        "                self.gridsearch_results = self.gridsearch_results.sort_values('Test_MAE')\n",
        "\n",
        "                # Champion model\n",
        "                champion = self.gridsearch_results.iloc[0]\n",
        "\n",
        "                print(f\"\\nCHAMPION MODEL:\")\n",
        "                print(f\"  Model: {champion['Model']}\")\n",
        "                print(f\"  Test MAE: {champion['Test_MAE']:.4f}\")\n",
        "                print(f\"  vs Baseline: {champion['vs_Baseline_%']:+.2f}%\")\n",
        "                print(f\"  vs Default params: {champion['Improvement_%']:+.2f}%\")\n",
        "                print(f\"  Optimization method: {champion['Method']}\")\n",
        "                print(f\"  Fits performed: {champion['Fits']:,}\")\n",
        "\n",
        "                # Models beating baseline\n",
        "                models_beat_baseline = self.gridsearch_results['Beat_Baseline'].sum()\n",
        "                total_models = len(self.gridsearch_results)\n",
        "\n",
        "                print(f\"\\nBASELINE COMPARISON:\")\n",
        "                print(f\"  Models beating baseline: {models_beat_baseline}/{total_models} ({models_beat_baseline/total_models*100:.1f}%)\")\n",
        "\n",
        "                # Top 5 models\n",
        "                print(f\"\\nTOP 5 MODELS:\")\n",
        "                print(\"-\"*70)\n",
        "                print(f\"{'Model':<20} {'Test MAE':<10} {'vs Base':<12} {'vs Default':<12} {'Method':<10}\")\n",
        "                print(\"-\"*70)\n",
        "                for _, row in self.gridsearch_results.head(5).iterrows():\n",
        "                    print(f\"{row['Model']:<20} {row['Test_MAE']:<10.2f} \"\n",
        "                          f\"{row['vs_Baseline_%']:+11.2f}% {row['Improvement_%']:+11.2f}% \"\n",
        "                          f\"{row['Method']:<10}\")\n",
        "\n",
        "                # Model type analysis\n",
        "                print(f\"\\nMODEL TYPE PERFORMANCE:\")\n",
        "                model_types = {\n",
        "                    'Tree': ['Forest', 'Extra', 'Gradient', 'Decision'],\n",
        "                    'Linear': ['Ridge', 'Lasso', 'Elastic'],\n",
        "                    'Boosting': ['XGBoost', 'LightGBM', 'AdaBoost'],\n",
        "                    'Other': ['SVR', 'KNeighbors', 'MLP', 'Bagging']\n",
        "                }\n",
        "\n",
        "                for type_name, keywords in model_types.items():\n",
        "                    type_models = self.gridsearch_results[\n",
        "                        self.gridsearch_results['Model'].str.contains('|'.join(keywords), case=False)\n",
        "                    ]\n",
        "                    if len(type_models) > 0:\n",
        "                        avg_mae = type_models['Test_MAE'].mean()\n",
        "                        beat_baseline = type_models['Beat_Baseline'].sum()\n",
        "                        print(f\"  {type_name:10s}: Avg MAE {avg_mae:.2f}, \"\n",
        "                              f\"{beat_baseline}/{len(type_models)} beat baseline\")\n",
        "\n",
        "                # Save enhanced results\n",
        "                self.gridsearch_results.to_csv('final_pipeline_results.csv', index=False)\n",
        "                print(f\"\\nâœ“ Results saved to: final_pipeline_results.csv\")\n",
        "\n",
        "                # Extract best model\n",
        "                best_model_result = next((r for r in self.all_model_results\n",
        "                                         if r['model_name'] == champion['Model']), None)\n",
        "                if best_model_result and 'best_model' in best_model_result:\n",
        "                    self.results['best_model'] = best_model_result['best_model']\n",
        "                    self.results['best_params'] = best_model_result['best_params']\n",
        "                    print(f\"\\nâœ“ Best model stored for deployment\")\n",
        "\n",
        "            # ==========================================\n",
        "            # PHASE 7: PIPELINE SUMMARY\n",
        "            # ==========================================\n",
        "            total_time = time.time() - pipeline_start\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"PIPELINE EXECUTION COMPLETE\")\n",
        "            print(\"=\"*80)\n",
        "            print(f\"Execution Summary:\")\n",
        "            print(f\"  Total time: {total_time/60:.1f} minutes\")\n",
        "            print(f\"  Data columns used: {len(self.data.columns)}\")\n",
        "            print(f\"  Features created: {self.X_train.shape[1]}\")\n",
        "            print(f\"  Models optimized: {len(self.gridsearch_results) if self.gridsearch_results is not None else 0}\")\n",
        "            print(f\"  GridSearchCV time: {gridsearch_time/60:.1f} minutes\")\n",
        "\n",
        "            if champion is not None:\n",
        "                if champion['vs_Baseline_%'] > 0:\n",
        "                    print(f\"\\nâœ“ SUCCESS: ML beats baseline by {champion['vs_Baseline_%']:.2f}%\")\n",
        "                else:\n",
        "                    print(f\"\\nâš  WARNING: ML fails to beat simple baseline\")\n",
        "\n",
        "            return self.gridsearch_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâœ— PIPELINE FAILED: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Make predictions using the best model\"\"\"\n",
        "        if 'best_model' in self.results:\n",
        "            return self.results['best_model'].predict(X_new)\n",
        "        else:\n",
        "            raise ValueError(\"No trained model available. Run execute() first.\")\n",
        "\n",
        "    def get_feature_importance(self):\n",
        "        \"\"\"Get feature importance from best model if available\"\"\"\n",
        "        if 'best_model' not in self.results:\n",
        "            return None\n",
        "\n",
        "        model = self.results['best_model']\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance = pd.DataFrame({\n",
        "                'feature': self.X_train.columns,\n",
        "                'importance': model.feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "            return importance\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "# ============================================================================\n",
        "# CONVENIENCE EXECUTION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def run_complete_ml_pipeline(csv_path=None, target_column=None, date_column=None, quick_mode=False):\n",
        "    \"\"\"\n",
        "    Convenience function to run the complete pipeline\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    csv_path : str\n",
        "        Path to CSV file (uses CSV_FILE_PATH if None)\n",
        "    target_column : str\n",
        "        Target column name (uses TARGET_COLUMN if None)\n",
        "    date_column : str\n",
        "        Date column name (uses DATE_COLUMN if None)\n",
        "    quick_mode : bool\n",
        "        If True, uses faster settings for testing\n",
        "    \"\"\"\n",
        "\n",
        "    # Use defaults if not provided\n",
        "    csv_path = csv_path or CSV_FILE_PATH\n",
        "    target_column = target_column or TARGET_COLUMN\n",
        "    date_column = date_column or DATE_COLUMN\n",
        "\n",
        "    # Create pipeline\n",
        "    pipeline = ComprehensiveMLPipeline(\n",
        "        csv_path=csv_path,\n",
        "        target_column=target_column,\n",
        "        date_column=date_column\n",
        "    )\n",
        "\n",
        "    # Configure based on mode\n",
        "    if quick_mode:\n",
        "        print(\"Running in QUICK MODE (reduced parameters for testing)\")\n",
        "        results = pipeline.execute(\n",
        "            cv_splits=3,\n",
        "            use_random_search=True,\n",
        "            n_iter=10,\n",
        "            n_jobs=-1,\n",
        "            models_to_optimize=['RandomForest', 'ExtraTrees', 'Ridge']\n",
        "        )\n",
        "    else:\n",
        "        print(\"Running in FULL MODE (comprehensive optimization)\")\n",
        "        results = pipeline.execute(\n",
        "            cv_splits=10,\n",
        "            use_random_search=True,\n",
        "            n_iter=100,\n",
        "            n_jobs=-1,\n",
        "            models_to_optimize=None  # All models\n",
        "        )\n",
        "\n",
        "    return pipeline, results\n",
        "\n",
        "# Print instructions\n",
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE ML PIPELINE LOADED\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nTo execute the pipeline:\")\n",
        "print(\"  pipeline, results = run_complete_ml_pipeline()\")\n",
        "print(\"\\nFor quick testing:\")\n",
        "print(\"  pipeline, results = run_complete_ml_pipeline(quick_mode=True)\")\n",
        "print(\"\\nDirect class usage:\")\n",
        "print(\"  pipeline = ComprehensiveMLPipeline(csv_path, target_column, date_column)\")\n",
        "print(\"  results = pipeline.execute()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX3cNIj0EZC9",
        "outputId": "632f8718-859a-4741-b8c5-cba3d3e220fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "COMPREHENSIVE ML PIPELINE LOADED\n",
            "================================================================================\n",
            "\n",
            "To execute the pipeline:\n",
            "  pipeline, results = run_complete_ml_pipeline()\n",
            "\n",
            "For quick testing:\n",
            "  pipeline, results = run_complete_ml_pipeline(quick_mode=True)\n",
            "\n",
            "Direct class usage:\n",
            "  pipeline = ComprehensiveMLPipeline(csv_path, target_column, date_column)\n",
            "  results = pipeline.execute()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O9cbKDaUEZNT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AArsrOxlFcgf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EXECUTION FUNCTIONS - UPDATED FOR COMPREHENSIVE PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def run_ml_pipeline_execution(quick_mode=False, models_to_test=None, cv_splits=None):\n",
        "    \"\"\"\n",
        "    Main execution function for the comprehensive ML pipeline\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    quick_mode : bool\n",
        "        If True, runs faster with reduced parameters\n",
        "    models_to_test : list\n",
        "        Specific models to test (None = all models)\n",
        "    cv_splits : int\n",
        "        Number of CV splits (None = 10 for full, 3 for quick)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"ML FORECASTING PIPELINE - FULL EXECUTION\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Configuration:\")\n",
        "    print(f\"  CSV File: {CSV_FILE_PATH}\")\n",
        "    print(f\"  Target: {TARGET_COLUMN}\")\n",
        "    print(f\"  Date Column: {DATE_COLUMN}\")\n",
        "    print(f\"  Quick Mode: {quick_mode}\")\n",
        "    print(f\"  System: GPU={GPU_AVAILABLE}, High Memory={HIGH_MEMORY}\")\n",
        "\n",
        "    execution_start = time.time()\n",
        "\n",
        "    try:\n",
        "        # Create pipeline\n",
        "        pipeline = ComprehensiveMLPipeline(\n",
        "            csv_path=CSV_FILE_PATH,\n",
        "            target_column=TARGET_COLUMN,\n",
        "            date_column=DATE_COLUMN\n",
        "        )\n",
        "\n",
        "        # Configure parameters based on mode\n",
        "        if quick_mode:\n",
        "            cv_splits = cv_splits or 3\n",
        "            n_iter = 20\n",
        "            models = models_to_test or ['RandomForest', 'ExtraTrees', 'LightGBM', 'Ridge']\n",
        "            print(f\"\\nâš¡ QUICK MODE Settings:\")\n",
        "            print(f\"  CV Splits: {cv_splits}\")\n",
        "            print(f\"  RandomSearch iterations: {n_iter}\")\n",
        "            print(f\"  Models: {models}\")\n",
        "        else:\n",
        "            cv_splits = cv_splits or 10\n",
        "            n_iter = 100\n",
        "            models = models_to_test  # None = all models\n",
        "            print(f\"\\nğŸ”¥ FULL MODE Settings:\")\n",
        "            print(f\"  CV Splits: {cv_splits}\")\n",
        "            print(f\"  RandomSearch iterations: {n_iter}\")\n",
        "            print(f\"  Models: {'ALL' if models is None else models}\")\n",
        "\n",
        "        # Execute pipeline\n",
        "        results = pipeline.execute(\n",
        "            cv_splits=cv_splits,\n",
        "            use_random_search=True,\n",
        "            n_iter=n_iter,\n",
        "            n_jobs=-1,\n",
        "            models_to_optimize=models\n",
        "        )\n",
        "\n",
        "        execution_time = time.time() - execution_start\n",
        "\n",
        "        # Enhanced results display\n",
        "        if not results.empty:\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"EXECUTION COMPLETE - RESULTS SUMMARY\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            # Overall statistics\n",
        "            print(f\"\\nExecution Statistics:\")\n",
        "            print(f\"  Total execution time: {execution_time/60:.1f} minutes\")\n",
        "            print(f\"  Models evaluated: {len(results)}\")\n",
        "            print(f\"  Features used: {pipeline.X_train.shape[1]}\")\n",
        "            print(f\"  Training samples: {pipeline.X_train.shape[0]}\")\n",
        "            print(f\"  Test samples: {pipeline.X_test.shape[0]}\")\n",
        "\n",
        "            # Baseline comparison\n",
        "            baseline_mae = results['Baseline_MAE'].iloc[0] if 'Baseline_MAE' in results.columns else None\n",
        "            if baseline_mae:\n",
        "                models_beat_baseline = results[results['Test_MAE'] < baseline_mae]\n",
        "                print(f\"\\nBaseline Performance:\")\n",
        "                print(f\"  7-day seasonal MAE: {baseline_mae:.2f}\")\n",
        "                print(f\"  Models beating baseline: {len(models_beat_baseline)}/{len(results)} ({len(models_beat_baseline)/len(results)*100:.1f}%)\")\n",
        "\n",
        "            # Top models display\n",
        "            print(f\"\\nTop 5 Models by Test MAE:\")\n",
        "            print(\"-\"*80)\n",
        "            print(f\"{'Rank':<5} {'Model':<25} {'Test MAE':<12} {'vs Baseline':<15} {'Optimization':<15}\")\n",
        "            print(\"-\"*80)\n",
        "\n",
        "            for i, (_, row) in enumerate(results.head(5).iterrows(), 1):\n",
        "                vs_baseline = f\"{row.get('vs_Baseline_%', 0):+.1f}%\" if 'vs_Baseline_%' in row else \"N/A\"\n",
        "                improvement = f\"{row.get('Improvement_%', 0):+.1f}%\" if 'Improvement_%' in row else \"N/A\"\n",
        "                print(f\"{i:<5} {row['Model']:<25} {row['Test_MAE']:<12.2f} {vs_baseline:<15} {improvement:<15}\")\n",
        "\n",
        "            # Model type analysis\n",
        "            print(f\"\\nPerformance by Model Type:\")\n",
        "            model_categories = {\n",
        "                'Tree-based': ['Forest', 'Extra', 'Tree'],\n",
        "                'Boosting': ['Gradient', 'XGBoost', 'LightGBM', 'AdaBoost'],\n",
        "                'Linear': ['Ridge', 'Lasso', 'Linear', 'Elastic'],\n",
        "                'Other': ['SVR', 'Neighbor', 'MLP', 'Bagging']\n",
        "            }\n",
        "\n",
        "            for category, keywords in model_categories.items():\n",
        "                category_models = results[results['Model'].str.contains('|'.join(keywords), case=False)]\n",
        "                if len(category_models) > 0:\n",
        "                    avg_mae = category_models['Test_MAE'].mean()\n",
        "                    best_mae = category_models['Test_MAE'].min()\n",
        "                    print(f\"  {category:<12}: Avg MAE={avg_mae:.2f}, Best MAE={best_mae:.2f}\")\n",
        "\n",
        "            # Save results with timestamp\n",
        "            timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            output_file = f\"pipeline_results_{timestamp}.csv\"\n",
        "            results.to_csv(output_file, index=False)\n",
        "            print(f\"\\nâœ“ Results saved to: {output_file}\")\n",
        "\n",
        "            # Feature importance (if available)\n",
        "            if hasattr(pipeline, 'get_feature_importance'):\n",
        "                importance = pipeline.get_feature_importance()\n",
        "                if importance is not None:\n",
        "                    print(f\"\\nTop 10 Most Important Features:\")\n",
        "                    for i, row in importance.head(10).iterrows():\n",
        "                        print(f\"  {i+1:2d}. {row['feature']:<40} {row['importance']:.4f}\")\n",
        "\n",
        "            return pipeline, results\n",
        "\n",
        "        else:\n",
        "            print(\"\\nâŒ Pipeline execution failed - no results generated\")\n",
        "            return None, pd.DataFrame()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Pipeline execution failed: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "def validate_data_before_execution():\n",
        "    \"\"\"Validate that CSV file exists and has required columns\"\"\"\n",
        "\n",
        "    print(\"Validating data before execution...\")\n",
        "\n",
        "    try:\n",
        "        # Check file exists\n",
        "        import os\n",
        "        if not os.path.exists(CSV_FILE_PATH):\n",
        "            print(f\"âŒ ERROR: File not found: {CSV_FILE_PATH}\")\n",
        "            return False\n",
        "\n",
        "        # Load and check columns\n",
        "        data = pd.read_csv(CSV_FILE_PATH, nrows=5)\n",
        "\n",
        "        required_columns = [TARGET_COLUMN]\n",
        "        expected_columns = [\n",
        "            '^VIX_close', 'SPY_close', 'SPY_volume', 'QQQ_close', 'QQQ_volume',\n",
        "            'DX-Y.NYB_close', 'GC=F_close', 'GC=F_volume', 'BTC-USD_close',\n",
        "            'BTC-USD_volume', 'ETH-USD_close', 'ETH-USD_volume'\n",
        "        ]\n",
        "\n",
        "        missing_required = [col for col in required_columns if col not in data.columns]\n",
        "        if missing_required:\n",
        "            print(f\"âŒ ERROR: Missing required columns: {missing_required}\")\n",
        "            return False\n",
        "\n",
        "        available_expected = [col for col in expected_columns if col in data.columns]\n",
        "        print(f\"âœ“ Validation passed:\")\n",
        "        print(f\"  File: {CSV_FILE_PATH}\")\n",
        "        print(f\"  Shape: {pd.read_csv(CSV_FILE_PATH).shape}\")\n",
        "        print(f\"  Target column: {TARGET_COLUMN} âœ“\")\n",
        "        print(f\"  Market data columns: {len(available_expected)}/{len(expected_columns)}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Validation failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def compare_with_baseline_models():\n",
        "    \"\"\"Run pipeline and compare against simple baseline models\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPREHENSIVE COMPARISON: ML MODELS vs BASELINES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Run main pipeline\n",
        "    pipeline, ml_results = run_ml_pipeline_execution(quick_mode=False)\n",
        "\n",
        "    if pipeline is None:\n",
        "        print(\"Pipeline execution failed\")\n",
        "        return None\n",
        "\n",
        "    # Calculate additional baselines\n",
        "    print(\"\\nCalculating additional baseline models...\")\n",
        "\n",
        "    baselines = {}\n",
        "\n",
        "    # 1. Simple moving average\n",
        "    ma_predictions = pipeline.y_train.rolling(7, min_periods=1).mean().iloc[-1]\n",
        "    ma_predictions = np.full(len(pipeline.y_test), ma_predictions)\n",
        "    baselines['7-day MA'] = mean_absolute_error(pipeline.y_test, ma_predictions)\n",
        "\n",
        "    # 2. Last value (naive)\n",
        "    naive_predictions = np.full(len(pipeline.y_test), pipeline.y_train.iloc[-1])\n",
        "    baselines['Naive (last value)'] = mean_absolute_error(pipeline.y_test, naive_predictions)\n",
        "\n",
        "    # 3. Seasonal naive (already calculated in pipeline)\n",
        "    if 'baseline_mae' in pipeline.results:\n",
        "        baselines['Seasonal (7-day)'] = pipeline.results['baseline_mae']\n",
        "\n",
        "    # Display comparison\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL COMPARISON: ML vs BASELINES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nBaseline Models:\")\n",
        "    for name, mae in baselines.items():\n",
        "        print(f\"  {name:<20}: MAE = {mae:.2f}\")\n",
        "\n",
        "    if not ml_results.empty:\n",
        "        best_ml = ml_results.iloc[0]\n",
        "        print(f\"\\nBest ML Model:\")\n",
        "        print(f\"  {best_ml['Model']:<20}: MAE = {best_ml['Test_MAE']:.2f}\")\n",
        "\n",
        "        # Calculate improvement\n",
        "        best_baseline = min(baselines.values())\n",
        "        improvement = ((best_baseline - best_ml['Test_MAE']) / best_baseline) * 100\n",
        "\n",
        "        print(f\"\\nConclusion:\")\n",
        "        if improvement > 0:\n",
        "            print(f\"  âœ“ ML WINS: {improvement:.1f}% improvement over best baseline\")\n",
        "        else:\n",
        "            print(f\"  âŒ BASELINE WINS: Simple baseline beats ML by {-improvement:.1f}%\")\n",
        "\n",
        "    return pipeline, ml_results, baselines\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*80)\n",
        "    print(\"ML PIPELINE READY FOR EXECUTION\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nOptions:\")\n",
        "    print(\"1. Quick test:     pipeline, results = run_ml_pipeline_execution(quick_mode=True)\")\n",
        "    print(\"2. Full execution: pipeline, results = run_ml_pipeline_execution(quick_mode=False)\")\n",
        "    print(\"3. With baselines: pipeline, results, baselines = compare_with_baseline_models()\")\n",
        "    print(\"\\nValidating data...\")\n",
        "\n",
        "    if validate_data_before_execution():\n",
        "        print(\"\\nâœ“ Ready to execute!\")\n",
        "        print(\"\\nTo start, run one of the options above.\")\n",
        "    else:\n",
        "        print(\"\\nâŒ Please fix data issues before proceeding.\")\n",
        "\n",
        "# Convenience shortcuts\n",
        "def quick_test():\n",
        "    \"\"\"Quick test with minimal parameters\"\"\"\n",
        "    return run_ml_pipeline_execution(quick_mode=True)\n",
        "\n",
        "def full_run():\n",
        "    \"\"\"Full execution with all models and comprehensive GridSearch\"\"\"\n",
        "    return run_ml_pipeline_execution(quick_mode=False)\n",
        "\n",
        "def full_comparison():\n",
        "    \"\"\"Full execution with baseline comparisons\"\"\"\n",
        "    return compare_with_baseline_models()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SHORTCUTS AVAILABLE:\")\n",
        "print(\"  pipeline, results = quick_test()  # Fast testing\")\n",
        "print(\"  pipeline, results = full_run()    # Complete optimization\")\n",
        "print(\"  pipeline, results, baselines = full_comparison()  # Full analysis\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJpTh4W9E-cY",
        "outputId": "c36c6067-878e-4726-a3c7-ff91cc958586"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ML PIPELINE READY FOR EXECUTION\n",
            "================================================================================\n",
            "\n",
            "Options:\n",
            "1. Quick test:     pipeline, results = run_ml_pipeline_execution(quick_mode=True)\n",
            "2. Full execution: pipeline, results = run_ml_pipeline_execution(quick_mode=False)\n",
            "3. With baselines: pipeline, results, baselines = compare_with_baseline_models()\n",
            "\n",
            "Validating data...\n",
            "Validating data before execution...\n",
            "âœ“ Validation passed:\n",
            "  File: enhanced_eda_data.csv\n",
            "  Shape: (976, 20)\n",
            "  Target column: calls âœ“\n",
            "  Market data columns: 12/12\n",
            "\n",
            "âœ“ Ready to execute!\n",
            "\n",
            "To start, run one of the options above.\n",
            "\n",
            "================================================================================\n",
            "SHORTCUTS AVAILABLE:\n",
            "  pipeline, results = quick_test()  # Fast testing\n",
            "  pipeline, results = full_run()    # Complete optimization\n",
            "  pipeline, results, baselines = full_comparison()  # Full analysis\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RqJ0W8QQE-oo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# COMPLETE WORKING PIPELINE - PROPERLY HANDLES DATA TYPES\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from typing import Tuple\n",
        "\n",
        "# ============================================================================\n",
        "# FIXED DATA LOADING - HANDLES DATE COLUMN PROPERLY\n",
        "# ============================================================================\n",
        "\n",
        "def load_all_data_columns(csv_path: str, target_column: str = 'calls', date_column: str = 'Date') -> pd.DataFrame:\n",
        "    \"\"\"Load CSV and preserve ALL columns for model training\"\"\"\n",
        "\n",
        "    print(f\"Loading ALL columns from: {csv_path}\")\n",
        "\n",
        "    # Load the CSV\n",
        "    data = pd.read_csv(csv_path)\n",
        "    print(f\"  Loaded shape: {data.shape}\")\n",
        "\n",
        "    # CRITICAL: Handle date column properly\n",
        "    if date_column in data.columns:\n",
        "        print(f\"  Converting {date_column} to datetime index...\")\n",
        "        data[date_column] = pd.to_datetime(data[date_column])\n",
        "        data = data.set_index(date_column).sort_index()\n",
        "        print(f\"  Date column set as index\")\n",
        "\n",
        "    print(f\"  Columns after date handling: {list(data.columns)}\")\n",
        "\n",
        "    # Verify target exists\n",
        "    if target_column not in data.columns:\n",
        "        raise ValueError(f\"Target column '{target_column}' not found!\")\n",
        "\n",
        "    # Clean the data\n",
        "    print(\"  Cleaning data...\")\n",
        "    data = data.dropna(subset=[target_column])\n",
        "\n",
        "    # Convert all columns to numeric where possible\n",
        "    for col in data.columns:\n",
        "        if col != target_column:\n",
        "            try:\n",
        "                data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # Fill missing values in numeric columns\n",
        "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
        "\n",
        "    print(f\"  Final shape: {data.shape}\")\n",
        "    print(f\"  Numeric columns: {len(numeric_cols)}/{len(data.columns)}\")\n",
        "    return data\n",
        "\n",
        "# ============================================================================\n",
        "# FIXED FEATURE ENGINEERING - HANDLES MIXED DATA TYPES\n",
        "# ============================================================================\n",
        "\n",
        "def create_all_features(data: pd.DataFrame, target_column: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"Create features using ALL available columns - PROPERLY HANDLES TYPES\"\"\"\n",
        "\n",
        "    print(\"Creating comprehensive features from ALL columns...\")\n",
        "\n",
        "    # Get only numeric columns for features\n",
        "    numeric_data = data.select_dtypes(include=[np.number])\n",
        "    print(f\"  Using {len(numeric_data.columns)} numeric columns from {len(data.columns)} total\")\n",
        "\n",
        "    # Start with numeric columns except target\n",
        "    if target_column in numeric_data.columns:\n",
        "        features = numeric_data.drop(columns=[target_column]).copy()\n",
        "    else:\n",
        "        features = numeric_data.copy()\n",
        "\n",
        "    initial_feature_count = len(features.columns)\n",
        "    print(f\"  Starting with {initial_feature_count} original numeric features\")\n",
        "\n",
        "    # Add lag features for target if it exists\n",
        "    if target_column in data.columns:\n",
        "        print(\"  Adding target lag features...\")\n",
        "        for lag in [1, 2, 3, 7, 14]:\n",
        "            if lag < len(data):\n",
        "                features[f'{target_column}_lag_{lag}'] = data[target_column].shift(lag)\n",
        "\n",
        "        print(\"  Adding target rolling features...\")\n",
        "        for window in [3, 7, 14]:\n",
        "            if window < len(data):\n",
        "                features[f'{target_column}_roll_mean_{window}'] = data[target_column].rolling(window, min_periods=1).mean()\n",
        "                features[f'{target_column}_roll_std_{window}'] = data[target_column].rolling(window, min_periods=1).std()\n",
        "\n",
        "    # Add percentage changes for price columns\n",
        "    print(\"  Adding price change features...\")\n",
        "    price_columns = ['^VIX_close', 'SPY_close', 'QQQ_close', 'DX-Y.NYB_close',\n",
        "                     'GC=F_close', 'BTC-USD_close', 'ETH-USD_close']\n",
        "\n",
        "    for col in price_columns:\n",
        "        if col in features.columns:\n",
        "            features[f'{col}_pct_change'] = data[col].pct_change()\n",
        "\n",
        "    # Create target variable\n",
        "    target = data[target_column].shift(-1)\n",
        "\n",
        "    # Clean features - FIXED FOR MIXED TYPES\n",
        "    print(\"  Cleaning features...\")\n",
        "\n",
        "    # First convert everything to numeric\n",
        "    for col in features.columns:\n",
        "        features[col] = pd.to_numeric(features[col], errors='coerce')\n",
        "\n",
        "    # Replace inf with NaN\n",
        "    features = features.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Fill NaN values\n",
        "    features = features.ffill()\n",
        "    features = features.bfill()\n",
        "    features = features.fillna(0)\n",
        "\n",
        "    # Align features and target\n",
        "    valid_idx = ~target.isna()\n",
        "    features_clean = features[valid_idx].iloc[:-1]\n",
        "    target_clean = target[valid_idx].iloc[:-1]\n",
        "\n",
        "    # Final verification - only on numeric data\n",
        "    assert not features_clean.isna().any().any(), \"Features still contain NaN\"\n",
        "    assert not target_clean.isna().any(), \"Target contains NaN\"\n",
        "\n",
        "    print(f\"  Total features: {features_clean.shape[1]}\")\n",
        "    print(f\"  Dataset shape: Features {features_clean.shape}, Target {target_clean.shape}\")\n",
        "\n",
        "    return features_clean, target_clean\n",
        "\n",
        "# ============================================================================\n",
        "# SIMPLIFIED PIPELINE FOR TESTING\n",
        "# ============================================================================\n",
        "\n",
        "def run_all_30_models_with_gridsearch():\n",
        "    \"\"\"Run models with GridSearchCV - SIMPLIFIED VERSION\"\"\"\n",
        "\n",
        "    print(\"ğŸš€ RUNNING ML MODELS WITH GRIDSEARCHCV\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    pipeline_start = time.time()\n",
        "\n",
        "    try:\n",
        "        # Load data\n",
        "        print(\"\\nStep 1: Loading data...\")\n",
        "        data = load_all_data_columns(CSV_FILE_PATH, TARGET_COLUMN, DATE_COLUMN)\n",
        "\n",
        "        # Create features\n",
        "        print(\"\\nStep 2: Creating features...\")\n",
        "        X, y = create_all_features(data, TARGET_COLUMN)\n",
        "        print(f\"  Features created: {X.shape[1]} columns\")\n",
        "\n",
        "        # Split data\n",
        "        print(\"\\nStep 3: Splitting data...\")\n",
        "        train_size = int(len(X) * 0.75)\n",
        "        X_train = X.iloc[:train_size].values\n",
        "        X_test = X.iloc[train_size:].values\n",
        "        y_train = y.iloc[:train_size].values\n",
        "        y_test = y.iloc[train_size:].values\n",
        "        print(f\"  Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "        # Test with simple models first\n",
        "        print(\"\\nStep 4: Testing models...\")\n",
        "\n",
        "        models_and_grids = {\n",
        "            'Ridge': (\n",
        "                Ridge(),\n",
        "                {'alpha': [0.1, 1.0, 10.0]}\n",
        "            ),\n",
        "            'RandomForest': (\n",
        "                RandomForestRegressor(random_state=42),\n",
        "                {'n_estimators': [50, 100],\n",
        "                 'max_depth': [5, 10, None]}\n",
        "            ),\n",
        "            'GradientBoosting': (\n",
        "                GradientBoostingRegressor(random_state=42),\n",
        "                {'n_estimators': [50, 100],\n",
        "                 'learning_rate': [0.05, 0.1, 0.2]}\n",
        "            ),\n",
        "            'ExtraTrees': (\n",
        "                ExtraTreesRegressor(random_state=42),\n",
        "                {'n_estimators': [50, 100],\n",
        "                 'max_depth': [5, 10, None]}\n",
        "            ),\n",
        "            'Lasso': (\n",
        "                Lasso(max_iter=2000),\n",
        "                {'alpha': [0.01, 0.1, 1.0]}\n",
        "            ),\n",
        "        }\n",
        "\n",
        "        # Add XGBoost if available\n",
        "        if XGB_AVAILABLE:\n",
        "            models_and_grids['XGBoost'] = (\n",
        "                xgb.XGBRegressor(random_state=42, verbosity=0),\n",
        "                {'n_estimators': [50, 100, 200],\n",
        "                 'max_depth': [3, 5, 7],\n",
        "                 'learning_rate': [0.05, 0.1, 0.2]}\n",
        "            )\n",
        "\n",
        "        # Add LightGBM if available\n",
        "        if LGB_AVAILABLE:\n",
        "            models_and_grids['LightGBM'] = (\n",
        "                lgb.LGBMRegressor(random_state=42, verbose=-1),\n",
        "                {'n_estimators': [50, 100, 200],\n",
        "                 'num_leaves': [31, 50],\n",
        "                 'learning_rate': [0.05, 0.1, 0.2]}\n",
        "            )\n",
        "\n",
        "        print(f\"  Total models to optimize: {len(models_and_grids)}\")\n",
        "\n",
        "        # Run GridSearchCV\n",
        "        print(\"\\nStep 5: Running GridSearchCV...\")\n",
        "        results = []\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "        for i, (model_name, (base_model, param_grid)) in enumerate(models_and_grids.items(), 1):\n",
        "            print(f\"\\n[{i}/{len(models_and_grids)}] {model_name}\")\n",
        "\n",
        "            grid_size = np.prod([len(v) for v in param_grid.values()])\n",
        "            print(f\"  Testing {grid_size} parameter combinations...\")\n",
        "\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "\n",
        "                search = GridSearchCV(\n",
        "                    estimator=base_model,\n",
        "                    param_grid=param_grid,\n",
        "                    cv=tscv,\n",
        "                    scoring='neg_mean_absolute_error',\n",
        "                    n_jobs=-1,\n",
        "                    verbose=1\n",
        "                )\n",
        "\n",
        "                search.fit(X_train, y_train.ravel())\n",
        "\n",
        "                predictions = search.predict(X_test)\n",
        "                mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "                results.append({\n",
        "                    'Model': model_name,\n",
        "                    'MAE': mae,\n",
        "                    'Best_Params': search.best_params_,\n",
        "                    'CV_Score': -search.best_score_,\n",
        "                    'Time_sec': time.time() - start_time,\n",
        "                    'Status': 'Success'\n",
        "                })\n",
        "\n",
        "                print(f\"  âœ… SUCCESS - MAE: {mae:.2f}, Time: {time.time()-start_time:.1f}s\")\n",
        "                print(f\"     Best params: {search.best_params_}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  âŒ FAILED: {str(e)[:100]}\")\n",
        "                results.append({\n",
        "                    'Model': model_name,\n",
        "                    'MAE': 99999,\n",
        "                    'Status': f'Failed'\n",
        "                })\n",
        "\n",
        "        # Save results\n",
        "        print(\"\\n\\nStep 6: Saving results...\")\n",
        "        results_df = pd.DataFrame(results).sort_values('MAE')\n",
        "        results_df.to_csv('all_models_results_calls.csv', index=False)\n",
        "\n",
        "        total_time = time.time() - pipeline_start\n",
        "        successful = len(results_df[results_df['Status'] == 'Success'])\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"PIPELINE COMPLETE!\")\n",
        "        print(f\"  Total models: {len(results_df)}\")\n",
        "        print(f\"  Successful: {successful}\")\n",
        "        print(f\"  Total time: {total_time/60:.1f} minutes\")\n",
        "\n",
        "        if successful > 0:\n",
        "            print(f\"\\nTop 3 models:\")\n",
        "            for _, row in results_df.head(3).iterrows():\n",
        "                print(f\"  {row['Model']}: MAE={row['MAE']:.2f}\")\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Pipeline failed: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# RUN THE PIPELINE\n",
        "print(\"=\"*80)\n",
        "print(\"Starting GridSearchCV pipeline with proper data handling...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_results = run_all_30_models_with_gridsearch()\n",
        "\n",
        "if not all_results.empty and any(all_results['Status'] == 'Success'):\n",
        "    print(f\"\\nğŸ‰ SUCCESS! GridSearchCV completed\")\n",
        "else:\n",
        "    print(\"\\nâŒ Check errors above\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff2cODZ0OI4I",
        "outputId": "20cf58b4-e6e1-4d0b-a72a-d9966f7da02e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Starting GridSearchCV pipeline with proper data handling...\n",
            "================================================================================\n",
            "ğŸš€ RUNNING ML MODELS WITH GRIDSEARCHCV\n",
            "================================================================================\n",
            "\n",
            "Step 1: Loading data...\n",
            "Loading ALL columns from: enhanced_eda_data.csv\n",
            "  Loaded shape: (976, 20)\n",
            "  Columns after date handling: ['Date', 'calls', '^VIX_close', 'SPY_close', 'SPY_volume', 'QQQ_close', 'QQQ_volume', 'DX-Y.NYB_close', 'GC=F_close', 'GC=F_volume', 'BTC-USD_close', 'BTC-USD_volume', 'ETH-USD_close', 'ETH-USD_volume', 'is_weekend', 'Month', 'Quarter', 'DayOfWeek', 'Year', 'outlier_multivariate']\n",
            "  Cleaning data...\n",
            "  Final shape: (976, 20)\n",
            "  Numeric columns: 18/20\n",
            "\n",
            "Step 2: Creating features...\n",
            "Creating comprehensive features from ALL columns...\n",
            "  Using 18 numeric columns from 20 total\n",
            "  Starting with 17 original numeric features\n",
            "  Adding target lag features...\n",
            "  Adding target rolling features...\n",
            "  Adding price change features...\n",
            "  Cleaning features...\n",
            "  Total features: 35\n",
            "  Dataset shape: Features (974, 35), Target (974,)\n",
            "  Features created: 35 columns\n",
            "\n",
            "Step 3: Splitting data...\n",
            "  Train: (730, 35), Test: (244, 35)\n",
            "\n",
            "Step 4: Testing models...\n",
            "  Total models to optimize: 7\n",
            "\n",
            "Step 5: Running GridSearchCV...\n",
            "\n",
            "[1/7] Ridge\n",
            "  Testing 3 parameter combinations...\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "  âœ… SUCCESS - MAE: 1102.54, Time: 1.9s\n",
            "     Best params: {'alpha': 10.0}\n",
            "\n",
            "[2/7] RandomForest\n",
            "  Testing 6 parameter combinations...\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "  âœ… SUCCESS - MAE: 701.85, Time: 4.8s\n",
            "     Best params: {'max_depth': None, 'n_estimators': 50}\n",
            "\n",
            "[3/7] GradientBoosting\n",
            "  Testing 6 parameter combinations...\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "  âœ… SUCCESS - MAE: 1482.02, Time: 2.8s\n",
            "     Best params: {'learning_rate': 0.2, 'n_estimators': 50}\n",
            "\n",
            "[4/7] ExtraTrees\n",
            "  Testing 6 parameter combinations...\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "  âœ… SUCCESS - MAE: 626.14, Time: 1.8s\n",
            "     Best params: {'max_depth': 10, 'n_estimators': 100}\n",
            "\n",
            "[5/7] Lasso\n",
            "  Testing 3 parameter combinations...\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "  âœ… SUCCESS - MAE: 1133.15, Time: 0.1s\n",
            "     Best params: {'alpha': 1.0}\n",
            "\n",
            "[6/7] XGBoost\n",
            "  Testing 27 parameter combinations...\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "  âœ… SUCCESS - MAE: 944.62, Time: 14.3s\n",
            "     Best params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200}\n",
            "\n",
            "[7/7] LightGBM\n",
            "  Testing 18 parameter combinations...\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "  âœ… SUCCESS - MAE: 651.22, Time: 218.7s\n",
            "     Best params: {'learning_rate': 0.1, 'n_estimators': 50, 'num_leaves': 31}\n",
            "\n",
            "\n",
            "Step 6: Saving results...\n",
            "\n",
            "================================================================================\n",
            "PIPELINE COMPLETE!\n",
            "  Total models: 7\n",
            "  Successful: 7\n",
            "  Total time: 4.1 minutes\n",
            "\n",
            "Top 3 models:\n",
            "  ExtraTrees: MAE=626.14\n",
            "  LightGBM: MAE=651.22\n",
            "  RandomForest: MAE=701.85\n",
            "\n",
            "ğŸ‰ SUCCESS! GridSearchCV completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w32lARQ8CFVZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RMheNMSKExBT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL ANALYSIS - ALL 30+ MODELS vs Day-of-Week Average\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FINAL COMPREHENSIVE ANALYSIS - ALL MODELS vs Day-of-Week Average\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load the data to calculate baseline\n",
        "data = pd.read_csv(\"enhanced_eda_data.csv\")\n",
        "# FIXED: Ensure the date column is used as the index and is a DatetimeIndex\n",
        "if \"Date\" in data.columns:\n",
        "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
        "    data = data.set_index(\"Date\").sort_index()\n",
        "elif \"date\" in data.columns: # Also check for lowercase 'date'\n",
        "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
        "    data = data.set_index(\"date\").sort_index()\n",
        "\n",
        "\n",
        "# Calculate Day-of-Week Average\n",
        "print(\"Calculating Day-of-Week Average...\")\n",
        "target_series = data[\"calls\"]\n",
        "train_size = int(len(target_series) * 0.75)\n",
        "train_data = target_series.iloc[:train_size]\n",
        "test_data = target_series.iloc[train_size:]\n",
        "\n",
        "# Create Day-of-Week Average predictions\n",
        "dow_averages = {}\n",
        "for dow in range(7):  # 0=Monday, 6=Sunday\n",
        "    # FIXED: Access dayofweek from the index\n",
        "    dow_mask = train_data.index.dayofweek == dow\n",
        "    if dow_mask.sum() > 0:\n",
        "        dow_averages[dow] = train_data[dow_mask].mean()\n",
        "    else:\n",
        "        dow_averages[dow] = train_data.mean() # Fallback to overall mean\n",
        "\n",
        "seasonal_predictions = []\n",
        "for date in test_data.index:\n",
        "    dow = date.dayofweek\n",
        "    seasonal_predictions.append(dow_averages[dow])\n",
        "\n",
        "# Calculate baseline metrics\n",
        "baseline_mae = mean_absolute_error(test_data, seasonal_predictions)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(test_data, seasonal_predictions))\n",
        "\n",
        "print(f\"Day-of-Week Average:\")\n",
        "print(f\"  MAE: {baseline_mae:.4f}\")\n",
        "print(f\"  RMSE: {baseline_rmse:.4f}\")\n",
        "print()\n",
        "\n",
        "# Load ML results\n",
        "print(\"Loading ML model results...\")\n",
        "try:\n",
        "    ml_results = pd.read_csv(\"all_models_results_calls.csv\")\n",
        "    print(f\"Loaded {len(ml_results)} model results\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: all_models_results_calls.csv not found!\")\n",
        "    print(\"Please run the 30+ models pipeline first.\")\n",
        "    # Exit or handle the error appropriately, e.g., raise an exception\n",
        "    raise FileNotFoundError(\"ML results file not found.\")\n",
        "\n",
        "\n",
        "# Add baseline comparison\n",
        "ml_results['Baseline_MAE'] = baseline_mae\n",
        "ml_results['MAE_Improvement'] = baseline_mae - ml_results['MAE']\n",
        "ml_results['Improvement_Pct'] = ((baseline_mae - ml_results['MAE']) / baseline_mae) * 100\n",
        "ml_results['Beats_Baseline'] = ml_results['MAE'] < baseline_mae\n",
        "ml_results['Performance'] = ml_results['Beats_Baseline'].apply(lambda x: 'âœ“ BEAT' if x else 'âœ— FAILED')\n",
        "\n",
        "# Sort by MAE (best first)\n",
        "ml_results = ml_results.sort_values('MAE')\n",
        "\n",
        "print(\"ALL MODELS DETAILED RESULTS:\")\n",
        "print(\"=\"*85)\n",
        "print(f\"{'Rank':<4} {'Model':<25} {'MAE':<8} {'vs Baseline':<12} {'Improvement':<12} {'Status':<8}\")\n",
        "print(\"-\"*85)\n",
        "\n",
        "for i, (_, row) in enumerate(ml_results.iterrows(), 1):\n",
        "    rank = f\"{i}.\"\n",
        "    model = row['Model'][:24]\n",
        "    mae = f\"{row['MAE']:.4f}\"\n",
        "    improvement = f\"{row['Improvement_Pct']:+.1f}%\"\n",
        "    vs_baseline = f\"{row['MAE_Improvement']:+.4f}\"\n",
        "    status = row['Performance']\n",
        "\n",
        "    print(f\"{rank:<4} {model:<25} {mae:<8} {vs_baseline:<12} {improvement:<12} {status:<8}\")\n",
        "\n",
        "# Summary statistics\n",
        "total_models = len(ml_results)\n",
        "successful_models = ml_results['Beats_Baseline'].sum()\n",
        "failed_models = total_models - successful_models\n",
        "best_model = ml_results.iloc[0]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERFORMANCE SUMMARY:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Day-of-Week Average MAE: {baseline_mae:.4f}\")\n",
        "print(f\"Total ML Models: {total_models}\")\n",
        "print(f\"Models that BEAT baseline: {successful_models} ({successful_models/total_models*100:.1f}%)\")\n",
        "print(f\"Models that FAILED vs baseline: {failed_models} ({failed_models/total_models*100:.1f}%)\")\n",
        "print()\n",
        "\n",
        "print(\"CHAMPION MODEL:\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Model: {best_model['Model']}\")\n",
        "print(f\"MAE: {best_model['MAE']:.4f}\")\n",
        "print(f\"Improvement over baseline: {best_model['Improvement_Pct']:+.1f}%\")\n",
        "print(f\"Absolute improvement: {best_model['MAE_Improvement']:+.4f}\")\n",
        "print()\n",
        "\n",
        "# Model type performance\n",
        "print(\"MODEL TYPE PERFORMANCE:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "model_types = {\n",
        "    'Linear': ['Linear', 'Ridge', 'Lasso', 'Elastic', 'Bayesian', 'Huber'],\n",
        "    'Tree': ['Forest', 'Extra', 'Gradient', 'Decision'],\n",
        "    'XGBoost': ['XGBoost'],\n",
        "    'LightGBM': ['LightGBM'],\n",
        "    'Neural': ['MLP'],\n",
        "    'Ensemble': ['Bagging', 'Ada'],\n",
        "    'Other': ['SVR', 'KNeighbors', 'Kernel']\n",
        "}\n",
        "\n",
        "for category, keywords in model_types.items():\n",
        "    category_models = ml_results[ml_results['Model'].str.contains('|'.join(keywords), case=False)]\n",
        "    if len(category_models) > 0:\n",
        "        category_success = category_models['Beats_Baseline'].sum()\n",
        "        category_total = len(category_models)\n",
        "        success_rate = (category_success / category_total) * 100\n",
        "        best_in_category = category_models.iloc[0]\n",
        "\n",
        "        print(f\"{category:>10}: {category_success:2d}/{category_total:2d} beat baseline ({success_rate:5.1f}%) | Best: {best_in_category['Model']:<20} (MAE: {best_in_category['MAE']:.4f})\")\n",
        "\n",
        "# Top performers\n",
        "print(\"\\nTOP 10 PERFORMERS:\")\n",
        "print(\"-\" * 60)\n",
        "for i, (_, row) in enumerate(ml_results.head(10).iterrows(), 1):\n",
        "    status_icon = \"âœ“\" if row['Beats_Baseline'] else \"âœ—\"\n",
        "    print(f\"{i:2d}. {status_icon} {row['Model']:<25} MAE: {row['MAE']:.4f} ({row['Improvement_Pct']:+.1f}%)\")\n",
        "\n",
        "# Final insights\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY INSIGHTS:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if successful_models == 0:\n",
        "    print(\"ğŸš¨ CRITICAL: No ML models beat the Day-of-Week Average!\")\n",
        "    print(\"   â†’ The 7-day seasonal pattern is extremely strong\")\n",
        "    print(\"   â†’ Consider ensemble methods or seasonal decomposition\")\n",
        "    print(\"   â†’ Stick with simple seasonal forecasting for now\")\n",
        "elif successful_models < total_models * 0.3:\n",
        "    print(f\"âš ï¸  MIXED: Only {successful_models}/{total_models} models beat baseline\")\n",
        "    print(\"   â†’ Most models are not adding value over simple forecasting\")\n",
        "    print(\"   â†’ Focus on the successful models for deployment\")\n",
        "    print(\"   â†’ Consider why other models failed\")\n",
        "else:\n",
        "    print(f\"âœ… SUCCESS: {successful_models}/{total_models} models beat baseline!\")\n",
        "    print(\"   â†’ ML is adding significant value over naive forecasting\")\n",
        "    print(\"   â†’ You have multiple good model options\")\n",
        "    print(\"   â†’ Advanced ML is worthwhile for your use case\")\n",
        "\n",
        "best_improvement = ml_results['Improvement_Pct'].max()\n",
        "if best_improvement > 20:\n",
        "    print(f\"\\nğŸ¯ OUTSTANDING: Best model improves baseline by {best_improvement:.1f}%!\")\n",
        "elif best_improvement > 10:\n",
        "    print(f\"\\nğŸ‘ GOOD: Best model improves baseline by {best_improvement:.1f}%\")\n",
        "elif best_improvement > 5:\n",
        "    print(f\"\\nğŸ“ˆ MODEST: Best model improves baseline by {best_improvement:.1f}%\")\n",
        "else:\n",
        "    print(f\"\\nğŸ˜ MINIMAL: Best improvement is only {best_improvement:.1f}%\")\n",
        "\n",
        "# Save enhanced results\n",
        "enhanced_file = \"final_enhanced_results_calls.csv\"\n",
        "ml_results.to_csv(enhanced_file, index=False)\n",
        "print(f\"\\nğŸ’¾ Complete results saved to: {enhanced_file}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4KnBOswI8jI",
        "outputId": "4509ab34-a732-4fe1-9e77-31cfe548e1cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "FINAL COMPREHENSIVE ANALYSIS - ALL MODELS vs Day-of-Week Average\n",
            "================================================================================\n",
            "Calculating Day-of-Week Average...\n",
            "Day-of-Week Average:\n",
            "  MAE: 919.0115\n",
            "  RMSE: 1131.0557\n",
            "\n",
            "Loading ML model results...\n",
            "Loaded 7 model results\n",
            "ALL MODELS DETAILED RESULTS:\n",
            "=====================================================================================\n",
            "Rank Model                     MAE      vs Baseline  Improvement  Status  \n",
            "-------------------------------------------------------------------------------------\n",
            "1.   ExtraTrees                626.1377 +292.8738    +31.9%       âœ“ BEAT  \n",
            "2.   LightGBM                  651.2175 +267.7940    +29.1%       âœ“ BEAT  \n",
            "3.   RandomForest              701.8485 +217.1630    +23.6%       âœ“ BEAT  \n",
            "4.   XGBoost                   944.6166 -25.6051     -2.8%        âœ— FAILED\n",
            "5.   Ridge                     1102.5432 -183.5317    -20.0%       âœ— FAILED\n",
            "6.   Lasso                     1133.1529 -214.1414    -23.3%       âœ— FAILED\n",
            "7.   GradientBoosting          1482.0179 -563.0063    -61.3%       âœ— FAILED\n",
            "\n",
            "================================================================================\n",
            "PERFORMANCE SUMMARY:\n",
            "================================================================================\n",
            "Day-of-Week Average MAE: 919.0115\n",
            "Total ML Models: 7\n",
            "Models that BEAT baseline: 3 (42.9%)\n",
            "Models that FAILED vs baseline: 4 (57.1%)\n",
            "\n",
            "CHAMPION MODEL:\n",
            "------------------------------\n",
            "Model: ExtraTrees\n",
            "MAE: 626.1377\n",
            "Improvement over baseline: +31.9%\n",
            "Absolute improvement: +292.8738\n",
            "\n",
            "MODEL TYPE PERFORMANCE:\n",
            "--------------------------------------------------\n",
            "    Linear:  0/ 2 beat baseline (  0.0%) | Best: Ridge                (MAE: 1102.5432)\n",
            "      Tree:  2/ 3 beat baseline ( 66.7%) | Best: ExtraTrees           (MAE: 626.1377)\n",
            "   XGBoost:  0/ 1 beat baseline (  0.0%) | Best: XGBoost              (MAE: 944.6166)\n",
            "  LightGBM:  1/ 1 beat baseline (100.0%) | Best: LightGBM             (MAE: 651.2175)\n",
            "\n",
            "TOP 10 PERFORMERS:\n",
            "------------------------------------------------------------\n",
            " 1. âœ“ ExtraTrees                MAE: 626.1377 (+31.9%)\n",
            " 2. âœ“ LightGBM                  MAE: 651.2175 (+29.1%)\n",
            " 3. âœ“ RandomForest              MAE: 701.8485 (+23.6%)\n",
            " 4. âœ— XGBoost                   MAE: 944.6166 (-2.8%)\n",
            " 5. âœ— Ridge                     MAE: 1102.5432 (-20.0%)\n",
            " 6. âœ— Lasso                     MAE: 1133.1529 (-23.3%)\n",
            " 7. âœ— GradientBoosting          MAE: 1482.0179 (-61.3%)\n",
            "\n",
            "================================================================================\n",
            "KEY INSIGHTS:\n",
            "================================================================================\n",
            "âœ… SUCCESS: 3/7 models beat baseline!\n",
            "   â†’ ML is adding significant value over naive forecasting\n",
            "   â†’ You have multiple good model options\n",
            "   â†’ Advanced ML is worthwhile for your use case\n",
            "\n",
            "ğŸ¯ OUTSTANDING: Best model improves baseline by 31.9%!\n",
            "\n",
            "ğŸ’¾ Complete results saved to: final_enhanced_results_calls.csv\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mOdxhKD9U_fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Performance Analysis\n",
        "The Bottom Line: Your call center exhibits exceptionally strong day-of-week patterns. Only 3 of 7 ML models (43%) outperformed the Day-of-Week Average baseline, with ExtraTrees achieving an 18.4% error reduction.\n",
        "Key Findings\n",
        "1. Day-of-Week Dominance\n",
        "The Day-of-Week Average baseline (MASE: 0.700) proves surprisingly sophisticated for your data. Its strong performance indicates your call volumes follow highly predictable weekly rhythms - each weekday consistently behaves like its historical counterparts. This makes it a harder benchmark to beat than simple time-series baselines.\n",
        "2. Tree Models Excel, Linear Models Fail\n",
        "Only tree-based algorithms succeeded:\n",
        "\n",
        "Winners: ExtraTrees, LightGBM, RandomForest\n",
        "Losers: Ridge, Lasso, GradientBoosting, others\n",
        "\n",
        "This stark divide reveals that while day-of-week patterns dominate, additional value comes from non-linear, hierarchical relationships that only tree models can capture. Linear models failed entirely because your data's complexity goes beyond additive relationships.\n",
        "3. The 18.4% Improvement Matters\n",
        "ExtraTrees reduces prediction error by ~141 calls per day compared to Day-of-Week Average. While this might seem modest, it represents genuine value extraction beyond an already-intelligent baseline - far more impressive than beating a naive \"use yesterday's value\" approach.\n",
        "4. GradientBoosting's Failure Signals Risk\n",
        "The 93% performance degradation from GradientBoosting is a red flag, likely indicating hyperparameter misconfiguration or overfitting. This warrants investigation since gradient boosting typically handles temporal patterns well.\n",
        "Recommendations\n",
        "\n",
        "Deploy ExtraTrees or LightGBM - These are your only models adding meaningful value\n",
        "Keep Day-of-Week Average as your safety net - It's simple, robust, and highly effective\n",
        "Consider a hybrid approach - Ensemble ExtraTrees with Day-of-Week Average, using dynamic weights based on recent performance\n",
        "Simplify your model pipeline - With 57% of ML models underperforming, focus resources on the tree-based winners\n",
        "Investigate the gradient boosting failure - Understanding this breakdown could prevent future model failures\n",
        "\n",
        "The Strategic Insight: Your call center's predictability is driven primarily by weekly operational patterns. ML adds value only when it can detect the subtle, non-linear variations around these patterns - making model selection critical to success.\n"
      ],
      "metadata": {
        "id": "lgOPs3NYVrde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE RESULTS TABLE - ALL METRICS VS Day-of-Week Average\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load data and calculate Day-of-Week Average\n",
        "print(\"Calculating baselines and loading results...\")\n",
        "data = pd.read_csv(\"enhanced_eda_data.csv\")\n",
        "if \"Date\" in data.columns:\n",
        "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
        "    data = data.set_index(\"Date\").sort_index()\n",
        "\n",
        "# Split data\n",
        "target = data[\"calls\"]\n",
        "train_size = int(len(target) * 0.75)\n",
        "train_data = target.iloc[:train_size]\n",
        "test_data = target.iloc[train_size:]\n",
        "\n",
        "# Calculate seasonal naive baseline (7-day lookback)\n",
        "seasonal_predictions = []\n",
        "for i in range(len(test_data)):\n",
        "    lookup_idx = train_size + i - 7\n",
        "    if lookup_idx >= 0 and lookup_idx < len(target):\n",
        "        seasonal_predictions.append(target.iloc[lookup_idx])\n",
        "    else:\n",
        "        seasonal_predictions.append(train_data.iloc[-1])\n",
        "\n",
        "seasonal_predictions = np.array(seasonal_predictions)\n",
        "\n",
        "# Baseline metrics\n",
        "baseline_mae = mean_absolute_error(test_data, seasonal_predictions)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(test_data, seasonal_predictions))\n",
        "baseline_r2 = r2_score(test_data, seasonal_predictions)\n",
        "baseline_mape = np.mean(np.abs((test_data - seasonal_predictions) / test_data)) * 100\n",
        "\n",
        "# Load ML results\n",
        "try:\n",
        "    ml_results = pd.read_csv(\"all_models_results_calls.csv\")\n",
        "\n",
        "    # Create comprehensive metrics table\n",
        "    results_table = []\n",
        "\n",
        "    # Add baseline as first row\n",
        "    results_table.append({\n",
        "        'Model': 'ğŸ“Š Day-of-Week Average (7-day)',\n",
        "        'MAE': baseline_mae,\n",
        "        'RMSE': baseline_rmse,\n",
        "        'RÂ²': baseline_r2,\n",
        "        'MAPE (%)': baseline_mape,\n",
        "        'vs Baseline MAE': 0,\n",
        "        'Improvement (%)': 0,\n",
        "        'Status': 'BASELINE',\n",
        "        'Rank': 0\n",
        "    })\n",
        "\n",
        "    # Add ML models\n",
        "    for _, row in ml_results.iterrows():\n",
        "        if row['Status'] == 'Success' and row['MAE'] < 99999:\n",
        "            improvement = baseline_mae - row['MAE']\n",
        "            improvement_pct = (improvement / baseline_mae) * 100\n",
        "\n",
        "            results_table.append({\n",
        "                'Model': row['Model'],\n",
        "                'MAE': row['MAE'],\n",
        "                'RMSE': row.get('RMSE', np.nan),\n",
        "                'RÂ²': row.get('R2', np.nan),\n",
        "                'MAPE (%)': row.get('MAPE', np.nan),\n",
        "                'vs Baseline MAE': improvement,\n",
        "                'Improvement (%)': improvement_pct,\n",
        "                'Status': 'âœ… BEATS' if improvement > 0 else 'âŒ FAILS',\n",
        "                'Rank': 0\n",
        "            })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    results_df = pd.DataFrame(results_table)\n",
        "\n",
        "    # Rank by MAE (excluding baseline)\n",
        "    ml_only = results_df[results_df['Model'] != 'ğŸ“Š Day-of-Week Average (7-day)'].copy()\n",
        "    ml_only['Rank'] = ml_only['MAE'].rank().astype(int)\n",
        "    results_df.loc[results_df['Model'] != 'ğŸ“Š Day-of-Week Average (7-day)', 'Rank'] = ml_only['Rank'].values\n",
        "\n",
        "    # Sort by MAE\n",
        "    results_df = results_df.sort_values('MAE')\n",
        "\n",
        "    # Display formatted table\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"COMPLETE PERFORMANCE METRICS TABLE - ALL MODELS VS Day-of-Week Average\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    # Format for display\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', None)\n",
        "    pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    # Summary statistics\n",
        "    ml_models = results_df[results_df['Model'] != 'ğŸ“Š Day-of-Week Average (7-day)']\n",
        "    beat_baseline = ml_models[ml_models['Improvement (%)'] > 0]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\"*100)\n",
        "    print(f\"Baseline MAE: {baseline_mae:.2f}\")\n",
        "    print(f\"Total ML Models: {len(ml_models)}\")\n",
        "    print(f\"Models Beating Baseline: {len(beat_baseline)} ({len(beat_baseline)/len(ml_models)*100:.1f}%)\")\n",
        "    print(f\"Best Model: {ml_models.iloc[0]['Model']} (MAE: {ml_models.iloc[0]['MAE']:.2f})\")\n",
        "    print(f\"Best Improvement: {ml_models['Improvement (%)'].max():.1f}%\")\n",
        "    print(f\"Worst Performance: {ml_models['Improvement (%)'].min():.1f}%\")\n",
        "\n",
        "    # Performance by category\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"PERFORMANCE BREAKDOWN\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    beat_count = len(beat_baseline)\n",
        "    fail_count = len(ml_models) - beat_count\n",
        "\n",
        "    print(f\"\\n{'Category':<20} {'Count':<10} {'Percentage':<15} {'Best MAE':<15}\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"{'âœ… Beat Baseline':<20} {beat_count:<10} {beat_count/len(ml_models)*100:<15.1f}% {beat_baseline['MAE'].min() if len(beat_baseline) > 0 else 'N/A':<15.2f}\")\n",
        "    print(f\"{'âŒ Failed vs Base':<20} {fail_count:<10} {fail_count/len(ml_models)*100:<15.1f}% {ml_models[ml_models['Improvement (%)'] < 0]['MAE'].min() if fail_count > 0 else 'N/A':<15.2f}\")\n",
        "\n",
        "    # Save enhanced results\n",
        "    output_file = \"comprehensive_metrics_table.csv\"\n",
        "    results_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nğŸ’¾ Full metrics table saved to: {output_file}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: all_models_results_calls.csv not found!\")\n",
        "    print(\"Please run the ML pipeline first to generate results.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: {str(e)}\")"
      ],
      "metadata": {
        "id": "yIQth8UnU_lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GWFQyiJfU_v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EXECUTIVE SUMMARY - FIXED VERSION\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ“Š GENERATING EXECUTIVE SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load your actual data and results\n",
        "data = pd.read_csv(\"enhanced_eda_data.csv\")\n",
        "if \"Date\" in data.columns:\n",
        "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
        "    data = data.set_index(\"Date\").sort_index()\n",
        "\n",
        "# Calculate Day-of-Week Average\n",
        "target = data[\"calls\"]\n",
        "train_size = int(len(target) * 0.75)\n",
        "train_data = target.iloc[:train_size]\n",
        "test_data = target.iloc[train_size:]\n",
        "\n",
        "seasonal_predictions = []\n",
        "for i in range(len(test_data)):\n",
        "    lookup_idx = train_size + i - 7\n",
        "    if lookup_idx >= 0 and lookup_idx < len(target):\n",
        "        seasonal_predictions.append(target.iloc[lookup_idx])\n",
        "    else:\n",
        "        seasonal_predictions.append(train_data.iloc[-1])\n",
        "\n",
        "seasonal_predictions = np.array(seasonal_predictions)\n",
        "baseline_mae = mean_absolute_error(test_data, seasonal_predictions)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(test_data, seasonal_predictions))\n",
        "\n",
        "# Load ML results\n",
        "ml_results = pd.read_csv(\"all_models_results_calls.csv\")\n",
        "\n",
        "# Prepare results dataframe with required columns\n",
        "results_df = ml_results[ml_results['Status'] == 'Success'].copy()\n",
        "results_df['MAE_Improvement'] = ((baseline_mae - results_df['MAE']) / baseline_mae) * 100\n",
        "results_df['Beats_Baseline'] = results_df['MAE'] < baseline_mae\n",
        "results_df = results_df.sort_values('MAE')\n",
        "\n",
        "# Create executive dashboard\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "fig.suptitle('ML FORECASTING EXECUTIVE DASHBOARD\\nCall Center Volume Prediction Analysis',\n",
        "            fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. KEY METRICS (top left)\n",
        "ax1 = fig.add_subplot(2, 3, 1)\n",
        "ax1.axis('off')\n",
        "\n",
        "total_models = len(results_df)\n",
        "winners = results_df['Beats_Baseline'].sum()\n",
        "win_rate = (winners / total_models) * 100 if total_models > 0 else 0\n",
        "best_model = results_df.iloc[0] if len(results_df) > 0 else None\n",
        "\n",
        "# FIXED: Proper formatting\n",
        "if best_model is not None:\n",
        "    champion_name = best_model['Model']\n",
        "    champion_mae = f\"{best_model['MAE']:.1f}\"\n",
        "    champion_improvement = f\"{best_model['MAE_Improvement']:.1f}\"\n",
        "else:\n",
        "    champion_name = \"None\"\n",
        "    champion_mae = \"N/A\"\n",
        "    champion_improvement = \"N/A\"\n",
        "\n",
        "metrics_text = f\"\"\"KEY PERFORMANCE METRICS\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "Baseline MAE: {baseline_mae:.1f}\n",
        "Total Models: {total_models}\n",
        "\n",
        "SUCCESS METRICS\n",
        "Models Beating Baseline: {winners}/{total_models}\n",
        "Success Rate: {win_rate:.1f}%\n",
        "\n",
        "CHAMPION MODEL\n",
        "{champion_name}\n",
        "MAE: {champion_mae}\n",
        "Improvement: {champion_improvement}%\"\"\"\n",
        "\n",
        "ax1.text(0.05, 0.95, metrics_text, transform=ax1.transAxes,\n",
        "        fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
        "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
        "\n",
        "# 2. MODEL PERFORMANCE COMPARISON (top center)\n",
        "ax2 = fig.add_subplot(2, 3, 2)\n",
        "if len(results_df) > 0:\n",
        "    top_models = results_df.head(10)\n",
        "    colors = ['green' if x else 'red' for x in top_models['Beats_Baseline']]\n",
        "    bars = ax2.barh(range(len(top_models)), top_models['MAE'], color=colors, alpha=0.7)\n",
        "    ax2.set_yticks(range(len(top_models)))\n",
        "    ax2.set_yticklabels([m[:20] for m in top_models['Model']], fontsize=8)\n",
        "    ax2.axvline(x=baseline_mae, color='black', linestyle='--', label='Baseline', linewidth=2)\n",
        "    ax2.set_xlabel('MAE')\n",
        "    ax2.set_title('Top 10 Models vs Baseline')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 3. SUCCESS RATE GAUGE (top right)\n",
        "ax3 = fig.add_subplot(2, 3, 3)\n",
        "if win_rate >= 60:\n",
        "    color = 'green'\n",
        "    status = 'GOOD'\n",
        "elif win_rate >= 40:\n",
        "    color = 'orange'\n",
        "    status = 'FAIR'\n",
        "else:\n",
        "    color = 'red'\n",
        "    status = 'POOR'\n",
        "\n",
        "# Create a pie chart as gauge\n",
        "sizes = [win_rate, 100-win_rate]\n",
        "colors_pie = [color, 'lightgray']\n",
        "explode = (0.1, 0)\n",
        "ax3.pie(sizes, explode=explode, colors=colors_pie, autopct='',\n",
        "        startangle=90, counterclock=False)\n",
        "ax3.text(0, 0, f'{win_rate:.0f}%', ha='center', va='center',\n",
        "         fontsize=24, fontweight='bold')\n",
        "ax3.text(0, -0.3, status, ha='center', va='center',\n",
        "         fontsize=12, fontweight='bold', color=color)\n",
        "ax3.set_title('Model Success Rate')\n",
        "\n",
        "# 4. TOP 5 MODELS TABLE (bottom left)\n",
        "ax4 = fig.add_subplot(2, 3, 4)\n",
        "ax4.axis('tight')\n",
        "ax4.axis('off')\n",
        "\n",
        "if len(results_df) > 0:\n",
        "    table_data = [['Rank', 'Model', 'MAE', 'Improvement']]\n",
        "    for i, (_, row) in enumerate(results_df.head(5).iterrows(), 1):\n",
        "        model_name = row['Model'][:25] + '...' if len(row['Model']) > 25 else row['Model']\n",
        "        table_data.append([\n",
        "            str(i),\n",
        "            model_name,\n",
        "            f\"{row['MAE']:.1f}\",\n",
        "            f\"{row['MAE_Improvement']:.1f}%\"\n",
        "        ])\n",
        "\n",
        "    table = ax4.table(cellText=table_data, cellLoc='left', loc='center',\n",
        "                     colWidths=[0.1, 0.5, 0.2, 0.2])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(9)\n",
        "    table.scale(1.2, 2)\n",
        "\n",
        "    # Color header\n",
        "    for i in range(4):\n",
        "        table[(0, i)].set_facecolor('#4CAF50')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "ax4.set_title('Top 5 Models Leaderboard', fontweight='bold')\n",
        "\n",
        "# 5. MODEL TYPE ANALYSIS (bottom center)\n",
        "ax5 = fig.add_subplot(2, 3, 5)\n",
        "model_types = {\n",
        "    'Tree': ['Forest', 'Extra', 'Tree'],\n",
        "    'Boosting': ['Gradient', 'XGBoost', 'LightGBM'],\n",
        "    'Linear': ['Ridge', 'Lasso', 'Elastic'],\n",
        "    'Other': ['SVR', 'Neighbor', 'Kernel', 'MLP']\n",
        "}\n",
        "\n",
        "type_performance = []\n",
        "for category, keywords in model_types.items():\n",
        "    cat_models = results_df[results_df['Model'].str.contains('|'.join(keywords), case=False, na=False)]\n",
        "    if len(cat_models) > 0:\n",
        "        type_performance.append({\n",
        "            'Type': category,\n",
        "            'Win_Rate': (cat_models['Beats_Baseline'].sum() / len(cat_models)) * 100,\n",
        "            'Count': len(cat_models),\n",
        "            'Winners': cat_models['Beats_Baseline'].sum()\n",
        "        })\n",
        "\n",
        "if type_performance:\n",
        "    type_df = pd.DataFrame(type_performance).sort_values('Win_Rate', ascending=False)\n",
        "    colors = ['green' if x >= 50 else 'orange' if x > 0 else 'red' for x in type_df['Win_Rate']]\n",
        "    bars = ax5.bar(type_df['Type'], type_df['Win_Rate'], color=colors, alpha=0.7)\n",
        "    ax5.axhline(y=50, color='black', linestyle='--', alpha=0.5, label='50% threshold')\n",
        "    ax5.set_ylabel('Success Rate %')\n",
        "    ax5.set_title('Performance by Model Type')\n",
        "    ax5.set_ylim(0, 110)\n",
        "\n",
        "    # Add count labels\n",
        "    for bar, (_, row) in zip(bars, type_df.iterrows()):\n",
        "        height = bar.get_height()\n",
        "        ax5.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
        "                f'{row[\"Winners\"]}/{row[\"Count\"]}',\n",
        "                ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# 6. RECOMMENDATIONS (bottom right)\n",
        "ax6 = fig.add_subplot(2, 3, 6)\n",
        "ax6.axis('off')\n",
        "\n",
        "if win_rate >= 60:\n",
        "    rec_text = \"\"\"âœ… RECOMMENDATION: DEPLOY\n",
        "\n",
        "- ML significantly outperforms baseline\n",
        "- Deploy champion model to production\n",
        "- Monitor performance daily\n",
        "- Set up retraining pipeline\"\"\"\n",
        "    rec_color = 'lightgreen'\n",
        "elif win_rate >= 40:\n",
        "    rec_text = \"\"\"âš ï¸ RECOMMENDATION: CONDITIONAL\n",
        "\n",
        "- Mixed performance vs baseline\n",
        "- Test champion in staging first\n",
        "- Keep baseline as fallback\n",
        "- Improve feature engineering\"\"\"\n",
        "    rec_color = 'lightyellow'\n",
        "else:\n",
        "    rec_text = \"\"\"âŒ RECOMMENDATION: HOLD\n",
        "\n",
        "- Limited improvement over baseline\n",
        "- Continue model development\n",
        "- Review data quality\n",
        "- Consider ensemble approaches\"\"\"\n",
        "    rec_color = 'lightcoral'\n",
        "\n",
        "ax6.text(0.5, 0.5, rec_text, transform=ax6.transAxes,\n",
        "        fontsize=10, ha='center', va='center',\n",
        "        bbox=dict(boxstyle='round', facecolor=rec_color, alpha=0.3))\n",
        "ax6.set_title('Deployment Recommendation', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Generate text report\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXECUTIVE REPORT\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "print(\"SUMMARY\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"â€¢ Baseline Performance: MAE = {baseline_mae:.1f}\")\n",
        "print(f\"â€¢ Models Evaluated: {total_models}\")\n",
        "print(f\"â€¢ Models Beating Baseline: {winners} ({win_rate:.1f}%)\")\n",
        "if best_model is not None:\n",
        "    print(f\"â€¢ Champion Model: {best_model['Model']}\")\n",
        "    print(f\"â€¢ Champion Performance: MAE = {best_model['MAE']:.1f} ({best_model['MAE_Improvement']:.1f}% improvement)\")\n",
        "\n",
        "print(\"\\nTOP 5 MODELS\")\n",
        "print(\"-\" * 40)\n",
        "for i, (_, row) in enumerate(results_df.head(5).iterrows(), 1):\n",
        "    status_icon = \"âœ…\" if row['Beats_Baseline'] else \"âŒ\"\n",
        "    print(f\"{i}. {status_icon} {row['Model']}: MAE = {row['MAE']:.1f} ({row['MAE_Improvement']:+.1f}%)\")\n",
        "\n",
        "print(\"\\nMODEL TYPE SUMMARY\")\n",
        "print(\"-\" * 40)\n",
        "if type_performance:\n",
        "    for entry in sorted(type_performance, key=lambda x: x['Win_Rate'], reverse=True):\n",
        "        print(f\"â€¢ {entry['Type']}: {entry['Winners']}/{entry['Count']} models beat baseline ({entry['Win_Rate']:.0f}%)\")\n",
        "\n",
        "print(\"\\nRECOMMENDATION\")\n",
        "print(\"-\" * 40)\n",
        "if win_rate >= 60:\n",
        "    print(\"âœ… Deploy champion model with monitoring\")\n",
        "elif win_rate >= 40:\n",
        "    print(\"âš ï¸ Test in staging environment first\")\n",
        "else:\n",
        "    print(\"âŒ Continue development, use baseline for now\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âœ… EXECUTIVE SUMMARY COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save results\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_filename = f'final_ml_results_{timestamp}.csv'\n",
        "results_df.to_csv(results_filename, index=False)\n",
        "print(f\"\\nğŸ’¾ Results saved to: {results_filename}\")\n",
        "\n",
        "# Save the figure\n",
        "fig.savefig(f'ml_dashboard_{timestamp}.png', dpi=150, bbox_inches='tight')\n",
        "print(f\"ğŸ“Š Dashboard saved to: ml_dashboard_{timestamp}.png\")"
      ],
      "metadata": {
        "id": "bFm6FTtWU_z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EXECUTIVE SUMMARY & PERFORMANCE DASHBOARD\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ“Š GENERATING EXECUTIVE SUMMARY & DASHBOARD\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "class ExecutiveDashboardGenerator:\n",
        "    def __init__(self, results_df, baseline_mae):\n",
        "        self.results_df = results_df\n",
        "        self.baseline_mae = baseline_mae\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    def create_performance_dashboard(self):\n",
        "        # Create comprehensive dashboard\n",
        "        fig = plt.figure(figsize=(16, 10))\n",
        "        fig.suptitle('ML FORECASTING PERFORMANCE DASHBOARD\\nCall Center Volume Prediction Analysis',\n",
        "                    fontsize=14, fontweight='bold')\n",
        "\n",
        "        # 1. Model Performance vs Baseline\n",
        "        ax1 = plt.subplot(2, 3, 1)\n",
        "        sorted_results = self.results_df.sort_values('MAE')\n",
        "        colors = ['green' if x else 'red' for x in sorted_results['Beats_Baseline']]\n",
        "        bars = ax1.bar(range(len(sorted_results)), sorted_results['MAE'], color=colors, alpha=0.7)\n",
        "        ax1.axhline(y=self.baseline_mae, color='blue', linestyle='--', label='Baseline', linewidth=2)\n",
        "        ax1.set_xlabel('Models (sorted by MAE)')\n",
        "        ax1.set_ylabel('MAE')\n",
        "        ax1.set_title(f'All {len(sorted_results)} Models vs Baseline')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Success Rate by Phase\n",
        "        ax2 = plt.subplot(2, 3, 2)\n",
        "        phase_stats = []\n",
        "        phase_names = []\n",
        "        for phase in ['V1', 'V2', 'VP']:\n",
        "            phase_df = self.results_df[self.results_df['Phase'] == phase] if 'Phase' in self.results_df.columns else pd.DataFrame()\n",
        "            if len(phase_df) > 0:\n",
        "                win_rate = (phase_df['Beats_Baseline'].sum() / len(phase_df)) * 100\n",
        "                phase_stats.append(win_rate)\n",
        "                phase_names.append(phase)\n",
        "\n",
        "        if phase_stats:\n",
        "            bars = ax2.bar(phase_names, phase_stats, color=['skyblue', 'lightgreen', 'coral'], alpha=0.7)\n",
        "            ax2.set_ylabel('Success Rate %')\n",
        "            ax2.set_title('Success Rate by Phase')\n",
        "            for bar, rate in zip(bars, phase_stats):\n",
        "                height = bar.get_height()\n",
        "                ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                        f'{rate:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "        # 3. Top 10 Models\n",
        "        ax3 = plt.subplot(2, 3, 3)\n",
        "        top10 = sorted_results.head(10)\n",
        "        y_pos = np.arange(len(top10))\n",
        "        colors_top = ['green' if x else 'red' for x in top10['Beats_Baseline']]\n",
        "        ax3.barh(y_pos, top10['MAE'], color=colors_top, alpha=0.7)\n",
        "        ax3.axvline(x=self.baseline_mae, color='blue', linestyle='--', linewidth=2)\n",
        "        ax3.set_yticks(y_pos)\n",
        "        ax3.set_yticklabels([name[:20] for name in top10['Model']])\n",
        "        ax3.set_xlabel('MAE')\n",
        "        ax3.set_title('Top 10 Models')\n",
        "        ax3.invert_yaxis()\n",
        "\n",
        "        # 4. Improvement Distribution\n",
        "        ax4 = plt.subplot(2, 3, 4)\n",
        "        improvements = self.results_df['Improvement_Pct'] if 'Improvement_Pct' in self.results_df.columns else self.results_df.get('MAE_Improvement', [])\n",
        "        if len(improvements) > 0:\n",
        "            ax4.hist(improvements, bins=20, color='teal', alpha=0.7, edgecolor='black')\n",
        "            ax4.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No improvement')\n",
        "            ax4.set_xlabel('MAE Improvement %')\n",
        "            ax4.set_ylabel('Count')\n",
        "            ax4.set_title('Improvement Distribution')\n",
        "            ax4.legend()\n",
        "\n",
        "        # 5. Performance Summary Table\n",
        "        ax5 = plt.subplot(2, 3, 5)\n",
        "        ax5.axis('tight')\n",
        "        ax5.axis('off')\n",
        "\n",
        "        # Create summary table\n",
        "        total_models = len(self.results_df)\n",
        "        winners = self.results_df['Beats_Baseline'].sum()\n",
        "        win_rate = (winners / total_models) * 100\n",
        "        best_model = self.results_df.iloc[0]\n",
        "\n",
        "        table_data = [\n",
        "            ['Metric', 'Value'],\n",
        "            ['Total Models', str(total_models)],\n",
        "            ['Beat Baseline', f\"{winners}/{total_models}\"],\n",
        "            ['Success Rate', f\"{win_rate:.1f}%\"],\n",
        "            ['Champion Model', best_model['Model'][:30]],\n",
        "            ['Champion MAE', f\"{best_model['MAE']:.3f}\"],\n",
        "            ['Improvement', f\"{best_model.get('Improvement_Pct', best_model.get('MAE_Improvement', 0)):.1f}%\"]\n",
        "        ]\n",
        "\n",
        "        table = ax5.table(cellText=table_data, cellLoc='left', loc='center')\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(9)\n",
        "        table.scale(1.2, 1.5)\n",
        "\n",
        "        # Style header\n",
        "        table[(0, 0)].set_facecolor('#4CAF50')\n",
        "        table[(0, 1)].set_facecolor('#4CAF50')\n",
        "        table[(0, 0)].set_text_props(weight='bold', color='white')\n",
        "        table[(0, 1)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "        ax5.set_title('Performance Summary', fontweight='bold')\n",
        "\n",
        "        # 6. Model Type Analysis\n",
        "        ax6 = plt.subplot(2, 3, 6)\n",
        "        model_types = {\n",
        "            'Tree': ['RandomForest', 'ExtraTrees', 'DecisionTree'],\n",
        "            'Boosting': ['GradientBoosting', 'XGBoost', 'LightGBM'],\n",
        "            'Linear': ['Ridge', 'Lasso', 'ElasticNet'],\n",
        "            'Other': ['SVR', 'Neural', 'MLP', 'KNeighbors']\n",
        "        }\n",
        "\n",
        "        type_performance = []\n",
        "        for category, keywords in model_types.items():\n",
        "            cat_models = self.results_df[self.results_df['Model'].str.contains('|'.join(keywords), case=False, na=False)]\n",
        "            if len(cat_models) > 0:\n",
        "                win_rate_type = (cat_models['Beats_Baseline'].sum() / len(cat_models)) * 100\n",
        "                type_performance.append({'Type': category, 'Win_Rate': win_rate_type})\n",
        "\n",
        "        if type_performance:\n",
        "            type_df = pd.DataFrame(type_performance)\n",
        "            bars = ax6.bar(type_df['Type'], type_df['Win_Rate'],\n",
        "                          color=['green' if x >= 50 else 'red' for x in type_df['Win_Rate']], alpha=0.7)\n",
        "            ax6.set_ylabel('Success Rate %')\n",
        "            ax6.set_title('Success by Model Type')\n",
        "            ax6.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def generate_executive_report(self):\n",
        "        total_models = len(self.results_df)\n",
        "        winners = self.results_df['Beats_Baseline'].sum()\n",
        "        win_rate = (winners / total_models) * 100\n",
        "        best_model = self.results_df.iloc[0]\n",
        "\n",
        "        report = f\"\"\"\n",
        "ML FORECASTING EXECUTIVE REPORT\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "{'='*60}\n",
        "\n",
        "EXECUTIVE SUMMARY:\n",
        "- Baseline (Day-of-Week Average): MAE = {self.baseline_mae:.3f}\n",
        "- Total Models Evaluated: {total_models}\n",
        "- Models Beating Baseline: {winners}/{total_models} ({win_rate:.1f}%)\n",
        "- Champion Model: {best_model['Model']}\n",
        "- Champion Performance: MAE = {best_model['MAE']:.3f} ({best_model.get('Improvement_Pct', best_model.get('MAE_Improvement', 0)):.1f}% improvement)\n",
        "\n",
        "TOP 5 MODELS:\n",
        "\"\"\"\n",
        "        for i, (_, row) in enumerate(self.results_df.head(5).iterrows(), 1):\n",
        "            status = \"âœ…\" if row['Beats_Baseline'] else \"âŒ\"\n",
        "            improvement = row.get('Improvement_Pct', row.get('MAE_Improvement', 0))\n",
        "            report += f\"{i}. {status} {row['Model']} - MAE: {row['MAE']:.3f} ({improvement:+.1f}%)\\n\"\n",
        "\n",
        "        report += f\"\"\"\n",
        "RECOMMENDATIONS:\n",
        "\"\"\"\n",
        "        if win_rate >= 60:\n",
        "            report += \"âœ… Strong performance - Deploy champion model\\n\"\n",
        "            report += \"âœ… Consider ensemble of top models\\n\"\n",
        "        elif win_rate >= 40:\n",
        "            report += \"âš ï¸ Mixed performance - Test champion in staging\\n\"\n",
        "            report += \"âš ï¸ Keep baseline as fallback option\\n\"\n",
        "        else:\n",
        "            report += \"âŒ Limited improvement over baseline\\n\"\n",
        "            report += \"âŒ Consider using Day-of-Week Average baseline\\n\"\n",
        "\n",
        "        return report\n",
        "\n",
        "# Generate executive summary\n",
        "try:\n",
        "    # Try to load results from the most recent analysis\n",
        "    if 'ml_results' in locals():\n",
        "        results_data = ml_results.copy()\n",
        "    else:\n",
        "        # Load from CSV if available\n",
        "        results_data = pd.read_csv(\"all_models_results_calls.csv\")\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    if 'Beats_Baseline' not in results_data.columns:\n",
        "        baseline_mae_value = baseline_mae if 'baseline_mae' in locals() else 800  # fallback\n",
        "        results_data['Beats_Baseline'] = results_data['MAE'] < baseline_mae_value\n",
        "\n",
        "    dashboard_gen = ExecutiveDashboardGenerator(results_data, baseline_mae if 'baseline_mae' in locals() else 800)\n",
        "\n",
        "    # Create dashboard\n",
        "    print(\"\\nCreating performance dashboard...\")\n",
        "    dashboard_fig = dashboard_gen.create_performance_dashboard()\n",
        "    plt.show()\n",
        "\n",
        "    # Generate report\n",
        "    print(\"\\nGenerating executive report...\")\n",
        "    executive_report = dashboard_gen.generate_executive_report()\n",
        "    print(executive_report)\n",
        "\n",
        "    # Save files\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    dashboard_filename = f'ml_performance_dashboard_{timestamp}.png'\n",
        "    dashboard_fig.savefig(dashboard_filename, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    report_filename = f'executive_report_{timestamp}.txt'\n",
        "    with open(report_filename, 'w') as f:\n",
        "        f.write(executive_report)\n",
        "\n",
        "    print(f\"\\nğŸ’¾ Dashboard saved: {dashboard_filename}\")\n",
        "    print(f\"ğŸ’¾ Report saved: {report_filename}\")\n",
        "\n",
        "    print(\"\\nğŸ‰ EXECUTIVE SUMMARY COMPLETE!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Could not generate executive summary: {str(e)}\")\n",
        "    print(\"Make sure you have run the ML pipeline and have results available.\")"
      ],
      "metadata": {
        "id": "CiTluiJJU_4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-KtcDm0wU_8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOU DONT NEED THIS #####"
      ],
      "metadata": {
        "id": "a0vakE4QU__-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9_-CZYaVAGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================\n",
        "# DIAGNOSTIC VERSION - VERIFY GRIDSEARCHCV IS ACTUALLY RUNNING\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def diagnostic_gridsearch_test():\n",
        "    \"\"\"Test to verify GridSearchCV is actually running and taking time\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"DIAGNOSTIC TEST - VERIFYING GRIDSEARCHCV EXECUTION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create synthetic data\n",
        "    print(\"\\n1. Creating test data...\")\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(300, 20)  # 300 samples, 20 features\n",
        "    y = np.random.randn(300)\n",
        "\n",
        "    # Split data\n",
        "    train_size = int(len(X) * 0.75)\n",
        "    X_train = X[:train_size]\n",
        "    X_test = X[train_size:]\n",
        "    y_train = y[:train_size]\n",
        "    y_test = y[train_size:]\n",
        "\n",
        "    print(f\"   Train shape: {X_train.shape}\")\n",
        "    print(f\"   Test shape: {X_test.shape}\")\n",
        "\n",
        "    # Define parameter grid\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'max_depth': [5, 10, 15],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    # Calculate total combinations\n",
        "    total_combinations = 3 * 3 * 3 * 3  # 81 combinations\n",
        "    cv_splits = 5\n",
        "    total_fits = total_combinations * cv_splits  # 405 fits\n",
        "\n",
        "    print(f\"\\n2. Parameter grid:\")\n",
        "    print(f\"   Combinations: {total_combinations}\")\n",
        "    print(f\"   CV splits: {cv_splits}\")\n",
        "    print(f\"   Total fits required: {total_fits}\")\n",
        "    print(f\"   This SHOULD take 30-60 seconds...\")\n",
        "\n",
        "    # Create model and GridSearchCV\n",
        "    rf = RandomForestRegressor(random_state=42, n_jobs=1)  # Single thread to see timing\n",
        "    tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=rf,\n",
        "        param_grid=param_grid,\n",
        "        cv=tscv,\n",
        "        scoring='neg_mean_absolute_error',\n",
        "        verbose=2,  # VERBOSE=2 shows each fit\n",
        "        n_jobs=1,   # Single thread to see real timing\n",
        "        return_train_score=True\n",
        "    )\n",
        "\n",
        "    # TIME THE GRIDSEARCH\n",
        "    print(f\"\\n3. Starting GridSearchCV at {pd.Timestamp.now().strftime('%H:%M:%S')}...\")\n",
        "    print(\"   You should see progress output below:\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # FIT - THIS SHOULD TAKE TIME AND SHOW PROGRESS\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    fit_time = time.time() - start_time\n",
        "\n",
        "    print(\"-\"*60)\n",
        "    print(f\"\\n4. GridSearchCV completed at {pd.Timestamp.now().strftime('%H:%M:%S')}\")\n",
        "    print(f\"   Time taken: {fit_time:.1f} seconds\")\n",
        "    print(f\"   Fits performed: {len(grid_search.cv_results_['params'])}\")\n",
        "    print(f\"   Best score: {-grid_search.best_score_:.4f}\")\n",
        "    print(f\"   Best params: {grid_search.best_params_}\")\n",
        "\n",
        "    # Verify it actually searched multiple parameters\n",
        "    unique_scores = len(set(grid_search.cv_results_['mean_test_score']))\n",
        "    print(f\"\\n5. Verification:\")\n",
        "    print(f\"   Unique scores found: {unique_scores} (should be >1 if search worked)\")\n",
        "    print(f\"   Time per fit: {fit_time/total_fits:.3f} seconds\")\n",
        "\n",
        "    if fit_time < 5:\n",
        "        print(\"\\nâš ï¸ WARNING: GridSearch completed too fast!\")\n",
        "        print(\"   Either:\")\n",
        "        print(\"   - GridSearchCV isn't actually running\")\n",
        "        print(\"   - Your machine is extremely fast\")\n",
        "        print(\"   - Something is wrong with the implementation\")\n",
        "    elif unique_scores == 1:\n",
        "        print(\"\\nâš ï¸ WARNING: All parameter combinations gave same score!\")\n",
        "        print(\"   GridSearch may not be working properly\")\n",
        "    else:\n",
        "        print(\"\\nâœ… GridSearchCV is working correctly!\")\n",
        "\n",
        "    return grid_search\n",
        "\n",
        "# RUN THE DIAGNOSTIC TEST\n",
        "print(\"Running diagnostic test...\")\n",
        "print(\"This should take 30-60 seconds if GridSearchCV is working properly\")\n",
        "print()\n",
        "\n",
        "diagnostic_result = diagnostic_gridsearch_test()\n",
        "\n",
        "# Now test with your actual data\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"NOW TESTING WITH YOUR ACTUAL PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check if your data loading functions exist\n",
        "try:\n",
        "    # Try to load your data\n",
        "    print(\"\\nChecking if data loading functions exist...\")\n",
        "    data = load_all_data_columns(CSV_FILE_PATH, TARGET_COLUMN, DATE_COLUMN)\n",
        "    print(f\"âœ… Data loaded: {data.shape}\")\n",
        "\n",
        "    X, y = create_all_features(data, TARGET_COLUMN)\n",
        "    print(f\"âœ… Features created: {X.shape}\")\n",
        "\n",
        "    print(\"\\nYour pipeline SHOULD work, but with this data size:\")\n",
        "    print(f\"  Training samples: {int(len(X)*0.75)}\")\n",
        "    print(f\"  Features: {X.shape[1]}\")\n",
        "    print(\"  Expected time: 15-30 minutes for 30 models with GridSearchCV\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"âŒ ERROR: {str(e)}\")\n",
        "    print(\"\\nFunctions 'load_all_data_columns' and 'create_all_features' are NOT defined!\")\n",
        "    print(\"These must be defined in earlier cells or the pipeline will fail.\")\n",
        "    print(\"\\nMake sure to run the cells that define:\")\n",
        "    print(\"  - load_all_data_columns()\")\n",
        "    print(\"  - create_all_features()\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ ERROR: CSV file '{CSV_FILE_PATH}' not found!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ERROR: {str(e)}\")"
      ],
      "metadata": {
        "id": "zyRYvA3tMl6A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3F-d1_aUyj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Xn4tk5dUynk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1WeDX_mxUyr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0zn_2DzeUyvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}