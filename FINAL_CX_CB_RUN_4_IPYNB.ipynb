{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOBF6PoEHVU5Px9nT5nopx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/FINAL_CX_CB_RUN_4_IPYNB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: Complete Environment Setup\n",
        "import sys\n",
        "import torch\n",
        "import psutil\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# System check\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"Colab: {'google.colab' in sys.modules}\")\n",
        "\n",
        "# GPU check\n",
        "!nvidia-smi\n",
        "GPU_AVAILABLE = torch.cuda.is_available()\n",
        "if GPU_AVAILABLE:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"✗ Using CPU\")\n",
        "\n",
        "# Memory check\n",
        "available_ram_gb = psutil.virtual_memory().available / 1e9\n",
        "total_ram_gb = psutil.virtual_memory().total / 1e9\n",
        "HIGH_MEMORY = available_ram_gb > 16\n",
        "print(f\"✓ RAM: {available_ram_gb:.1f}/{total_ram_gb:.1f} GB ({'HIGH' if HIGH_MEMORY else 'LOW'} memory)\")\n",
        "\n",
        "# Install packages (only what's needed)\n",
        "!pip install -q numpy==1.26.4  # Fix numpy version\n",
        "!pip install -q xgboost lightgbm catboost  # Boosting libraries\n",
        "!pip install -q scikit-learn pandas matplotlib seaborn  # Core ML\n",
        "\n",
        "print(\"\\n✅ Environment ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6l09KxODTm7",
        "outputId": "63c27fd0-fbc4-41d1-8118-5a0505a4c4da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "Colab: True\n",
            "Wed Sep 24 05:21:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "✓ GPU: Tesla T4\n",
            "  Memory: 15.8 GB\n",
            "✓ RAM: 53.0/54.8 GB (HIGH memory)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m136.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "✅ Environment ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Exhaustive pip installs for all 47 ML models + baseline\n",
        "\n",
        "\n",
        "# Core ML libraries (our models)\n",
        "!pip install -q xgboost      # For XGBoost_V1, XGBoost_V2\n",
        "!pip install -q lightgbm     # For LightGBM_V1, LightGBM_VP\n",
        "!pip install -q catboost     # For CatBoost_V1, CatBoost_VP\n",
        "\n",
        "# Additional useful packages\n",
        "!pip install -q scikit-learn --upgrade  # Ensure latest sklearn for HistGradientBoostingRegressor\n",
        "!pip install -q pandas numpy matplotlib seaborn  # Data handling and visualization\n",
        "!pip install -q tqdm  # Progress bars for long runs\n",
        "\n",
        "# These are from original notebook but NOT needed for our ML models:\n",
        "# !pip install tensorflow  # Not using TF for our models\n",
        "# !pip install tbats      # Time series specific - not needed\n",
        "# !pip install pmdarima   # Time series specific - not needed\n",
        "\n",
        "print(\"✓ All packages for 47 ML models installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtftdrV3qN66",
        "outputId": "e10e3f39-d3fb-4839-9b22-65ea6803d96c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✓ All packages for 47 ML models installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "Mhga7tyQaaEJ",
        "outputId": "1102441b-e9f0-431e-aebf-f8f882de0c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "b81c7146c99e4ab5b58311610eb3811c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2.5: Import All Libraries and Check Availability\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Track what's available\n",
        "AVAILABLE_MODELS = {'base': True}  # sklearn is always available\n",
        "\n",
        "# Core sklearn\n",
        "from sklearn.ensemble import (RandomForestRegressor, ExtraTreesRegressor,\n",
        "                             GradientBoostingRegressor, BaggingRegressor,\n",
        "                             AdaBoostRegressor, VotingRegressor, StackingRegressor,\n",
        "                             HistGradientBoostingRegressor)\n",
        "from sklearn.linear_model import (Ridge, Lasso, ElasticNet, HuberRegressor,\n",
        "                                  BayesianRidge, ARDRegression)\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "print(\"✓ Sklearn models loaded\")\n",
        "\n",
        "# Optional libraries\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    AVAILABLE_MODELS['xgboost'] = True\n",
        "    print(\"✓ XGBoost available\")\n",
        "except:\n",
        "    AVAILABLE_MODELS['xgboost'] = False\n",
        "    print(\"✗ XGBoost not available\")\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    AVAILABLE_MODELS['lightgbm'] = True\n",
        "    print(\"✓ LightGBM available\")\n",
        "except:\n",
        "    AVAILABLE_MODELS['lightgbm'] = False\n",
        "    print(\"✗ LightGBM not available\")\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "    AVAILABLE_MODELS['catboost'] = True\n",
        "    print(\"✓ CatBoost available\")\n",
        "except:\n",
        "    AVAILABLE_MODELS['catboost'] = False\n",
        "    print(\"✗ CatBoost not available\")\n",
        "\n",
        "print(f\"\\nTotal available model types: {sum(AVAILABLE_MODELS.values())}/4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXK8ukObpFBU",
        "outputId": "bfcc2df1-929f-4baa-c54b-7a682a632f26"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Sklearn models loaded\n",
            "✓ XGBoost available\n",
            "✓ LightGBM available\n",
            "✓ CatBoost available\n",
            "\n",
            "Total available model types: 4/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dTGLk8IdqL5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Enhanced imports with debugging setup (following notebook structure)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import time\n",
        "import logging\n",
        "import sys\n",
        "import traceback\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Optional, Tuple, Any, Union\n",
        "from dataclasses import dataclass\n",
        "import itertools\n",
        "\n",
        "# Configure comprehensive logging for notebook visibility\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"DEBUG LOGGING INITIALIZED\")\n",
        "\n",
        "# Core ML imports\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Tree-based models\n",
        "from sklearn.ensemble import (RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor,\n",
        "                             BaggingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor,\n",
        "                             HistGradientBoostingRegressor)\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Check optional libraries with detailed reporting\n",
        "logger.info(\"Checking optional ML libraries...\")\n",
        "\n",
        "XGB_AVAILABLE = False\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGB_AVAILABLE = True\n",
        "    logger.info(f\"XGBoost available: {xgb.__version__}\")\n",
        "except ImportError:\n",
        "    logger.warning(\"XGBoost not available\")\n",
        "\n",
        "LGB_AVAILABLE = False\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGB_AVAILABLE = True\n",
        "    logger.info(f\"LightGBM available: {lgb.__version__}\")\n",
        "except ImportError:\n",
        "    logger.warning(\"LightGBM not available\")\n",
        "\n",
        "CATBOOST_AVAILABLE = False\n",
        "try:\n",
        "    import catboost as cb\n",
        "    CATBOOST_AVAILABLE = True\n",
        "    logger.info(f\"CatBoost available: {cb.__version__}\")\n",
        "except ImportError:\n",
        "    logger.warning(\"CatBoost not available\")\n",
        "\n",
        "# Linear models\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression, Ridge, Lasso, ElasticNet,\n",
        "    BayesianRidge, ARDRegression, HuberRegressor,\n",
        "    SGDRegressor, PassiveAggressiveRegressor\n",
        ")\n",
        "\n",
        "# Neural networks\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Other models\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "# Suppress sklearn warnings but keep our logging\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# Set reproducible seed\n",
        "np.random.seed(42)\n",
        "\n",
        "logger.info(\"All imports completed successfully\")\n",
        "print(f\"\\nLIBRARY STATUS:\")\n",
        "print(f\"  XGBoost: {'✅' if XGB_AVAILABLE else '❌'}\")\n",
        "print(f\"  LightGBM: {'✅' if LGB_AVAILABLE else '❌'}\")\n",
        "print(f\"  CatBoost: {'✅' if CATBOOST_AVAILABLE else '❌'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUPCPpa4rHOI",
        "outputId": "499aa9f5-827f-47f5-e8b8-1b425c88b9fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LIBRARY STATUS:\n",
            "  XGBoost: ✅\n",
            "  LightGBM: ✅\n",
            "  CatBoost: ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Configuration for our ML models + baseline\n",
        "import psutil\n",
        "\n",
        "# Check memory availability for this cell\n",
        "available_ram_gb = psutil.virtual_memory().available / 1e9\n",
        "HIGH_MEMORY = available_ram_gb > 16\n",
        "\n",
        "@dataclass\n",
        "class DebugMLForecastingConfig:\n",
        "    \"\"\"Configuration for general-purpose ML pipeline\"\"\"\n",
        "\n",
        "    # Data parameters\n",
        "    target_column: str = \"calls\"\n",
        "    date_column: str = \"date\"\n",
        "    test_split_ratio: float = 0.75\n",
        "    validation_split_ratio: float = 0.15\n",
        "\n",
        "    # Feature engineering\n",
        "    max_lags: int = 14\n",
        "    rolling_windows: List[int] = None\n",
        "    create_technical_indicators: bool = True\n",
        "    create_calendar_features: bool = True\n",
        "\n",
        "    # Model families to use\n",
        "    use_tree_models: bool = True\n",
        "    use_linear_models: bool = True\n",
        "    use_neural_models: bool = True\n",
        "    use_ensemble_models: bool = True\n",
        "    use_other_models: bool = True\n",
        "    use_gradient_boosting: bool = True\n",
        "\n",
        "    # Version control\n",
        "    run_v1_models: bool = True\n",
        "    run_v2_models: bool = True\n",
        "    run_vp_optimization: bool = True\n",
        "\n",
        "    # GridSearchCV parameters\n",
        "    cv_splits: int = 3\n",
        "    parallel_jobs: int = 1  # Single job for debugging\n",
        "    scoring_metric: str = 'neg_mean_absolute_error'\n",
        "    gridsearch_verbose: int = 2\n",
        "\n",
        "    # Optimization settings\n",
        "    quick_search: bool = False\n",
        "    detailed_search: bool = True\n",
        "    top_models_for_optimization: int = 5\n",
        "\n",
        "    # Debug settings\n",
        "    debug_mode: bool = True\n",
        "    show_progress: bool = True\n",
        "    time_each_phase: bool = True\n",
        "    save_intermediate_results: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Check memory locally\n",
        "        available_ram_gb = psutil.virtual_memory().available / 1e9\n",
        "        high_memory = available_ram_gb > 16\n",
        "\n",
        "        if self.rolling_windows is None:\n",
        "            if high_memory:\n",
        "                self.rolling_windows = [3, 7, 14, 21]\n",
        "                self.max_lags = 21\n",
        "            else:\n",
        "                self.rolling_windows = [3, 7, 14]\n",
        "                self.max_lags = 14\n",
        "\n",
        "        logger.info(f\"Configuration initialized:\")\n",
        "        logger.info(f\"  Target: {self.target_column}\")\n",
        "        logger.info(f\"  Max lags: {self.max_lags}\")\n",
        "        logger.info(f\"  Rolling windows: {self.rolling_windows}\")\n",
        "        logger.info(f\"  V1/V2/VP phases: {self.run_v1_models}/{self.run_v2_models}/{self.run_vp_optimization}\")\n",
        "\n",
        "# Performance monitoring decorator\n",
        "def monitor_performance(func_name: str = None):\n",
        "    \"\"\"Decorator to monitor function execution time\"\"\"\n",
        "    def decorator(func):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            name = func_name or func.__name__\n",
        "            start_time = time.time()\n",
        "            logger.info(f\"🚀 Starting {name}...\")\n",
        "\n",
        "            try:\n",
        "                result = func(*args, **kwargs)\n",
        "                duration = time.time() - start_time\n",
        "                logger.info(f\"✅ {name} completed in {duration:.1f}s\")\n",
        "                return result\n",
        "            except Exception as e:\n",
        "                duration = time.time() - start_time\n",
        "                logger.error(f\"❌ {name} failed after {duration:.1f}s: {str(e)}\")\n",
        "                raise\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "print(\"Configuration and monitoring setup complete\")\n",
        "\n",
        "# Initialize config\n",
        "config = DebugMLForecastingConfig()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76XGjYajrePA",
        "outputId": "eb2701e9-3232-4656-c3ed-2b4e241057ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration and monitoring setup complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Base ML Forecaster with Enhanced Debugging\n",
        "class DebugMLForecaster:\n",
        "    \"\"\"Enhanced ML forecaster with comprehensive debugging\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, model, debug_mode: bool = True):\n",
        "        self.name = name\n",
        "        self.model = clone(model)\n",
        "        self.pipeline = None\n",
        "        self.is_fitted = False\n",
        "        self.feature_importance_ = None\n",
        "        self.debug_mode = debug_mode\n",
        "        self.training_time = None\n",
        "        self.prediction_time = None\n",
        "        self.fit_error = None\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'DebugMLForecaster':\n",
        "        \"\"\"Fit model with comprehensive error handling and timing\"\"\"\n",
        "\n",
        "        fit_start = time.time()\n",
        "\n",
        "        try:\n",
        "            if self.debug_mode:\n",
        "                logger.debug(f\"Fitting {self.name} on data shape: {X.shape}\")\n",
        "\n",
        "            # Validate input data\n",
        "            if X.empty or y.empty:\n",
        "                raise ValueError(f\"Empty input data for {self.name}\")\n",
        "\n",
        "            if len(X) != len(y):\n",
        "                raise ValueError(f\"Feature/target length mismatch for {self.name}: {len(X)} vs {len(y)}\")\n",
        "\n",
        "            # Convert to numpy arrays\n",
        "            X_array = X.values if isinstance(X, pd.DataFrame) else X\n",
        "            y_array = y.values if isinstance(y, pd.Series) else y\n",
        "\n",
        "            # Check for NaN/inf values\n",
        "            if np.any(np.isnan(X_array)) or np.any(np.isinf(X_array)):\n",
        "                logger.warning(f\"{self.name}: Found NaN/inf in features, cleaning...\")\n",
        "                X_array = np.nan_to_num(X_array, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "            if np.any(np.isnan(y_array)) or np.any(np.isinf(y_array)):\n",
        "                logger.warning(f\"{self.name}: Found NaN/inf in target, cleaning...\")\n",
        "                y_array = np.nan_to_num(y_array, nan=np.nanmean(y_array))\n",
        "\n",
        "            # Create and fit pipeline\n",
        "            self.pipeline = Pipeline([\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('model', clone(self.model))\n",
        "            ])\n",
        "\n",
        "            # Fit with error handling\n",
        "            self.pipeline.fit(X_array, y_array.ravel())\n",
        "            self.is_fitted = True\n",
        "\n",
        "            # Test prediction capability\n",
        "            test_pred = self.pipeline.predict(X_array[:min(5, len(X_array))])\n",
        "            if np.any(np.isnan(test_pred)) or np.any(np.isinf(test_pred)):\n",
        "                logger.warning(f\"{self.name}: Model produces invalid predictions\")\n",
        "                self.is_fitted = False\n",
        "\n",
        "        except Exception as e:\n",
        "            self.fit_error = str(e)\n",
        "            logger.error(f\"{self.name} fit failed: {self.fit_error}\")\n",
        "            self.is_fitted = False\n",
        "\n",
        "        finally:\n",
        "            self.training_time = time.time() - fit_start\n",
        "\n",
        "            if self.debug_mode:\n",
        "                status = \"✅ SUCCESS\" if self.is_fitted else \"❌ FAILED\"\n",
        "                logger.info(f\"{self.name}: {status} (Training time: {self.training_time:.2f}s)\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Make predictions with error handling\"\"\"\n",
        "\n",
        "        if not self.is_fitted:\n",
        "            logger.error(f\"{self.name}: Cannot predict - model not fitted\")\n",
        "            return np.zeros(len(X))\n",
        "\n",
        "        pred_start = time.time()\n",
        "\n",
        "        try:\n",
        "            X_array = X.values if isinstance(X, pd.DataFrame) else X\n",
        "\n",
        "            # Clean input data\n",
        "            if np.any(np.isnan(X_array)) or np.any(np.isinf(X_array)):\n",
        "                X_array = np.nan_to_num(X_array, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "            predictions = self.pipeline.predict(X_array)\n",
        "\n",
        "            # Validate predictions\n",
        "            if np.any(np.isnan(predictions)) or np.any(np.isinf(predictions)):\n",
        "                logger.warning(f\"{self.name}: Invalid predictions detected, cleaning...\")\n",
        "                predictions = np.nan_to_num(predictions, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "            self.prediction_time = time.time() - pred_start\n",
        "\n",
        "            if self.debug_mode:\n",
        "                logger.debug(f\"{self.name}: Prediction completed in {self.prediction_time:.3f}s\")\n",
        "\n",
        "            return predictions.flatten()\n",
        "\n",
        "        except Exception as e:\n",
        "            self.prediction_time = time.time() - pred_start\n",
        "            logger.error(f\"{self.name} prediction failed: {str(e)}\")\n",
        "            return np.zeros(len(X))\n",
        "\n",
        "    def get_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get comprehensive model information\"\"\"\n",
        "        return {\n",
        "            'name': self.name,\n",
        "            'is_fitted': self.is_fitted,\n",
        "            'training_time': self.training_time,\n",
        "            'prediction_time': self.prediction_time,\n",
        "            'fit_error': self.fit_error\n",
        "        }\n",
        "\n",
        "logger.info(\"Enhanced ML Forecaster class loaded\")"
      ],
      "metadata": {
        "id": "ZFOf-vqCrtl5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UlZFd_awaHGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Model Factory - Total: 39 models (with all libraries) or 33 models (without optional libraries)\n",
        "class DebugMLModelFactory:\n",
        "    \"\"\"Model factory for creating all ML model variants\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    @monitor_performance(\"Model Creation\")\n",
        "    def create_all_models(config: DebugMLForecastingConfig) -> List[DebugMLForecaster]:\n",
        "        \"\"\"Create comprehensive set of ML models\"\"\"\n",
        "\n",
        "        logger.info(\"🏭 Creating ML models...\")\n",
        "        all_models = []\n",
        "\n",
        "        # Random Forest Family (6 models)\n",
        "        if config.use_tree_models:\n",
        "            logger.info(\"  Creating Random Forest family...\")\n",
        "            all_models.extend([\n",
        "                DebugMLForecaster(\"V1_RandomForest\", RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42), config.debug_mode),\n",
        "                DebugMLForecaster(\"V2_RandomForest\", RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42), config.debug_mode),\n",
        "                DebugMLForecaster(\"RandomForest_Optimized\", RandomForestRegressor(n_estimators=200, min_samples_split=5, random_state=42), config.debug_mode),\n",
        "                DebugMLForecaster(\"RandomForest_VP\", RandomForestRegressor(n_estimators=100, random_state=42), config.debug_mode),  # For GridSearch\n",
        "                DebugMLForecaster(\"V1_ExtraTrees\", ExtraTreesRegressor(n_estimators=50, max_depth=10, random_state=42), config.debug_mode),\n",
        "                DebugMLForecaster(\"V2_ExtraTrees\", ExtraTreesRegressor(n_estimators=100, max_depth=None, random_state=42), config.debug_mode),\n",
        "            ])\n",
        "\n",
        "        # Gradient Boosting Family (11 models)\n",
        "        if config.use_gradient_boosting:\n",
        "            logger.info(\"  Creating Gradient Boosting family...\")\n",
        "            all_models.extend([\n",
        "                DebugMLForecaster(\"V1_GradientBoosting\", GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, random_state=42), config.debug_mode),\n",
        "                DebugMLForecaster(\"V2_GradientBoosting\", GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42), config.debug_mode),\n",
        "                DebugMLForecaster(\"GradientBoosting_Optimized\", GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, random_state=42), config.debug_mode),\n",
        "                DebugMLForecaster(\"GradientBoosting_VP\", GradientBoostingRegressor(n_estimators=100, random_state=42), config.debug_mode),\n",
        "                DebugMLForecaster(\"HistGradientBoosting\", HistGradientBoostingRegressor(max_iter=100, random_state=42), config.debug_mode),\n",
        "            ])\n",
        "\n",
        "            # XGBoost if available\n",
        "            if XGB_AVAILABLE:\n",
        "                all_models.extend([\n",
        "                    DebugMLForecaster(\"XGBoost_V1\", xgb.XGBRegressor(n_estimators=50, random_state=42), config.debug_mode),\n",
        "                    DebugMLForecaster(\"XGBoost_V2\", xgb.XGBRegressor(n_estimators=100, random_state=42), config.debug_mode),\n",
        "                ])\n",
        "\n",
        "            # LightGBM if available\n",
        "            if LGB_AVAILABLE:\n",
        "                all_models.extend([\n",
        "                    DebugMLForecaster(\"LightGBM_V1\", lgb.LGBMRegressor(n_estimators=50, random_state=42, verbose=-1), config.debug_mode),\n",
        "                    DebugMLForecaster(\"LightGBM_VP\", lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1), config.debug_mode),\n",
        "                ])\n",
        "\n",
        "            # CatBoost if available\n",
        "            if CATBOOST_AVAILABLE:\n",
        "                all_models.extend([\n",
        "                    DebugMLForecaster(\"CatBoost_V1\", cb.CatBoostRegressor(iterations=50, random_state=42, verbose=False), config.debug_mode),\n",
        "                    DebugMLForecaster(\"CatBoost_VP\", cb.CatBoostRegressor(iterations=100, random_state=42, verbose=False), config.debug_mode),\n",
        "                ])\n",
        "\n",
        "        # Linear & Regularized Models (10 models)\n",
        "        # NOTE: Ridge, BayesianRidge, HuberRegressor do NOT have random_state\n",
        "        # Only Lasso and ElasticNet have random_state (for selection='random')\n",
        "        if config.use_linear_models:\n",
        "            logger.info(\"  Creating Linear family...\")\n",
        "            all_models.extend([\n",
        "                DebugMLForecaster(\"Ridge_FIXED\", Ridge(alpha=1.0, solver='auto'), config.debug_mode),  # No random_state\n",
        "                DebugMLForecaster(\"V1_Ridge\", Ridge(alpha=0.1), config.debug_mode),  # No random_state\n",
        "                DebugMLForecaster(\"V2_Ridge\", Ridge(alpha=1.0), config.debug_mode),  # No random_state\n",
        "                DebugMLForecaster(\"Ridge_Optimized\", Ridge(alpha=10.0), config.debug_mode),  # No random_state\n",
        "                DebugMLForecaster(\"Lasso_V1\", Lasso(alpha=0.1, max_iter=2000, random_state=42), config.debug_mode),  # Has random_state\n",
        "                DebugMLForecaster(\"Lasso_V2\", Lasso(alpha=1.0, max_iter=2000, random_state=42), config.debug_mode),  # Has random_state\n",
        "                DebugMLForecaster(\"ElasticNet_V1\", ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=2000, random_state=42), config.debug_mode),\n",
        "                DebugMLForecaster(\"ElasticNet_V2\", ElasticNet(alpha=1.0, l1_ratio=0.7, max_iter=2000, random_state=42), config.debug_mode),\n",
        "                DebugMLForecaster(\"BayesianRidge\", BayesianRidge(), config.debug_mode),  # No random_state!\n",
        "                DebugMLForecaster(\"V1_RobustRegression\", HuberRegressor(epsilon=1.35, max_iter=200), config.debug_mode),  # No random_state!\n",
        "            ])\n",
        "\n",
        "        # Neural Network Models (2 models)\n",
        "        if config.use_neural_models:\n",
        "            logger.info(\"  Creating Neural Network family...\")\n",
        "            all_models.extend([\n",
        "                DebugMLForecaster(\"V1_SimpleNN\", MLPRegressor(hidden_layer_sizes=(50,), max_iter=1000, random_state=42), config.debug_mode),\n",
        "                DebugMLForecaster(\"V2_SimpleNN\", MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42), config.debug_mode),\n",
        "            ])\n",
        "\n",
        "        # SVR Models (3 models) - SVR does NOT have random_state\n",
        "        if config.use_other_models:\n",
        "            logger.info(\"  Creating SVR family...\")\n",
        "            all_models.extend([\n",
        "                DebugMLForecaster(\"V1_SVR\", SVR(kernel='rbf', C=1.0), config.debug_mode),  # No random_state\n",
        "                DebugMLForecaster(\"V2_SVR\", SVR(kernel='rbf', C=10.0), config.debug_mode),  # No random_state\n",
        "                DebugMLForecaster(\"SVR_Optimized\", SVR(kernel='rbf', C=5.0, epsilon=0.1), config.debug_mode),  # No random_state\n",
        "            ])\n",
        "\n",
        "            # K-Neighbors Models (4 models) - KNN does NOT have random_state\n",
        "            logger.info(\"  Creating KNN family...\")\n",
        "            all_models.extend([\n",
        "                DebugMLForecaster(\"KNN_3\", KNeighborsRegressor(n_neighbors=3), config.debug_mode),  # No random_state\n",
        "                DebugMLForecaster(\"KNN_5\", KNeighborsRegressor(n_neighbors=5), config.debug_mode),  # No random_state\n",
        "                DebugMLForecaster(\"KNN_10\", KNeighborsRegressor(n_neighbors=10), config.debug_mode),  # No random_state\n",
        "                DebugMLForecaster(\"KernelRidge\", KernelRidge(alpha=1.0), config.debug_mode),  # No random_state\n",
        "            ])\n",
        "\n",
        "        # Ensemble Models (4 models)\n",
        "        if config.use_ensemble_models:\n",
        "            logger.info(\"  Creating Ensemble family...\")\n",
        "            all_models.extend([\n",
        "                DebugMLForecaster(\"BaggingRegressor\", BaggingRegressor(random_state=42), config.debug_mode),  # Has random_state\n",
        "                DebugMLForecaster(\"AdaBoostRegressor\", AdaBoostRegressor(n_estimators=50, random_state=42), config.debug_mode),  # Has random_state\n",
        "            ])\n",
        "\n",
        "            # Voting ensemble\n",
        "            voting_models = [\n",
        "                ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),\n",
        "                ('ridge', Ridge(alpha=1.0))  # Ridge has no random_state\n",
        "            ]\n",
        "            all_models.append(\n",
        "                DebugMLForecaster(\"VotingRegressor\", VotingRegressor(estimators=voting_models), config.debug_mode)\n",
        "            )\n",
        "\n",
        "            # Stacking ensemble\n",
        "            base_models = [\n",
        "                ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),\n",
        "                ('gb', GradientBoostingRegressor(n_estimators=50, random_state=42))\n",
        "            ]\n",
        "            all_models.append(\n",
        "                DebugMLForecaster(\"StackingRegressor_V2\", StackingRegressor(estimators=base_models, final_estimator=Ridge()), config.debug_mode)\n",
        "            )\n",
        "\n",
        "        # Baseline Tree\n",
        "        all_models.append(\n",
        "            DebugMLForecaster(\"DecisionTree_Baseline\", DecisionTreeRegressor(max_depth=5, random_state=42), config.debug_mode)\n",
        "        )\n",
        "\n",
        "        logger.info(f\"✅ Created {len(all_models)} ML models\")\n",
        "\n",
        "        # Count by type for verification\n",
        "        families = {\n",
        "            'RandomForest/ExtraTrees': len([m for m in all_models if 'Forest' in m.name or 'Extra' in m.name]),\n",
        "            'GradientBoosting': len([m for m in all_models if 'Gradient' in m.name or 'XGB' in m.name or 'LightGBM' in m.name or 'CatBoost' in m.name or 'Hist' in m.name]),\n",
        "            'Linear': len([m for m in all_models if any(x in m.name for x in ['Ridge', 'Lasso', 'Elastic', 'Bayesian', 'Robust'])]),\n",
        "            'Neural': len([m for m in all_models if 'NN' in m.name]),\n",
        "            'SVR': len([m for m in all_models if 'SVR' in m.name]),\n",
        "            'KNN': len([m for m in all_models if 'KNN' in m.name or 'Kernel' in m.name]),\n",
        "            'Ensemble': len([m for m in all_models if any(x in m.name for x in ['Bagging', 'AdaBoost', 'Voting', 'Stacking'])]),\n",
        "            'Other': len([m for m in all_models if 'Decision' in m.name])\n",
        "        }\n",
        "\n",
        "        for family, count in families.items():\n",
        "            if count > 0:\n",
        "                logger.info(f\"  {family}: {count} models\")\n",
        "\n",
        "        return all_models\n",
        "\n",
        "# Test creation\n",
        "test_models = DebugMLModelFactory.create_all_models(config)\n",
        "print(f\"\\n🎯 Successfully created {len(test_models)} models\")\n",
        "print(\"Model names:\", [m.name for m in test_models[:5]], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckTTS5x9Yyq6",
        "outputId": "374ea760-d7d1-44c1-8e7d-4a27cdce911d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 Successfully created 41 models\n",
            "Model names: ['V1_RandomForest', 'V2_RandomForest', 'RandomForest_Optimized', 'RandomForest_VP', 'V1_ExtraTrees'] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6.5: Model Validation (NEW CELL - Add after Cell 6)\n",
        "@monitor_performance(\"Model Validation\")\n",
        "def validate_created_models(models: List[DebugMLForecaster]) -> List[DebugMLForecaster]:\n",
        "    \"\"\"Validate models were created properly before training\"\"\"\n",
        "\n",
        "    logger.info(\"🔍 Validating created models...\")\n",
        "    valid_models = []\n",
        "    invalid_models = []\n",
        "\n",
        "    # Create dummy data for testing\n",
        "    X_dummy = np.random.randn(20, 10)\n",
        "    y_dummy = np.random.randn(20)\n",
        "\n",
        "    for model in models:\n",
        "        try:\n",
        "            # Check model has required attributes\n",
        "            assert hasattr(model, 'model'), f\"Missing model attribute\"\n",
        "            assert hasattr(model, 'name'), f\"Missing name attribute\"\n",
        "            assert model.model is not None, f\"Model is None\"\n",
        "\n",
        "            # Test clone capability (important for GridSearch)\n",
        "            test_model = clone(model.model)\n",
        "\n",
        "            # Test fit/predict cycle with dummy data\n",
        "            test_model.fit(X_dummy, y_dummy)\n",
        "            preds = test_model.predict(X_dummy[:5])\n",
        "\n",
        "            # Validate predictions\n",
        "            assert len(preds) == 5, f\"Prediction length mismatch: expected 5, got {len(preds)}\"\n",
        "            assert not np.all(np.isnan(preds)), f\"Model produces all NaN predictions\"\n",
        "            assert not np.all(preds == preds[0]), f\"Model produces constant predictions\"\n",
        "\n",
        "            valid_models.append(model)\n",
        "            logger.debug(f\"  ✓ {model.name:<30} validated successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            invalid_models.append({'model': model.name, 'error': str(e)})\n",
        "            logger.warning(f\"  ✗ {model.name:<30} validation failed: {str(e)[:50]}...\")\n",
        "\n",
        "    # Summary\n",
        "    logger.info(f\"✅ Validation complete: {len(valid_models)}/{len(models)} models valid\")\n",
        "\n",
        "    if invalid_models:\n",
        "        logger.warning(f\"⚠️ {len(invalid_models)} models failed validation:\")\n",
        "        for item in invalid_models[:5]:  # Show first 5 failures\n",
        "            logger.warning(f\"    - {item['model']}: {item['error'][:60]}\")\n",
        "\n",
        "    # Warn if too many failures\n",
        "    failure_rate = len(invalid_models) / len(models) * 100 if models else 0\n",
        "    if failure_rate > 20:\n",
        "        logger.error(f\"❌ High failure rate: {failure_rate:.1f}% - Check model definitions!\")\n",
        "\n",
        "    return valid_models\n",
        "\n",
        "# Run validation on the created models\n",
        "logger.info(\"=\"*60)\n",
        "logger.info(\"RUNNING MODEL VALIDATION\")\n",
        "logger.info(\"=\"*60)\n",
        "\n",
        "validated_models = validate_created_models(test_models)\n",
        "\n",
        "# Verify we have enough models to proceed\n",
        "min_required_models = 20\n",
        "if len(validated_models) < min_required_models:\n",
        "    logger.error(f\"❌ Insufficient valid models: {len(validated_models)} < {min_required_models}\")\n",
        "    raise ValueError(f\"Need at least {min_required_models} valid models to proceed\")\n",
        "else:\n",
        "    logger.info(f\"✅ Proceeding with {len(validated_models)} validated models\")\n",
        "\n",
        "# Update the models list for subsequent cells\n",
        "all_models = validated_models\n",
        "print(f\"\\n🎯 Validation complete: {len(all_models)} models ready for training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbOOFiQDd09l",
        "outputId": "6664fc45-6131-470b-c0e4-8118beab89b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:  ✗ HistGradientBoosting           validation failed: Model produces constant predictions...\n",
            "WARNING:__main__:  ✗ LightGBM_V1                    validation failed: Model produces constant predictions...\n",
            "WARNING:__main__:  ✗ LightGBM_VP                    validation failed: Model produces constant predictions...\n",
            "WARNING:__main__:  ✗ Lasso_V2                       validation failed: Model produces constant predictions...\n",
            "WARNING:__main__:  ✗ ElasticNet_V2                  validation failed: Model produces constant predictions...\n",
            "WARNING:__main__:⚠️ 5 models failed validation:\n",
            "WARNING:__main__:    - HistGradientBoosting: Model produces constant predictions\n",
            "WARNING:__main__:    - LightGBM_V1: Model produces constant predictions\n",
            "WARNING:__main__:    - LightGBM_VP: Model produces constant predictions\n",
            "WARNING:__main__:    - Lasso_V2: Model produces constant predictions\n",
            "WARNING:__main__:    - ElasticNet_V2: Model produces constant predictions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 Validation complete: 36 models ready for training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N1J2nN60lMTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Data Loading and Seasonal Naive Baseline - REAL DATA VERSION\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "@monitor_performance(\"Data Loading\")\n",
        "def load_enhanced_eda_data(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load the enhanced EDA data with calls and market features\"\"\"\n",
        "\n",
        "    logger.info(f\"📂 Loading data from: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        # Load the CSV\n",
        "        data = pd.read_csv(file_path)\n",
        "\n",
        "        # Check for date column (case-insensitive)\n",
        "        date_col = None\n",
        "        for col in data.columns:\n",
        "            if col.lower() == 'date':\n",
        "                date_col = col\n",
        "                break\n",
        "\n",
        "        if date_col is None:\n",
        "            raise ValueError(\"No date column found (tried 'date', 'Date', 'DATE')\")\n",
        "\n",
        "        # Convert date to datetime and set as index\n",
        "        data[date_col] = pd.to_datetime(data[date_col])\n",
        "        data = data.set_index(date_col)\n",
        "\n",
        "        # Rename index to 'date' for consistency\n",
        "        data.index.name = 'date'\n",
        "\n",
        "        # Sort by date to ensure chronological order\n",
        "        data = data.sort_index()\n",
        "\n",
        "        logger.info(f\"✅ Data loaded successfully!\")\n",
        "        logger.info(f\"  Shape: {data.shape}\")\n",
        "        logger.info(f\"  Date range: {data.index.min()} to {data.index.max()}\")\n",
        "        logger.info(f\"  Total days: {len(data)}\")\n",
        "\n",
        "        # Check for required columns (case-insensitive for calls)\n",
        "        calls_col = None\n",
        "        for col in data.columns:\n",
        "            if col.lower() == 'calls':\n",
        "                calls_col = col\n",
        "                break\n",
        "\n",
        "        if calls_col is None:\n",
        "            raise ValueError(\"Missing 'calls' column in data\")\n",
        "\n",
        "        # Standardize column name to lowercase 'calls'\n",
        "        if calls_col != 'calls':\n",
        "            data = data.rename(columns={calls_col: 'calls'})\n",
        "            logger.info(f\"  Renamed '{calls_col}' to 'calls'\")\n",
        "\n",
        "        # Display column information\n",
        "        logger.info(f\"  Call volume range: {data['calls'].min():.0f} to {data['calls'].max():.0f}\")\n",
        "        logger.info(f\"  Mean call volume: {data['calls'].mean():.0f}\")\n",
        "\n",
        "        # List market features\n",
        "        market_columns = [col for col in data.columns if col not in ['calls']]\n",
        "        if market_columns:\n",
        "            logger.info(f\"  Market features available: {len(market_columns)}\")\n",
        "            sample_features = market_columns[:5]\n",
        "            logger.info(f\"  Sample features: {sample_features}\")\n",
        "            if len(market_columns) > 5:\n",
        "                logger.info(f\"  ... and {len(market_columns) - 5} more features\")\n",
        "\n",
        "        # Check for missing values\n",
        "        missing_pct = (data.isnull().sum() / len(data) * 100).round(2)\n",
        "        if missing_pct.any():\n",
        "            logger.warning(f\"  Missing values detected:\")\n",
        "            for col, pct in missing_pct[missing_pct > 0].items():\n",
        "                logger.warning(f\"    {col}: {pct}%\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"❌ File not found: {file_path}\")\n",
        "        logger.error(\"Please ensure 'enhanced_eda_data.csv' is in your working directory\")\n",
        "        logger.error(\"You can upload it using the file browser on the left panel.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ Error loading data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def calculate_day_of_week_average_baseline(data: pd.DataFrame, config: DebugMLForecastingConfig) -> float:\n",
        "    \"\"\"Calculate the Day-of-Week Average baseline\"\"\"\n",
        "\n",
        "    logger.info(\"🎯 Calculating Day-of-Week Average Baseline...\")\n",
        "    logger.info(\"-\" * 50)\n",
        "\n",
        "    target_col = config.target_column  # Should be 'calls'\n",
        "\n",
        "    # Check if target column exists\n",
        "    if target_col not in data.columns:\n",
        "        # Try case-insensitive search\n",
        "        for col in data.columns:\n",
        "            if col.lower() == target_col.lower():\n",
        "                target_col = col\n",
        "                break\n",
        "        else:\n",
        "            raise ValueError(f\"Target column '{target_col}' not found in data\")\n",
        "\n",
        "    # Split data\n",
        "    train_size = int(len(data) * config.test_split_ratio)\n",
        "    test_size = len(data) - train_size\n",
        "\n",
        "    train_data = data[target_col].iloc[:train_size]\n",
        "    test_data = data[target_col].iloc[train_size:]\n",
        "\n",
        "    logger.info(f\"  Train size: {len(train_data)} samples\")\n",
        "    logger.info(f\"  Test size: {len(test_data)} samples\")\n",
        "\n",
        "    # Create day-of-week average predictions\n",
        "    dow_averages = {}\n",
        "    train_data_with_index = data.iloc[:train_size]\n",
        "\n",
        "    # Calculate average for each day of week from training data\n",
        "    for dow in range(7):  # 0=Monday, 6=Sunday\n",
        "        dow_mask = train_data_with_index.index.dayofweek == dow\n",
        "        if dow_mask.sum() > 0:\n",
        "            dow_averages[dow] = train_data_with_index[target_col][dow_mask].mean()\n",
        "        else:\n",
        "            dow_averages[dow] = train_data.mean()  # fallback\n",
        "\n",
        "    # Generate predictions using day-of-week averages\n",
        "    dow_predictions = []\n",
        "    test_dates = data.index[train_size:]\n",
        "    for date in test_dates:\n",
        "        dow = date.dayofweek\n",
        "        dow_predictions.append(dow_averages[dow])\n",
        "\n",
        "    dow_predictions = np.array(dow_predictions)\n",
        "\n",
        "    # Calculate metrics\n",
        "    baseline_mae = mean_absolute_error(test_data.values, dow_predictions)\n",
        "    baseline_rmse = np.sqrt(mean_squared_error(test_data.values, dow_predictions))\n",
        "\n",
        "    # Calculate MAPE only if no zeros in test data\n",
        "    if (test_data.values != 0).all():\n",
        "        baseline_mape = np.mean(np.abs((test_data.values - dow_predictions) / test_data.values)) * 100\n",
        "        logger.info(f\"📊 DAY-OF-WEEK AVERAGE BASELINE:\")\n",
        "        logger.info(f\"   MAE:  {baseline_mae:.4f} ← THIS IS THE SCORE TO BEAT\")\n",
        "        logger.info(f\"   RMSE: {baseline_rmse:.4f}\")\n",
        "        logger.info(f\"   MAPE: {baseline_mape:.2f}%\")\n",
        "    else:\n",
        "        logger.info(f\"📊 DAY-OF-WEEK AVERAGE BASELINE:\")\n",
        "        logger.info(f\"   MAE:  {baseline_mae:.4f} ← THIS IS THE SCORE TO BEAT\")\n",
        "        logger.info(f\"   RMSE: {baseline_rmse:.4f}\")\n",
        "\n",
        "\n",
        "    return baseline_mae\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "# Specify your data file path\n",
        "DATA_FILE_PATH = \"enhanced_eda_data.csv\"  # Your file with calls + market data\n",
        "\n",
        "# Load the real data\n",
        "try:\n",
        "    data = load_enhanced_eda_data(DATA_FILE_PATH)\n",
        "except FileNotFoundError:\n",
        "    logger.error(\"=\" * 60)\n",
        "    logger.error(\"DATA FILE NOT FOUND!\")\n",
        "    logger.error(\"=\" * 60)\n",
        "    logger.error(\"Please ensure 'enhanced_eda_data.csv' is in your working directory\")\n",
        "    logger.error(\"This file should contain:\")\n",
        "    logger.error(\"  - 'Date' or 'date' column (datetime)\")\n",
        "    logger.error(\"  - 'calls' column (target variable)\")\n",
        "    logger.error(\"  - Market indicator columns (features)\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    logger.error(\"=\" * 60)\n",
        "    logger.error(\"AN ERROR OCCURRED DURING DATA LOADING:\")\n",
        "    logger.error(\"=\" * 60)\n",
        "    logger.error(f\"Error details: {str(e)}\")\n",
        "    logger.error(\"\\nPlease check the data file format and contents.\")\n",
        "    raise\n",
        "\n",
        "\n",
        "# Update config to ensure correct column names\n",
        "config.date_column = \"date\"  # We standardized to lowercase\n",
        "config.target_column = \"calls\"  # We standardized to lowercase\n",
        "\n",
        "# Calculate baseline performance\n",
        "BASELINE_MAE = calculate_day_of_week_average_baseline(data, config)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"🎯 BASELINE ESTABLISHED: {:.4f}\".format(BASELINE_MAE))\n",
        "print(\"=\" * 60)\n",
        "print(\"With real call center data, we expect:\")\n",
        "print(\"  • 30-50% of models beating baseline (not 89%)\")\n",
        "print(\"  • 5-20% typical improvements (not 50%)\")\n",
        "print(\"  • Seasonal patterns making baseline hard to beat\")\n",
        "print(\"  • Market features helping some models\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Quick data quality check\n",
        "print(\"\\n📊 Data Quality Summary:\")\n",
        "print(f\"  Total samples: {len(data)}\")\n",
        "print(f\"  Features available: {len(data.columns)}\")\n",
        "print(f\"  Date continuity: {'✓ Daily' if len(data) > 1 and (data.index[1] - data.index[0]).days == 1 else '✗ Gaps or single record'}\")\n",
        "print(f\"  Target statistics:\")\n",
        "print(f\"    Mean: {data['calls'].mean():.0f}\")\n",
        "print(f\"    Std:  {data['calls'].std():.0f}\")\n",
        "print(f\"    Min:  {data['calls'].min():.0f}\")\n",
        "print(f\"    Max:  {data['calls'].max():.0f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmb-ItHBlMrF",
        "outputId": "f06e981e-daeb-4d8d-88d7-3dcbf5617648"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🎯 BASELINE ESTABLISHED: 919.0115\n",
            "============================================================\n",
            "With real call center data, we expect:\n",
            "  • 30-50% of models beating baseline (not 89%)\n",
            "  • 5-20% typical improvements (not 50%)\n",
            "  • Seasonal patterns making baseline hard to beat\n",
            "  • Market features helping some models\n",
            "============================================================\n",
            "\n",
            "📊 Data Quality Summary:\n",
            "  Total samples: 976\n",
            "  Features available: 19\n",
            "  Date continuity: ✓ Daily\n",
            "  Target statistics:\n",
            "    Mean: 8225\n",
            "    Std:  2538\n",
            "    Min:  3462\n",
            "    Max:  24724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qrQ5Guf1ls4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "25GCx6yznlfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Feature Engineering - FIXED FOR YOUR DATA STRUCTURE\n",
        "class DebugFeatureEngineer:\n",
        "    \"\"\"Feature engineering for general-purpose ML models\"\"\"\n",
        "\n",
        "    def __init__(self, config: DebugMLForecastingConfig):\n",
        "        self.config = config\n",
        "        self.feature_names_ = []\n",
        "\n",
        "    @monitor_performance(\"Feature Engineering\")\n",
        "    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create comprehensive feature set\"\"\"\n",
        "\n",
        "        logger.info(f\"🛠️ Starting feature engineering on data shape: {data.shape}\")\n",
        "\n",
        "        target_col = self.config.target_column\n",
        "        if target_col not in data.columns:\n",
        "            raise ValueError(f\"Target column '{target_col}' not found\")\n",
        "\n",
        "        features_df = pd.DataFrame(index=data.index)\n",
        "        target_series = data[target_col].copy()\n",
        "\n",
        "        # Lag features\n",
        "        logger.info(\"  Creating lag features...\")\n",
        "        for lag in range(1, self.config.max_lags + 1):\n",
        "            features_df[f'lag_{lag}'] = target_series.shift(lag)\n",
        "\n",
        "        # Rolling features\n",
        "        logger.info(\"  Creating rolling features...\")\n",
        "        for window in self.config.rolling_windows:\n",
        "            features_df[f'rolling_mean_{window}'] = target_series.rolling(window=window, min_periods=1).mean()\n",
        "            features_df[f'rolling_std_{window}'] = target_series.rolling(window=window, min_periods=1).std()\n",
        "            features_df[f'rolling_min_{window}'] = target_series.rolling(window=window, min_periods=1).min()\n",
        "            features_df[f'rolling_max_{window}'] = target_series.rolling(window=window, min_periods=1).max()\n",
        "\n",
        "        # Calendar features (if datetime index)\n",
        "        if isinstance(data.index, pd.DatetimeIndex):\n",
        "            logger.info(\"  Creating calendar features...\")\n",
        "            features_df['dow'] = data.index.dayofweek\n",
        "            features_df['month'] = data.index.month\n",
        "            features_df['day'] = data.index.day\n",
        "\n",
        "            # Cyclical encoding\n",
        "            features_df['dow_sin'] = np.sin(2 * np.pi * features_df['dow'] / 7)\n",
        "            features_df['dow_cos'] = np.cos(2 * np.pi * features_df['dow'] / 7)\n",
        "            features_df['month_sin'] = np.sin(2 * np.pi * features_df['month'] / 12)\n",
        "            features_df['month_cos'] = np.cos(2 * np.pi * features_df['month'] / 12)\n",
        "\n",
        "        # Add market features and other columns from the original data\n",
        "        logger.info(\"  Adding market features and flags...\")\n",
        "\n",
        "        # Get all columns except the target\n",
        "        cols_to_add = [col for col in data.columns if col != target_col]\n",
        "\n",
        "        numeric_added = 0\n",
        "        boolean_added = 0\n",
        "\n",
        "        for col in cols_to_add:\n",
        "            if data[col].dtype == 'bool':\n",
        "                # Convert boolean to int (0/1)\n",
        "                features_df[col] = data[col].astype(int)\n",
        "                boolean_added += 1\n",
        "                logger.debug(f\"    Added boolean column '{col}' as 0/1\")\n",
        "            elif np.issubdtype(data[col].dtype, np.number):\n",
        "                # Add numeric columns directly\n",
        "                features_df[col] = data[col]\n",
        "                numeric_added += 1\n",
        "            else:\n",
        "                # Skip any text columns\n",
        "                logger.warning(f\"    Skipped non-numeric column '{col}' (dtype: {data[col].dtype})\")\n",
        "\n",
        "        logger.info(f\"    Added {numeric_added} numeric columns\")\n",
        "        logger.info(f\"    Converted {boolean_added} boolean columns to 0/1\")\n",
        "\n",
        "        # Clean features\n",
        "        logger.info(\"  Cleaning features...\")\n",
        "\n",
        "        # Replace inf values with NaN first\n",
        "        features_df = features_df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        # Fill NaN values - forward fill, then backward fill, then 0\n",
        "        features_df = features_df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "\n",
        "        # Ensure all data is float32 for consistency\n",
        "        features_df = features_df.astype(np.float32)\n",
        "\n",
        "        self.feature_names_ = list(features_df.columns)\n",
        "\n",
        "        # Count feature types\n",
        "        lag_features = len([f for f in self.feature_names_ if 'lag_' in f])\n",
        "        rolling_features = len([f for f in self.feature_names_ if 'rolling_' in f])\n",
        "        calendar_features = len([f for f in self.feature_names_ if any(x in f for x in ['dow', 'month', 'day', 'sin', 'cos'])])\n",
        "        market_features = len(cols_to_add)\n",
        "\n",
        "        logger.info(f\"✅ Feature engineering complete:\")\n",
        "        logger.info(f\"  Total features created: {len(self.feature_names_)}\")\n",
        "        logger.info(f\"  - Lag features: {lag_features}\")\n",
        "        logger.info(f\"  - Rolling features: {rolling_features}\")\n",
        "        logger.info(f\"  - Calendar features: {calendar_features}\")\n",
        "        logger.info(f\"  - Market/other features: {market_features}\")\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    def create_target_aligned(self, features_df: pd.DataFrame,\n",
        "                            original_data: pd.DataFrame,\n",
        "                            forecast_horizon: int = 1) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "        \"\"\"Create target variable aligned with features\"\"\"\n",
        "\n",
        "        target_col = self.config.target_column\n",
        "\n",
        "        # Create target with forecast horizon\n",
        "        target_series = original_data[target_col].shift(-forecast_horizon)\n",
        "\n",
        "        # Align and clean\n",
        "        common_index = features_df.index.intersection(target_series.index)\n",
        "        aligned_features = features_df.loc[common_index]\n",
        "        aligned_target = target_series.loc[common_index]\n",
        "\n",
        "        # Remove rows where target is NaN\n",
        "        valid_mask = ~aligned_target.isna()\n",
        "        final_features = aligned_features[valid_mask]\n",
        "        final_target = aligned_target[valid_mask]\n",
        "\n",
        "        # Ensure target is numeric float32\n",
        "        final_target = final_target.astype(np.float32)\n",
        "\n",
        "        logger.info(f\"✅ Target alignment: Features {final_features.shape}, Target {final_target.shape}\")\n",
        "\n",
        "        return final_features, final_target\n",
        "\n",
        "# Create features\n",
        "logger.info(\"=\"*60)\n",
        "logger.info(\"FEATURE ENGINEERING\")\n",
        "logger.info(\"=\"*60)\n",
        "\n",
        "feature_engineer = DebugFeatureEngineer(config)\n",
        "features_df = feature_engineer.create_features(data)\n",
        "X, y = feature_engineer.create_target_aligned(features_df, data)\n",
        "\n",
        "# Verify data types\n",
        "logger.info(\"Data verification:\")\n",
        "logger.info(f\"  All features numeric: {X.select_dtypes(include=[np.number]).shape[1] == X.shape[1]}\")\n",
        "logger.info(f\"  X shape: {X.shape}\")\n",
        "logger.info(f\"  y shape: {y.shape}\")\n",
        "logger.info(f\"  X dtype: float32 (all columns)\")\n",
        "logger.info(f\"  y dtype: {y.dtype}\")\n",
        "\n",
        "# Check for any remaining non-numeric data\n",
        "non_numeric_check = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "if non_numeric_check:\n",
        "    logger.error(f\"❌ Non-numeric columns still present: {non_numeric_check}\")\n",
        "    raise ValueError(\"Non-numeric data detected after feature engineering\")\n",
        "\n",
        "# Split data\n",
        "train_size = int(len(X) * config.test_split_ratio)\n",
        "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "\n",
        "print(f\"\\n📊 Data prepared:\")\n",
        "print(f\"  Train: {X_train.shape}\")\n",
        "print(f\"  Test: {X_test.shape}\")\n",
        "print(f\"  Feature count: {X_train.shape[1]}\")\n",
        "print(f\"  All numeric: ✅\")\n",
        "\n",
        "# Show sample of feature names\n",
        "print(f\"\\nSample features created:\")\n",
        "sample_features = feature_engineer.feature_names_[:10] if len(feature_engineer.feature_names_) > 10 else feature_engineer.feature_names_\n",
        "for feat in sample_features:\n",
        "    print(f\"  - {feat}\")\n",
        "if len(feature_engineer.feature_names_) > 10:\n",
        "    print(f\"  ... and {len(feature_engineer.feature_names_) - 10} more features\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7Gmpa0vnlkE",
        "outputId": "51bac803-cfb6-44b8-f5a2-bcac5f82ef61"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Data prepared:\n",
            "  Train: (731, 62)\n",
            "  Test: (244, 62)\n",
            "  Feature count: 62\n",
            "  All numeric: ✅\n",
            "\n",
            "Sample features created:\n",
            "  - lag_1\n",
            "  - lag_2\n",
            "  - lag_3\n",
            "  - lag_4\n",
            "  - lag_5\n",
            "  - lag_6\n",
            "  - lag_7\n",
            "  - lag_8\n",
            "  - lag_9\n",
            "  - lag_10\n",
            "  ... and 52 more features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v2FWkUZOk-4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dbz9GzAZk_NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rcThNYOVk_3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Parameter Grids for VP Optimization\n",
        "def get_verified_param_grids(config: DebugMLForecastingConfig) -> Dict[str, Dict]:\n",
        "    \"\"\"Get parameter grids for GridSearchCV optimization\"\"\"\n",
        "\n",
        "    logger.info(\"🎛️ Creating parameter grids for VP optimization\")\n",
        "\n",
        "    param_grids = {\n",
        "        'RandomForest': {\n",
        "            'model__n_estimators': [100, 200],\n",
        "            'model__max_depth': [10, None],\n",
        "            'model__min_samples_split': [2, 5]\n",
        "        },\n",
        "        'ExtraTrees': {\n",
        "            'model__n_estimators': [100, 200],\n",
        "            'model__max_depth': [10, None]\n",
        "        },\n",
        "        'GradientBoosting': {\n",
        "            'model__n_estimators': [100, 200],\n",
        "            'model__learning_rate': [0.05, 0.1],\n",
        "            'model__max_depth': [3, 5]\n",
        "        },\n",
        "        'Ridge': {\n",
        "            'model__alpha': [0.1, 1.0, 10.0]\n",
        "        },\n",
        "        'Lasso': {\n",
        "            'model__alpha': [0.1, 1.0, 10.0]\n",
        "        },\n",
        "        'ElasticNet': {\n",
        "            'model__alpha': [0.1, 1.0],\n",
        "            'model__l1_ratio': [0.3, 0.5, 0.7]\n",
        "        },\n",
        "        'SVR': {\n",
        "            'model__C': [0.1, 1.0, 10.0],\n",
        "            'model__epsilon': [0.01, 0.1]\n",
        "        },\n",
        "        'SimpleNN': {\n",
        "            'model__hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
        "            'model__alpha': [0.001, 0.01]\n",
        "        },\n",
        "        'KNN': {\n",
        "            'model__n_neighbors': [3, 5, 10, 15],\n",
        "            'model__weights': ['uniform', 'distance']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if XGB_AVAILABLE:\n",
        "        param_grids['XGBoost'] = {\n",
        "            'model__n_estimators': [100, 200],\n",
        "            'model__max_depth': [3, 5],\n",
        "            'model__learning_rate': [0.05, 0.1]\n",
        "        }\n",
        "\n",
        "    if LGB_AVAILABLE:\n",
        "        param_grids['LightGBM'] = {\n",
        "            'model__n_estimators': [100, 200],\n",
        "            'model__max_depth': [5, -1],\n",
        "            'model__learning_rate': [0.05, 0.1]\n",
        "        }\n",
        "\n",
        "    if CATBOOST_AVAILABLE:\n",
        "        param_grids['CatBoost'] = {\n",
        "            'model__iterations': [100, 200],\n",
        "            'model__depth': [4, 6],\n",
        "            'model__learning_rate': [0.05, 0.1]\n",
        "        }\n",
        "\n",
        "    # Log total combinations\n",
        "    for model_type, grid in param_grids.items():\n",
        "        combinations = 1\n",
        "        for param_values in grid.values():\n",
        "            combinations *= len(param_values)\n",
        "        logger.info(f\"  {model_type}: {combinations} combinations\")\n",
        "\n",
        "    return param_grids\n",
        "\n",
        "param_grids = get_verified_param_grids(config)\n",
        "print(f\"Parameter grids created for {len(param_grids)} model types\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5jxIE2XsXC4",
        "outputId": "351ee511-839d-4563-ef90-2adad47187f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter grids created for 12 model types\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: Training Functions for Each Phase - FIXED VERSION\n",
        "def train_v1_models(models: List[DebugMLForecaster],\n",
        "                    X_train: pd.DataFrame, y_train: pd.Series,\n",
        "                    X_test: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Train V1 baseline models\"\"\"\n",
        "\n",
        "    v1_predictions = {}\n",
        "    v1_models = [m for m in models if 'V1' in m.name or 'Baseline' in m.name]\n",
        "\n",
        "    logger.info(f\"🚀 Training {len(v1_models)} V1 models...\")\n",
        "\n",
        "    for i, model in enumerate(v1_models, 1):\n",
        "        try:\n",
        "            logger.info(f\"  [{i}/{len(v1_models)}] Training {model.name}...\")\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            if model.is_fitted:\n",
        "                predictions = model.predict(X_test)\n",
        "                v1_predictions[model.name] = predictions\n",
        "                logger.info(f\"    ✅ {model.name} SUCCESS\")\n",
        "            else:\n",
        "                logger.warning(f\"    ❌ {model.name} FAILED\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"    ❌ {model.name} ERROR: {str(e)[:50]}...\")\n",
        "\n",
        "    logger.info(f\"✅ V1 Phase Complete: {len(v1_predictions)}/{len(v1_models)} successful\")\n",
        "    return v1_predictions\n",
        "\n",
        "def train_v2_models(models: List[DebugMLForecaster],\n",
        "                    X_train: pd.DataFrame, y_train: pd.Series,\n",
        "                    X_test: pd.DataFrame,\n",
        "                    v1_predictions: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Train V2 enhanced models with CORRECTLY COMPUTED ensemble features\"\"\"\n",
        "\n",
        "    v2_predictions = {}\n",
        "    v2_models = [m for m in models if 'V2' in m.name or 'Optimized' in m.name]\n",
        "\n",
        "    logger.info(f\"🚀 Training {len(v2_models)} V2 models...\")\n",
        "\n",
        "    # Add ensemble features from V1\n",
        "    X_train_v2 = X_train.copy()\n",
        "    X_test_v2 = X_test.copy()\n",
        "\n",
        "    if len(v1_predictions) > 0:\n",
        "        logger.info(\"  Creating V1 ensemble features (FIXED)...\")\n",
        "\n",
        "        # FIX: Get V1 models to make predictions on TRAINING data\n",
        "        v1_train_predictions = []\n",
        "        v1_test_predictions = []\n",
        "\n",
        "        # Get the fitted V1 models and predict on training data\n",
        "        for model_name, test_preds in v1_predictions.items():\n",
        "            # Find the corresponding fitted model\n",
        "            v1_model = next((m for m in models if m.name == model_name and m.is_fitted), None)\n",
        "\n",
        "            if v1_model is not None:\n",
        "                # Get this model's predictions on training data\n",
        "                train_preds = v1_model.predict(X_train)\n",
        "                v1_train_predictions.append(train_preds)\n",
        "                v1_test_predictions.append(test_preds[:len(X_test)])\n",
        "                logger.debug(f\"    Got predictions from {model_name}\")\n",
        "\n",
        "        # Create ensemble features CORRECTLY\n",
        "        if v1_train_predictions:\n",
        "            # Training ensemble features - using V1 predictions on training data\n",
        "            train_ensemble = np.column_stack(v1_train_predictions)\n",
        "            X_train_v2['v1_ensemble_mean'] = np.mean(train_ensemble, axis=1)\n",
        "            X_train_v2['v1_ensemble_std'] = np.std(train_ensemble, axis=1)\n",
        "            X_train_v2['v1_ensemble_min'] = np.min(train_ensemble, axis=1)\n",
        "            X_train_v2['v1_ensemble_max'] = np.max(train_ensemble, axis=1)\n",
        "\n",
        "            # Test ensemble features - using V1 predictions on test data\n",
        "            test_ensemble = np.column_stack(v1_test_predictions)\n",
        "            X_test_v2['v1_ensemble_mean'] = np.mean(test_ensemble, axis=1)\n",
        "            X_test_v2['v1_ensemble_std'] = np.std(test_ensemble, axis=1)\n",
        "            X_test_v2['v1_ensemble_min'] = np.min(test_ensemble, axis=1)\n",
        "            X_test_v2['v1_ensemble_max'] = np.max(test_ensemble, axis=1)\n",
        "\n",
        "            logger.info(f\"    ✅ Added ensemble features from {len(v1_train_predictions)} V1 models\")\n",
        "            logger.info(f\"    Train ensemble shape: {train_ensemble.shape}\")\n",
        "            logger.info(f\"    Test ensemble shape: {test_ensemble.shape}\")\n",
        "        else:\n",
        "            logger.warning(\"    ⚠️ No V1 models available for ensemble features\")\n",
        "\n",
        "    # Now train V2 models with correct features\n",
        "    for i, model in enumerate(v2_models, 1):\n",
        "        try:\n",
        "            logger.info(f\"  [{i}/{len(v2_models)}] Training {model.name}...\")\n",
        "            model.fit(X_train_v2, y_train)\n",
        "\n",
        "            if model.is_fitted:\n",
        "                predictions = model.predict(X_test_v2)\n",
        "                v2_predictions[model.name] = predictions\n",
        "                logger.info(f\"    ✅ {model.name} SUCCESS\")\n",
        "            else:\n",
        "                logger.warning(f\"    ❌ {model.name} FAILED\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"    ❌ {model.name} ERROR: {str(e)[:50]}...\")\n",
        "\n",
        "    logger.info(f\"✅ V2 Phase Complete: {len(v2_predictions)}/{len(v2_models)} successful\")\n",
        "    return v2_predictions\n",
        "\n",
        "# Run V1 Phase\n",
        "all_models = DebugMLModelFactory.create_all_models(config)\n",
        "v1_predictions = train_v1_models(all_models, X_train, y_train, X_test)\n",
        "\n",
        "# Run V2 Phase with FIXED ensemble features\n",
        "v2_predictions = train_v2_models(all_models, X_train, y_train, X_test, v1_predictions)\n",
        "\n",
        "print(f\"\\n📊 Training Summary:\")\n",
        "print(f\"  V1 models trained: {len(v1_predictions)}\")\n",
        "print(f\"  V2 models trained: {len(v2_predictions)}\")\n",
        "\n",
        "# Verify the fix worked\n",
        "if len(v2_predictions) > 0:\n",
        "    print(f\"\\n✅ V2 models now using CORRECT ensemble features from V1 predictions\")\n",
        "    print(f\"   (Previously used y_train.mean() which was data leakage)\")\n",
        "else:\n",
        "    print(f\"\\n⚠️ No V2 models trained successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC04KdT1Sict",
        "outputId": "23e32f7d-2212-46bc-c2e1-8af644172d81"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Training Summary:\n",
            "  V1 models trained: 13\n",
            "  V2 models trained: 14\n",
            "\n",
            "✅ V2 models now using CORRECT ensemble features from V1 predictions\n",
            "   (Previously used y_train.mean() which was data leakage)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_Gt0QGySi1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: VP GridSearchCV Optimization - FULL IMPLEMENTATION\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# ============================================================================\n",
        "# CONTROL FLAGS\n",
        "# ============================================================================\n",
        "\n",
        "# SET THIS TO FALSE FOR FULL OPTIMIZATION\n",
        "RUN_QUICK_TEST = False  # False = FULL GridSearchCV with all parameters\n",
        "\n",
        "# ============================================================================\n",
        "# PARAMETER GRIDS FOR ALL MODEL TYPES\n",
        "# ============================================================================\n",
        "\n",
        "def get_full_param_grids():\n",
        "    \"\"\"Complete parameter grids for all supported model types\"\"\"\n",
        "    return {\n",
        "        'RandomForest': {\n",
        "            'model__n_estimators': [100, 200, 300],\n",
        "            'model__max_depth': [10, 20, None],\n",
        "            'model__min_samples_split': [2, 5, 10],\n",
        "            'model__min_samples_leaf': [1, 2, 4]\n",
        "        },\n",
        "        'ExtraTrees': {\n",
        "            'model__n_estimators': [100, 200, 300],\n",
        "            'model__max_depth': [10, 20, None],\n",
        "            'model__min_samples_split': [2, 5, 10],\n",
        "            'model__min_samples_leaf': [1, 2, 4]\n",
        "        },\n",
        "        'GradientBoosting': {\n",
        "            'model__n_estimators': [100, 200, 300],\n",
        "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'model__max_depth': [3, 5, 7],\n",
        "            'model__subsample': [0.8, 1.0]\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'model__n_estimators': [100, 200, 300],\n",
        "            'model__max_depth': [3, 5, 7],\n",
        "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'model__subsample': [0.8, 1.0],\n",
        "            'model__colsample_bytree': [0.8, 1.0]\n",
        "        },\n",
        "        'LightGBM': {\n",
        "            'model__n_estimators': [100, 200, 300],\n",
        "            'model__num_leaves': [31, 50, 100],\n",
        "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'model__feature_fraction': [0.8, 1.0],\n",
        "            'model__bagging_fraction': [0.8, 1.0]\n",
        "        },\n",
        "        'Ridge': {\n",
        "            'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
        "            'model__solver': ['auto', 'svd', 'cholesky', 'lsqr'],\n",
        "            'model__max_iter': [1000, 2000]\n",
        "        },\n",
        "        'Lasso': {\n",
        "            'model__alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
        "            'model__selection': ['cyclic', 'random'],\n",
        "            'model__max_iter': [1000, 2000, 3000]\n",
        "        },\n",
        "        'ElasticNet': {\n",
        "            'model__alpha': [0.001, 0.01, 0.1, 1.0],\n",
        "            'model__l1_ratio': [0.2, 0.5, 0.8],\n",
        "            'model__max_iter': [1000, 2000]\n",
        "        },\n",
        "        'SVR': {\n",
        "            'model__kernel': ['linear', 'rbf', 'poly'],\n",
        "            'model__C': [0.1, 1.0, 10.0, 100.0],\n",
        "            'model__epsilon': [0.01, 0.1, 0.2],\n",
        "            'model__gamma': ['scale', 'auto']\n",
        "        },\n",
        "        'KNeighbors': {\n",
        "            'model__n_neighbors': [3, 5, 7, 10],\n",
        "            'model__weights': ['uniform', 'distance'],\n",
        "            'model__p': [1, 2]\n",
        "        },\n",
        "        'MLP': {\n",
        "            'model__hidden_layer_sizes': [(50,), (100,), (100, 50), (200, 100)],\n",
        "            'model__activation': ['relu', 'tanh'],\n",
        "            'model__alpha': [0.001, 0.01, 0.1],\n",
        "            'model__learning_rate': ['constant', 'adaptive']\n",
        "        }\n",
        "    }\n",
        "\n",
        "def get_test_param_grids():\n",
        "    \"\"\"Minimal parameter grids for quick testing\"\"\"\n",
        "    return {\n",
        "        'RandomForest': {'model__n_estimators': [50, 100]},\n",
        "        'ExtraTrees': {'model__n_estimators': [50, 100]},\n",
        "        'GradientBoosting': {'model__n_estimators': [50, 100]},\n",
        "        'XGBoost': {'model__n_estimators': [50, 100]},\n",
        "        'LightGBM': {'model__n_estimators': [50, 100]},\n",
        "        'Ridge': {'model__alpha': [1.0, 10.0]},\n",
        "        'Lasso': {'model__alpha': [0.1, 1.0]},\n",
        "        'ElasticNet': {'model__alpha': [0.1, 1.0]},\n",
        "        'SVR': {'model__C': [1.0, 10.0]},\n",
        "        'KNeighbors': {'model__n_neighbors': [5, 10]},\n",
        "        'MLP': {'model__hidden_layer_sizes': [(50,), (100,)]}\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# GRIDSEARCH OPTIMIZER CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class VerifiedGridSearchOptimizer:\n",
        "    \"\"\"GridSearchCV optimizer for VP phase - ALL MODEL TYPES\"\"\"\n",
        "\n",
        "    def __init__(self, config, quick_test=True):\n",
        "        self.config = config\n",
        "        self.quick_test = quick_test\n",
        "        self.optimization_results = []\n",
        "\n",
        "        # Select parameter grids based on mode\n",
        "        if quick_test:\n",
        "            self.param_grids = get_test_param_grids()\n",
        "            self.config.top_models_for_optimization = 3\n",
        "            self.config.cv_splits = 2\n",
        "            print(\"🔋 QUICK TEST MODE - Limited parameters\")\n",
        "        else:\n",
        "            self.param_grids = get_full_param_grids()\n",
        "            # FORCE full mode settings (override any test settings)\n",
        "            self.config.top_models_for_optimization = 10  # Always use 10 in full mode\n",
        "            self.config.cv_splits = 5  # Always use 5-fold CV in full mode\n",
        "            print(\"🔥 FULL OPTIMIZATION MODE - Comprehensive search\")\n",
        "            print(\"   FORCING: 10 models, 5-fold CV\")\n",
        "\n",
        "        # Calculate total parameter combinations\n",
        "        total_combinations = 0\n",
        "        for model_type, grid in self.param_grids.items():\n",
        "            combos = 1\n",
        "            for param_values in grid.values():\n",
        "                combos *= len(param_values)\n",
        "            total_combinations += combos\n",
        "\n",
        "        print(f\"GridSearch Configuration:\")\n",
        "        print(f\"  Mode: {'QUICK TEST' if quick_test else 'FULL OPTIMIZATION'}\")\n",
        "        print(f\"  Top models to optimize: {self.config.top_models_for_optimization}\")\n",
        "        print(f\"  CV splits: {self.config.cv_splits}\")\n",
        "        print(f\"  Available model types: {len(self.param_grids)}\")\n",
        "        print(f\"  Total parameter combinations: {total_combinations}\")\n",
        "        print(f\"  Supported models: {list(self.param_grids.keys())}\")\n",
        "\n",
        "    def optimize_top_models(self, all_predictions, X_train, y_train, X_test, y_test):\n",
        "        \"\"\"Optimize top models with GridSearchCV\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"🚀 STARTING VP GRIDSEARCHCV OPTIMIZATION\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Rank all models by performance\n",
        "        print(\"\\n📊 Ranking all models by MAE...\")\n",
        "        model_performance = {}\n",
        "        for model_name, predictions in all_predictions.items():\n",
        "            try:\n",
        "                mae = mean_absolute_error(y_test.values[:len(predictions)], predictions)\n",
        "                model_performance[model_name] = mae\n",
        "            except:\n",
        "                model_performance[model_name] = float('inf')\n",
        "\n",
        "        # Select top models\n",
        "        top_models = sorted(model_performance.items(), key=lambda x: x[1])[:self.config.top_models_for_optimization]\n",
        "\n",
        "        print(f\"\\n📈 Top {len(top_models)} models selected for optimization:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, (name, mae) in enumerate(top_models, 1):\n",
        "            model_type = self._get_model_type(name)\n",
        "            print(f\"  {i:2d}. {name:25} MAE: {mae:8.3f} Type: {model_type}\")\n",
        "\n",
        "        # Initialize results storage\n",
        "        vp_predictions = {}\n",
        "        optimization_start = time.time()\n",
        "\n",
        "        # Statistics tracking\n",
        "        stats = {\n",
        "            'total': len(top_models),\n",
        "            'optimized': 0,\n",
        "            'skipped': 0,\n",
        "            'failed': 0,\n",
        "            'by_type': {}\n",
        "        }\n",
        "\n",
        "        # Optimize each top model\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"OPTIMIZING MODELS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for idx, (model_name, baseline_mae) in enumerate(top_models, 1):\n",
        "            print(f\"\\n[{idx}/{len(top_models)}] Processing: {model_name}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            model_type = self._get_model_type(model_name)\n",
        "            print(f\"  Model Type: {model_type}\")\n",
        "\n",
        "            # Track model types\n",
        "            if model_type not in stats['by_type']:\n",
        "                stats['by_type'][model_type] = {'count': 0, 'success': 0}\n",
        "            stats['by_type'][model_type]['count'] += 1\n",
        "\n",
        "            # Check if we have parameters for this model type\n",
        "            if model_type not in self.param_grids:\n",
        "                print(f\"  ⚠️ SKIPPED - No parameter grid for {model_type}\")\n",
        "                stats['skipped'] += 1\n",
        "                continue\n",
        "\n",
        "            # Get parameter grid details\n",
        "            param_grid = self.param_grids[model_type]\n",
        "            n_combinations = 1\n",
        "            for param_values in param_grid.values():\n",
        "                n_combinations *= len(param_values)\n",
        "\n",
        "            print(f\"  Parameter combinations: {n_combinations}\")\n",
        "            print(f\"  Total fits with {self.config.cv_splits}-fold CV: {n_combinations * self.config.cv_splits}\")\n",
        "\n",
        "            try:\n",
        "                # Create base model\n",
        "                base_model = self._create_base_model(model_type)\n",
        "                if base_model is None:\n",
        "                    print(f\"  ❌ FAILED - Could not create base model\")\n",
        "                    stats['failed'] += 1\n",
        "                    continue\n",
        "\n",
        "                # Create pipeline\n",
        "                pipeline = Pipeline([\n",
        "                    ('scaler', StandardScaler()),\n",
        "                    ('model', base_model)\n",
        "                ])\n",
        "\n",
        "                # Setup TimeSeriesSplit\n",
        "                tscv = TimeSeriesSplit(n_splits=self.config.cv_splits)\n",
        "\n",
        "                # Setup GridSearchCV\n",
        "                grid = GridSearchCV(\n",
        "                    pipeline,\n",
        "                    param_grid=param_grid,\n",
        "                    cv=tscv,\n",
        "                    scoring='neg_mean_absolute_error',\n",
        "                    n_jobs=-1 if not self.quick_test else 1,  # Use all cores in full mode\n",
        "                    verbose=1,  # Show progress\n",
        "                    return_train_score=True\n",
        "                )\n",
        "\n",
        "                # Fit GridSearchCV\n",
        "                print(f\"  🎯 Running GridSearchCV...\")\n",
        "                fit_start = time.time()\n",
        "                grid.fit(X_train, y_train)\n",
        "                fit_time = time.time() - fit_start\n",
        "\n",
        "                # Make predictions\n",
        "                predictions = grid.predict(X_test)\n",
        "                test_mae = mean_absolute_error(y_test.values[:len(predictions)], predictions)\n",
        "\n",
        "                # Store optimized model\n",
        "                vp_model_name = f\"{model_name}_VP\"\n",
        "                vp_predictions[vp_model_name] = predictions\n",
        "\n",
        "                # Calculate improvement\n",
        "                cv_mae = -grid.best_score_\n",
        "                improvement = ((baseline_mae - test_mae) / baseline_mae) * 100\n",
        "\n",
        "                # Store results\n",
        "                self.optimization_results.append({\n",
        "                    'model': model_name,\n",
        "                    'type': model_type,\n",
        "                    'baseline_mae': baseline_mae,\n",
        "                    'cv_mae': cv_mae,\n",
        "                    'test_mae': test_mae,\n",
        "                    'improvement': improvement,\n",
        "                    'best_params': grid.best_params_,\n",
        "                    'fit_time': fit_time,\n",
        "                    'n_combinations': n_combinations\n",
        "                })\n",
        "\n",
        "                # Update statistics\n",
        "                stats['optimized'] += 1\n",
        "                stats['by_type'][model_type]['success'] += 1\n",
        "\n",
        "                # Print results\n",
        "                print(f\"  ✅ SUCCESS!\")\n",
        "                print(f\"     Optimization time: {fit_time:.1f}s\")\n",
        "                print(f\"     Baseline MAE: {baseline_mae:.3f}\")\n",
        "                print(f\"     CV MAE: {cv_mae:.3f}\")\n",
        "                print(f\"     Test MAE: {test_mae:.3f}\")\n",
        "                print(f\"     Improvement: {improvement:+.1f}%\")\n",
        "                print(f\"     Best params: {grid.best_params_}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ FAILED - {str(e)[:100]}\")\n",
        "                stats['failed'] += 1\n",
        "\n",
        "        # Final summary\n",
        "        total_time = time.time() - optimization_start\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"✅ VP OPTIMIZATION COMPLETE\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total optimization time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
        "        print(f\"\\nStatistics:\")\n",
        "        print(f\"  Models attempted: {stats['total']}\")\n",
        "        print(f\"  Successfully optimized: {stats['optimized']}\")\n",
        "        print(f\"  Skipped (no grid): {stats['skipped']}\")\n",
        "        print(f\"  Failed: {stats['failed']}\")\n",
        "\n",
        "        if stats['by_type']:\n",
        "            print(f\"\\nBy Model Type:\")\n",
        "            for model_type, type_stats in stats['by_type'].items():\n",
        "                success_rate = (type_stats['success'] / type_stats['count'] * 100) if type_stats['count'] > 0 else 0\n",
        "                print(f\"  {model_type:15} {type_stats['success']}/{type_stats['count']} ({success_rate:.0f}%)\")\n",
        "\n",
        "        if self.optimization_results:\n",
        "            print(f\"\\nTop 3 Improvements:\")\n",
        "            sorted_results = sorted(self.optimization_results, key=lambda x: x['improvement'], reverse=True)\n",
        "            for i, result in enumerate(sorted_results[:3], 1):\n",
        "                print(f\"  {i}. {result['model']}: {result['improvement']:+.1f}% improvement\")\n",
        "\n",
        "        return vp_predictions\n",
        "\n",
        "    def _get_model_type(self, name):\n",
        "        \"\"\"Extract model type from name - COMPREHENSIVE DETECTION\"\"\"\n",
        "        name_upper = name.upper()\n",
        "\n",
        "        # Check each model type (order matters for similar names)\n",
        "        if 'XGBOOST' in name_upper or 'XGB' in name_upper:\n",
        "            return 'XGBoost'\n",
        "        elif 'LIGHTGBM' in name_upper or 'LGB' in name_upper or 'LGBM' in name_upper:\n",
        "            return 'LightGBM'\n",
        "        elif 'RANDOMFOREST' in name_upper or 'RANDOM_FOREST' in name_upper or 'RF_' in name_upper:\n",
        "            return 'RandomForest'\n",
        "        elif 'EXTRATREES' in name_upper or 'EXTRA_TREES' in name_upper or 'ET_' in name_upper:\n",
        "            return 'ExtraTrees'\n",
        "        elif 'GRADIENTBOOSTING' in name_upper or 'GBM' in name_upper or 'GRADIENT' in name_upper:\n",
        "            return 'GradientBoosting'\n",
        "        elif 'ELASTICNET' in name_upper or 'ELASTIC_NET' in name_upper:\n",
        "            return 'ElasticNet'\n",
        "        elif 'LASSO' in name_upper:  # Check Lasso before Ridge\n",
        "            return 'Lasso'\n",
        "        elif 'RIDGE' in name_upper:\n",
        "            return 'Ridge'\n",
        "        elif 'SVR' in name_upper or 'SVM' in name_upper or 'SUPPORTVECTOR' in name_upper:\n",
        "            return 'SVR'\n",
        "        elif 'KNEIGHBORS' in name_upper or 'KNN' in name_upper:\n",
        "            return 'KNeighbors'\n",
        "        elif 'MLP' in name_upper or 'NEURAL' in name_upper:\n",
        "            return 'MLP'\n",
        "        else:\n",
        "            return 'Unknown'\n",
        "\n",
        "    def _create_base_model(self, model_type):\n",
        "        \"\"\"Create base model instance for each type - FIXED FOR NESTED PARALLELISM\"\"\"\n",
        "        try:\n",
        "            if model_type == 'RandomForest':\n",
        "                # NO n_jobs to avoid nested parallelism with GridSearchCV\n",
        "                return RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "            elif model_type == 'ExtraTrees':\n",
        "                # NO n_jobs\n",
        "                return ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
        "            elif model_type == 'GradientBoosting':\n",
        "                return GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "            elif model_type == 'XGBoost':\n",
        "                try:\n",
        "                    import xgboost as xgb\n",
        "                    # NO n_jobs\n",
        "                    return xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "                except ImportError:\n",
        "                    print(\"    XGBoost not available\")\n",
        "                    return None\n",
        "            elif model_type == 'LightGBM':\n",
        "                try:\n",
        "                    import lightgbm as lgb\n",
        "                    # NO n_jobs, kept verbose=-1 to suppress output\n",
        "                    return lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)\n",
        "                except ImportError:\n",
        "                    print(\"    LightGBM not available\")\n",
        "                    return None\n",
        "            elif model_type == 'Ridge':\n",
        "                # Ridge doesn't have random_state\n",
        "                return Ridge()\n",
        "            elif model_type == 'Lasso':\n",
        "                return Lasso(random_state=42, max_iter=1000)\n",
        "            elif model_type == 'ElasticNet':\n",
        "                from sklearn.linear_model import ElasticNet\n",
        "                return ElasticNet(random_state=42, max_iter=1000)\n",
        "            elif model_type == 'SVR':\n",
        "                # SVR doesn't have random_state\n",
        "                return SVR()\n",
        "            elif model_type == 'KNeighbors':\n",
        "                from sklearn.neighbors import KNeighborsRegressor\n",
        "                # KNN doesn't have random_state\n",
        "                return KNeighborsRegressor()\n",
        "            elif model_type == 'MLP':\n",
        "                from sklearn.neural_network import MLPRegressor\n",
        "                return MLPRegressor(random_state=42, max_iter=1000, early_stopping=True)\n",
        "            else:\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"    Error creating {model_type}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "if RUN_QUICK_TEST:\n",
        "    print(\"⚡ QUICK TEST MODE - Limited GridSearchCV\")\n",
        "else:\n",
        "    print(\"🔥 FULL OPTIMIZATION MODE - Comprehensive GridSearchCV\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = VerifiedGridSearchOptimizer(config, quick_test=RUN_QUICK_TEST)\n",
        "\n",
        "# Combine all predictions\n",
        "all_predictions = {**v1_predictions, **v2_predictions}\n",
        "print(f\"\\nTotal models available for optimization: {len(all_predictions)}\")\n",
        "\n",
        "# Run optimization\n",
        "vp_predictions = optimizer.optimize_top_models(\n",
        "    all_predictions,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_test,\n",
        "    y_test\n",
        ")\n",
        "\n",
        "# Display final results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎯 FINAL VP RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"VP models created: {len(vp_predictions)}\")\n",
        "\n",
        "if vp_predictions:\n",
        "    print(\"\\nOptimized models:\")\n",
        "    for i, name in enumerate(vp_predictions.keys(), 1):\n",
        "        print(f\"  {i}. {name}\")\n",
        "\n",
        "# Store results for Cell 12\n",
        "vp_optimization_results = optimizer.optimization_results\n",
        "print(f\"\\n✅ Cell 11 complete - {len(vp_predictions)} VP models ready for evaluation\")"
      ],
      "metadata": {
        "id": "1cqwaKxkiNGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s28Z1gSWVKC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: # Calculate Day-of-Week Average baseline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# Calculate Day-of-Week Average\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate Seasonal Naive baseline (7-day lookback)\n",
        "print(\"\\n📊 Calculating Day-of-Week Average Baseline...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Get the original target values\n",
        "train_size = len(y_train)\n",
        "test_size = len(y_test)\n",
        "\n",
        "# Create day-of-week average predictions\n",
        "dow_averages = {}\n",
        "\n",
        "# Calculate average for each day of week from training data\n",
        "for dow in range(7):  # 0=Monday, 6=Sunday\n",
        "    dow_mask = X_train.index.dayofweek == dow\n",
        "    if dow_mask.sum() > 0:\n",
        "        dow_averages[dow] = y_train[dow_mask].mean()\n",
        "    else:\n",
        "        dow_averages[dow] = y_train.mean()  # fallback\n",
        "\n",
        "# Generate predictions using day-of-week averages\n",
        "dow_predictions = []\n",
        "for date in X_test.index:\n",
        "    dow = date.dayofweek\n",
        "    dow_predictions.append(dow_averages[dow])\n",
        "\n",
        "dow_predictions = np.array(dow_predictions)\n",
        "\n",
        "# Calculate baseline metrics\n",
        "dow_average_mae = mean_absolute_error(y_test, dow_predictions)\n",
        "dow_average_rmse = np.sqrt(mean_squared_error(y_test, dow_predictions))\n",
        "\n",
        "print(f\"Day-of-Week Average Baseline:\")\n",
        "print(f\"  MAE:  {dow_average_mae:.3f}\")\n",
        "print(f\"  RMSE: {dow_average_rmse:.3f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# COMPREHENSIVE MODEL EVALUATOR CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class ComprehensiveModelEvaluator:\n",
        "    \"\"\"Evaluate all models against Seasonal Naive baseline\"\"\"\n",
        "\n",
        "    def __init__(self, baseline_mae, baseline_rmse):\n",
        "        self.baseline_mae = baseline_mae\n",
        "        self.baseline_rmse = baseline_rmse\n",
        "        self.all_results = []\n",
        "\n",
        "    def evaluate_model(self, name, predictions, y_true, phase):\n",
        "        \"\"\"Calculate comprehensive metrics for a model\"\"\"\n",
        "        # Align predictions with y_true\n",
        "        min_len = min(len(predictions), len(y_true))\n",
        "        y_true_aligned = y_true.values[:min_len] if hasattr(y_true, 'values') else y_true[:min_len]\n",
        "        predictions_aligned = predictions[:min_len]\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = mean_absolute_error(y_true_aligned, predictions_aligned)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true_aligned, predictions_aligned))\n",
        "\n",
        "        # MAPE (avoiding division by zero)\n",
        "        mask = y_true_aligned != 0\n",
        "        if mask.sum() > 0:\n",
        "            mape = np.mean(np.abs((y_true_aligned[mask] - predictions_aligned[mask]) / y_true_aligned[mask])) * 100\n",
        "        else:\n",
        "            mape = np.inf\n",
        "\n",
        "        # R2 score\n",
        "        try:\n",
        "            r2 = r2_score(y_true_aligned, predictions_aligned)\n",
        "        except:\n",
        "            r2 = -np.inf\n",
        "\n",
        "        # Calculate improvement over baseline\n",
        "        mae_improvement = ((self.baseline_mae - mae) / self.baseline_mae) * 100\n",
        "        rmse_improvement = ((self.baseline_rmse - rmse) / self.baseline_rmse) * 100\n",
        "        beats_baseline = mae < self.baseline_mae\n",
        "\n",
        "        return {\n",
        "            'Model': name,\n",
        "            'Phase': phase,\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'MAPE': mape,\n",
        "            'R2': r2,\n",
        "            'MAE_Improvement': mae_improvement,\n",
        "            'RMSE_Improvement': rmse_improvement,\n",
        "            'Beats_Baseline': beats_baseline\n",
        "        }\n",
        "\n",
        "    def evaluate_all_models(self, v1_preds, v2_preds, vp_preds, y_test):\n",
        "        \"\"\"Evaluate all model predictions\"\"\"\n",
        "        print(f\"\\n📈 Evaluating All Models Against Baseline\")\n",
        "        print(f\"   Baseline MAE: {self.baseline_mae:.3f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Evaluate V1 models\n",
        "        print(f\"\\nEvaluating {len(v1_preds)} V1 models...\")\n",
        "        for model_name, predictions in v1_preds.items():\n",
        "            metrics = self.evaluate_model(model_name, predictions, y_test, 'V1')\n",
        "            self.all_results.append(metrics)\n",
        "\n",
        "        # Evaluate V2 models\n",
        "        print(f\"Evaluating {len(v2_preds)} V2 models...\")\n",
        "        for model_name, predictions in v2_preds.items():\n",
        "            metrics = self.evaluate_model(model_name, predictions, y_test, 'V2')\n",
        "            self.all_results.append(metrics)\n",
        "\n",
        "        # Evaluate VP models\n",
        "        print(f\"Evaluating {len(vp_preds)} VP models...\")\n",
        "        for model_name, predictions in vp_preds.items():\n",
        "            metrics = self.evaluate_model(model_name, predictions, y_test, 'VP')\n",
        "            self.all_results.append(metrics)\n",
        "\n",
        "        # Create results DataFrame\n",
        "        self.results_df = pd.DataFrame(self.all_results)\n",
        "        self.results_df = self.results_df.sort_values('MAE').reset_index(drop=True)\n",
        "\n",
        "        return self.results_df\n",
        "\n",
        "    def print_summary_statistics(self):\n",
        "        \"\"\"Print comprehensive summary statistics\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"📊 SUMMARY STATISTICS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        total_models = len(self.results_df)\n",
        "        beating_baseline = self.results_df['Beats_Baseline'].sum()\n",
        "        beat_rate = (beating_baseline / total_models) * 100\n",
        "\n",
        "        print(f\"Total Models Evaluated: {total_models}\")\n",
        "        print(f\"  V1 Models: {len(self.results_df[self.results_df['Phase'] == 'V1'])}\")\n",
        "        print(f\"  V2 Models: {len(self.results_df[self.results_df['Phase'] == 'V2'])}\")\n",
        "        print(f\"  VP Models: {len(self.results_df[self.results_df['Phase'] == 'VP'])}\")\n",
        "\n",
        "        print(f\"\\nModels Beating Baseline: {beating_baseline}/{total_models} ({beat_rate:.1f}%)\")\n",
        "\n",
        "        # Best model\n",
        "        best_model = self.results_df.iloc[0]\n",
        "        print(f\"\\n🏆 CHAMPION MODEL:\")\n",
        "        print(f\"  Name: {best_model['Model']}\")\n",
        "        print(f\"  Phase: {best_model['Phase']}\")\n",
        "        print(f\"  MAE: {best_model['MAE']:.3f}\")\n",
        "        print(f\"  Improvement over baseline: {best_model['MAE_Improvement']:.1f}%\")\n",
        "\n",
        "        # Top 5 models\n",
        "        print(f\"\\n🥇 Top 5 Models:\")\n",
        "        for i, row in self.results_df.head(5).iterrows():\n",
        "            status = \"✓\" if row['Beats_Baseline'] else \"✗\"\n",
        "            print(f\"  {i+1}. {status} {row['Model']:<30} MAE: {row['MAE']:.3f} ({row['MAE_Improvement']:+.1f}%)\")\n",
        "\n",
        "        # Phase comparison\n",
        "        print(f\"\\n📊 Performance by Phase:\")\n",
        "        for phase in ['V1', 'V2', 'VP']:\n",
        "            phase_df = self.results_df[self.results_df['Phase'] == phase]\n",
        "            if len(phase_df) > 0:\n",
        "                phase_beat = phase_df['Beats_Baseline'].sum()\n",
        "                phase_rate = (phase_beat / len(phase_df)) * 100\n",
        "                best_mae = phase_df['MAE'].min()\n",
        "                avg_mae = phase_df['MAE'].mean()\n",
        "                print(f\"  {phase}: {phase_beat}/{len(phase_df)} beat baseline ({phase_rate:.1f}%) | Best MAE: {best_mae:.3f} | Avg MAE: {avg_mae:.3f}\")\n",
        "\n",
        "        # Model type analysis\n",
        "        print(f\"\\n🔍 Performance by Model Type:\")\n",
        "        model_types = {\n",
        "            'Tree': ['RandomForest', 'ExtraTrees', 'DecisionTree'],\n",
        "            'Boosting': ['GradientBoosting', 'XGBoost', 'LightGBM', 'CatBoost'],\n",
        "            'Linear': ['Ridge', 'Lasso', 'ElasticNet'],\n",
        "            'Neural': ['SimpleNN', 'MLP'],\n",
        "            'Other': ['SVR', 'Robust', 'Stacking']\n",
        "        }\n",
        "\n",
        "        for category, keywords in model_types.items():\n",
        "            category_models = self.results_df[self.results_df['Model'].str.contains('|'.join(keywords), case=False, na=False)]\n",
        "            if len(category_models) > 0:\n",
        "                cat_beat = category_models['Beats_Baseline'].sum()\n",
        "                cat_rate = (cat_beat / len(category_models)) * 100\n",
        "                best_model_name = category_models.iloc[0]['Model']\n",
        "                best_mae = category_models.iloc[0]['MAE']\n",
        "                print(f\"  {category:10}: {cat_beat:2d}/{len(category_models):2d} beat baseline ({cat_rate:5.1f}%) | Best: {best_model_name[:20]} (MAE: {best_mae:.3f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# RUN EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ComprehensiveModelEvaluator(dow_average_mae, dow_average_rmse)\n",
        "\n",
        "# Evaluate all models\n",
        "print(\"\\nEvaluating all model predictions...\")\n",
        "results_df = evaluator.evaluate_all_models(\n",
        "    v1_predictions,\n",
        "    v2_predictions,\n",
        "    vp_predictions,\n",
        "    y_test\n",
        ")\n",
        "\n",
        "# Print summary statistics\n",
        "evaluator.print_summary_statistics()\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n📊 Creating visualizations...\")\n",
        "\n",
        "# Create figure with subplots\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# 1. MAE Distribution by Phase\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "phase_data = []\n",
        "phase_labels = []\n",
        "for phase in ['V1', 'V2', 'VP']:\n",
        "    phase_df = results_df[results_df['Phase'] == phase]\n",
        "    if len(phase_df) > 0:\n",
        "        phase_data.append(phase_df['MAE'].values)\n",
        "        phase_labels.append(f\"{phase}\\n(n={len(phase_df)})\")\n",
        "\n",
        "if phase_data:\n",
        "    bp = ax1.boxplot(phase_data, labels=phase_labels, patch_artist=True)\n",
        "    for patch, color in zip(bp['boxes'], ['skyblue', 'lightgreen', 'lightcoral']):\n",
        "        patch.set_facecolor(color)\n",
        "    ax1.axhline(y=seasonal_naive_mae, color='red', linestyle='--', label='Baseline', linewidth=2)\n",
        "    ax1.set_ylabel('MAE')\n",
        "    ax1.set_title('MAE Distribution by Phase')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Models vs Baseline\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "sorted_mae = results_df['MAE'].values\n",
        "colors = ['green' if x else 'red' for x in results_df['Beats_Baseline']]\n",
        "bars = ax2.bar(range(len(sorted_mae)), sorted_mae, color=colors, alpha=0.6)\n",
        "ax2.axhline(y=seasonal_naive_mae, color='blue', linestyle='--', label='Baseline', linewidth=2)\n",
        "ax2.set_xlabel('Models (sorted by MAE)')\n",
        "ax2.set_ylabel('MAE')\n",
        "ax2.set_title(f'All {len(results_df)} Models vs Baseline')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Top 10 Models\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "top10 = results_df.head(10)\n",
        "y_pos = np.arange(len(top10))\n",
        "colors_top = ['green' if x else 'red' for x in top10['Beats_Baseline']]\n",
        "ax3.barh(y_pos, top10['MAE'], color=colors_top, alpha=0.7)\n",
        "ax3.axvline(x=seasonal_naive_mae, color='blue', linestyle='--', linewidth=2)\n",
        "ax3.set_yticks(y_pos)\n",
        "ax3.set_yticklabels([name[:25] for name in top10['Model']])\n",
        "ax3.set_xlabel('MAE')\n",
        "ax3.set_title('Top 10 Models')\n",
        "ax3.invert_yaxis()\n",
        "\n",
        "# 4. Improvement Distribution\n",
        "ax4 = fig.add_subplot(gs[1, 0])\n",
        "improvements = results_df['MAE_Improvement'].values\n",
        "ax4.hist(improvements, bins=30, color='teal', alpha=0.7, edgecolor='black')\n",
        "ax4.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No improvement')\n",
        "ax4.set_xlabel('MAE Improvement %')\n",
        "ax4.set_ylabel('Count')\n",
        "ax4.set_title('Distribution of Improvements')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Phase Comparison - Win Rates\n",
        "ax5 = fig.add_subplot(gs[1, 1])\n",
        "phase_stats = []\n",
        "phase_names = []\n",
        "for phase in ['V1', 'V2', 'VP']:\n",
        "    phase_df = results_df[results_df['Phase'] == phase]\n",
        "    if len(phase_df) > 0:\n",
        "        win_rate = (phase_df['Beats_Baseline'].sum() / len(phase_df)) * 100\n",
        "        phase_stats.append(win_rate)\n",
        "        phase_names.append(phase)\n",
        "\n",
        "if phase_stats:\n",
        "    bars = ax5.bar(phase_names, phase_stats, color=['skyblue', 'lightgreen', 'lightcoral'], alpha=0.7)\n",
        "    ax5.axhline(y=50, color='black', linestyle='--', alpha=0.5)\n",
        "    ax5.set_ylabel('Win Rate %')\n",
        "    ax5.set_title('Success Rate by Phase')\n",
        "    for bar, rate in zip(bars, phase_stats):\n",
        "        height = bar.get_height()\n",
        "        ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{rate:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "# 6. R² vs MAE Scatter\n",
        "ax6 = fig.add_subplot(gs[1, 2])\n",
        "valid_r2 = results_df[results_df['R2'] > -10]  # Filter out extreme negative R²\n",
        "if len(valid_r2) > 0:\n",
        "    colors_scatter = ['green' if x else 'red' for x in valid_r2['Beats_Baseline']]\n",
        "    scatter = ax6.scatter(valid_r2['MAE'], valid_r2['R2'], c=colors_scatter, alpha=0.6, s=50)\n",
        "    ax6.axvline(x=seasonal_naive_mae, color='blue', linestyle='--', alpha=0.5, label='Baseline MAE')\n",
        "    ax6.set_xlabel('MAE')\n",
        "    ax6.set_ylabel('R²')\n",
        "    ax6.set_title('R² vs MAE')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "# 7. VP Optimization Impact\n",
        "ax7 = fig.add_subplot(gs[2, :])\n",
        "# Compare V2 models with their VP versions\n",
        "vp_comparison = []\n",
        "for vp_model in results_df[results_df['Phase'] == 'VP']['Model'].values:\n",
        "    base_name = vp_model.replace('_VP', '')\n",
        "    v2_model = results_df[(results_df['Phase'] == 'V2') & (results_df['Model'] == base_name)]\n",
        "    vp_model_data = results_df[(results_df['Phase'] == 'VP') & (results_df['Model'] == vp_model)]\n",
        "\n",
        "    if len(v2_model) > 0 and len(vp_model_data) > 0:\n",
        "        v2_mae = v2_model['MAE'].values[0]\n",
        "        vp_mae = vp_model_data['MAE'].values[0]\n",
        "        improvement = ((v2_mae - vp_mae) / v2_mae) * 100\n",
        "        vp_comparison.append({\n",
        "            'Model': base_name,\n",
        "            'V2_MAE': v2_mae,\n",
        "            'VP_MAE': vp_mae,\n",
        "            'Improvement': improvement\n",
        "        })\n",
        "\n",
        "if vp_comparison:\n",
        "    vp_df = pd.DataFrame(vp_comparison)\n",
        "    vp_df = vp_df.sort_values('Improvement', ascending=False)\n",
        "\n",
        "    x = np.arange(len(vp_df))\n",
        "    width = 0.35\n",
        "\n",
        "    bars1 = ax7.bar(x - width/2, vp_df['V2_MAE'], width, label='V2', color='lightgreen', alpha=0.7)\n",
        "    bars2 = ax7.bar(x + width/2, vp_df['VP_MAE'], width, label='VP', color='lightcoral', alpha=0.7)\n",
        "\n",
        "    ax7.axhline(y=seasonal_naive_mae, color='blue', linestyle='--', label='Baseline', linewidth=2)\n",
        "    ax7.set_xlabel('Model')\n",
        "    ax7.set_ylabel('MAE')\n",
        "    ax7.set_title('GridSearchCV Optimization Impact: V2 vs VP')\n",
        "    ax7.set_xticks(x)\n",
        "    ax7.set_xticklabels([name[:15] for name in vp_df['Model']], rotation=45, ha='right')\n",
        "    ax7.legend()\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add improvement percentages\n",
        "    for i, (idx, row) in enumerate(vp_df.iterrows()):\n",
        "        if row['Improvement'] > 0:\n",
        "            ax7.text(i, max(row['V2_MAE'], row['VP_MAE']) + 10,\n",
        "                    f\"+{row['Improvement']:.1f}%\", ha='center', fontsize=8, color='green')\n",
        "        else:\n",
        "            ax7.text(i, max(row['V2_MAE'], row['VP_MAE']) + 10,\n",
        "                    f\"{row['Improvement']:.1f}%\", ha='center', fontsize=8, color='red')\n",
        "\n",
        "plt.suptitle(f'ML Pipeline Evaluation: {len(results_df)} Models vs Seasonal Naive Baseline', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL INSIGHTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎯 KEY INSIGHTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Success rate analysis\n",
        "total_models = len(results_df)\n",
        "beating_baseline = results_df['Beats_Baseline'].sum()\n",
        "success_rate = (beating_baseline / total_models) * 100\n",
        "\n",
        "if success_rate < 20:\n",
        "    print(f\"⚠️ CHALLENGE: Only {beating_baseline}/{total_models} ({success_rate:.1f}%) models beat the baseline\")\n",
        "    print(\"   → The seasonal pattern is very strong and hard to improve upon\")\n",
        "    print(\"   → Consider using the seasonal naive baseline or the best ML model\")\n",
        "elif success_rate < 50:\n",
        "    print(f\"📊 MIXED RESULTS: {beating_baseline}/{total_models} ({success_rate:.1f}%) models beat the baseline\")\n",
        "    print(\"   → Some ML models add value, but many don't\")\n",
        "    print(\"   → Focus on the successful model types for deployment\")\n",
        "else:\n",
        "    print(f\"✅ SUCCESS: {beating_baseline}/{total_models} ({success_rate:.1f}%) models beat the baseline\")\n",
        "    print(\"   → ML is consistently adding value over naive forecasting\")\n",
        "    print(\"   → Multiple viable options for production deployment\")\n",
        "\n",
        "# VP optimization analysis\n",
        "vp_models = results_df[results_df['Phase'] == 'VP']\n",
        "if len(vp_models) > 0:\n",
        "    vp_beating = vp_models['Beats_Baseline'].sum()\n",
        "    vp_rate = (vp_beating / len(vp_models)) * 100\n",
        "    print(f\"\\n📈 GridSearchCV Impact:\")\n",
        "    print(f\"   {vp_beating}/{len(vp_models)} ({vp_rate:.1f}%) VP models beat baseline\")\n",
        "\n",
        "    best_vp = vp_models.iloc[0]\n",
        "    if best_vp['Beats_Baseline']:\n",
        "        print(f\"   Best VP model: {best_vp['Model']} (MAE: {best_vp['MAE']:.3f})\")\n",
        "        print(f\"   VP optimization was worthwhile!\")\n",
        "    else:\n",
        "        print(f\"   GridSearchCV didn't help models beat the baseline\")\n",
        "\n",
        "# Best overall strategy\n",
        "best_overall = results_df.iloc[0]\n",
        "if best_overall['Beats_Baseline']:\n",
        "    print(f\"\\n🏆 RECOMMENDATION:\")\n",
        "    print(f\"   Deploy {best_overall['Model']} (Phase: {best_overall['Phase']})\")\n",
        "    print(f\"   MAE: {best_overall['MAE']:.3f} ({best_overall['MAE_Improvement']:.1f}% better than baseline)\")\n",
        "else:\n",
        "    print(f\"\\n⚠️ RECOMMENDATION:\")\n",
        "    print(f\"   Consider using the Seasonal Naive baseline (MAE: {seasonal_naive_mae:.3f})\")\n",
        "    print(f\"   It outperforms all {total_models} ML models tested\")\n",
        "\n",
        "# Export results\n",
        "results_filename = 'model_evaluation_results.csv'\n",
        "results_df.to_csv(results_filename, index=False)\n",
        "print(f\"\\n💾 Results saved to: {results_filename}\")\n",
        "\n",
        "print(\"\\n✅ Cell 12 complete - Comprehensive evaluation finished!\")"
      ],
      "metadata": {
        "id": "GRvIMxI87VZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Executive Summary & Final Report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTIVE SUMMARY GENERATOR\n",
        "# ============================================================================\n",
        "\n",
        "class ExecutiveSummaryGenerator:\n",
        "    \"\"\"Generate professional executive summary and export results\"\"\"\n",
        "\n",
        "    def __init__(self, results_df, baseline_mae, baseline_rmse):\n",
        "        self.results_df = results_df\n",
        "        self.baseline_mae = baseline_mae\n",
        "        self.baseline_rmse = baseline_rmse\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    def create_executive_dashboard(self):\n",
        "        \"\"\"Create comprehensive executive dashboard\"\"\"\n",
        "\n",
        "        # Prepare enhanced data\n",
        "        df = self.results_df.copy()\n",
        "\n",
        "        # Create figure for executive dashboard\n",
        "        fig = plt.figure(figsize=(20, 12))\n",
        "        fig.suptitle('ML FORECASTING EXECUTIVE DASHBOARD\\nCall Center Volume Prediction Analysis',\n",
        "                    fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "        # Create grid\n",
        "        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
        "\n",
        "        # ============================================================================\n",
        "        # 1. KEY METRICS BOX (top left)\n",
        "        # ============================================================================\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        ax1.axis('off')\n",
        "\n",
        "        total_models = len(df)\n",
        "        winners = df['Beats_Baseline'].sum()\n",
        "        win_rate = (winners / total_models) * 100\n",
        "        best_model = df.iloc[0]\n",
        "\n",
        "        # Calculate key statistics\n",
        "        avg_improvement_winners = df[df['Beats_Baseline']]['MAE_Improvement'].mean()\n",
        "\n",
        "        metrics_text = f\"\"\"KEY PERFORMANCE METRICS\n",
        "━━━━━━━━━━━━━━━━━━━━━━\n",
        "Baseline MAE: {self.baseline_mae:.1f}\n",
        "Total Models: {total_models}\n",
        "\n",
        "SUCCESS METRICS\n",
        "Models Beating Baseline: {winners}/{total_models}\n",
        "Success Rate: {win_rate:.1f}%\n",
        "Avg Improvement: {avg_improvement_winners:.1f}%\n",
        "\n",
        "CHAMPION MODEL\n",
        "{best_model['Model']}\n",
        "MAE: {best_model['MAE']:.1f}\n",
        "Improvement: {best_model['MAE_Improvement']:.1f}%\"\"\"\n",
        "\n",
        "        ax1.text(0.05, 0.95, metrics_text, transform=ax1.transAxes,\n",
        "                fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
        "                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.3))\n",
        "\n",
        "        # ============================================================================\n",
        "        # 2. IMPROVEMENT WATERFALL (top center-right)\n",
        "        # ============================================================================\n",
        "        ax2 = fig.add_subplot(gs[0, 1:3])\n",
        "\n",
        "        # Show progression from baseline to best model\n",
        "        stages = ['Seasonal\\nBaseline', 'Best V1', 'Best V2', 'Best VP', 'Champion\\n(Lasso_V2)']\n",
        "\n",
        "        v1_best = df[df['Phase'] == 'V1']['MAE'].min() if len(df[df['Phase'] == 'V1']) > 0 else self.baseline_mae\n",
        "        v2_best = df[df['Phase'] == 'V2']['MAE'].min() if len(df[df['Phase'] == 'V2']) > 0 else v1_best\n",
        "        vp_best = df[df['Phase'] == 'VP']['MAE'].min() if len(df[df['Phase'] == 'VP']) > 0 else v2_best\n",
        "        champion = df['MAE'].min()\n",
        "\n",
        "        values = [self.baseline_mae, v1_best, v2_best, vp_best, champion]\n",
        "        improvements = [0]\n",
        "        for i in range(1, len(values)):\n",
        "            improvements.append(((values[i-1] - values[i]) / values[i-1]) * 100)\n",
        "\n",
        "        x_pos = np.arange(len(stages))\n",
        "        colors_waterfall = ['red', 'skyblue', 'lightgreen', 'coral', 'gold']\n",
        "\n",
        "        bars = ax2.bar(x_pos, values, color=colors_waterfall, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "\n",
        "        # Add improvement percentages\n",
        "        for i, (bar, val, imp) in enumerate(zip(bars, values, improvements)):\n",
        "            ax2.text(bar.get_x() + bar.get_width()/2., val + 20,\n",
        "                    f'{val:.0f}', ha='center', va='bottom', fontweight='bold')\n",
        "            if i > 0 and imp != 0:\n",
        "                ax2.text(bar.get_x() + bar.get_width()/2., val - 50,\n",
        "                        f'(-{imp:.1f}%)', ha='center', va='top', fontsize=8, color='darkgreen')\n",
        "\n",
        "        ax2.set_xticks(x_pos)\n",
        "        ax2.set_xticklabels(stages)\n",
        "        ax2.set_ylabel('MAE', fontweight='bold')\n",
        "        ax2.set_title('Performance Progression Across Phases', fontweight='bold')\n",
        "        ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # ============================================================================\n",
        "        # 3. SUCCESS RATE GAUGE (top right)\n",
        "        # ============================================================================\n",
        "        ax3 = fig.add_subplot(gs[0, 3])\n",
        "\n",
        "        # Create a semi-circular gauge\n",
        "        theta = np.linspace(np.pi, 0, 100)\n",
        "        r = np.linspace(0.8, 1, 2)\n",
        "\n",
        "        # Color based on success rate\n",
        "        if win_rate >= 80:\n",
        "            gauge_color = 'green'\n",
        "            status = 'EXCELLENT'\n",
        "        elif win_rate >= 60:\n",
        "            gauge_color = 'yellow'\n",
        "            status = 'GOOD'\n",
        "        elif win_rate >= 40:\n",
        "            gauge_color = 'orange'\n",
        "            status = 'FAIR'\n",
        "        else:\n",
        "            gauge_color = 'red'\n",
        "            status = 'POOR'\n",
        "\n",
        "        # Draw gauge\n",
        "        ax3.plot([0, np.cos(np.pi * (1 - win_rate/100))],\n",
        "                [0, np.sin(np.pi * (1 - win_rate/100))],\n",
        "                'k-', linewidth=3)\n",
        "\n",
        "        wedge = plt.Circle((0, 0), 0.7, color=gauge_color, alpha=0.3)\n",
        "        ax3.add_artist(wedge)\n",
        "\n",
        "        ax3.text(0, -0.3, f'{win_rate:.0f}%', ha='center', fontsize=24, fontweight='bold')\n",
        "        ax3.text(0, -0.5, status, ha='center', fontsize=12, fontweight='bold', color=gauge_color)\n",
        "        ax3.text(0, -0.7, 'Success Rate', ha='center', fontsize=10)\n",
        "\n",
        "        ax3.set_xlim(-1, 1)\n",
        "        ax3.set_ylim(-0.8, 1)\n",
        "        ax3.axis('off')\n",
        "        ax3.set_title('Model Success Rate', fontweight='bold', pad=20)\n",
        "\n",
        "        # ============================================================================\n",
        "        # 4. TOP 10 MODELS TABLE (middle left)\n",
        "        # ============================================================================\n",
        "        ax4 = fig.add_subplot(gs[1, :2])\n",
        "        ax4.axis('tight')\n",
        "        ax4.axis('off')\n",
        "\n",
        "        top10 = df.head(10)[['Model', 'Phase', 'MAE', 'MAE_Improvement']]\n",
        "\n",
        "        # Create table data\n",
        "        table_data = []\n",
        "        table_data.append(['Rank', 'Model', 'Phase', 'MAE', 'Improvement'])\n",
        "\n",
        "        for i, (_, row) in enumerate(top10.iterrows(), 1):\n",
        "            model_short = row['Model'][:25] + '...' if len(row['Model']) > 25 else row['Model']\n",
        "            table_data.append([\n",
        "                str(i),\n",
        "                model_short,\n",
        "                row['Phase'],\n",
        "                f\"{row['MAE']:.1f}\",\n",
        "                f\"{row['MAE_Improvement']:.1f}%\"\n",
        "            ])\n",
        "\n",
        "        table = ax4.table(cellText=table_data, cellLoc='left', loc='center',\n",
        "                         colWidths=[0.08, 0.45, 0.12, 0.15, 0.20])\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(8)\n",
        "        table.scale(1.2, 1.8)\n",
        "\n",
        "        # Style header row\n",
        "        for i in range(5):\n",
        "            table[(0, i)].set_facecolor('#4CAF50')\n",
        "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "        # Color code improvements\n",
        "        for i in range(1, 11):\n",
        "            val = float(table_data[i][4].rstrip('%'))\n",
        "            if val > 40:\n",
        "                table[(i, 4)].set_facecolor('lightgreen')\n",
        "            elif val > 20:\n",
        "                table[(i, 4)].set_facecolor('lightyellow')\n",
        "\n",
        "        ax4.set_title('Top 10 Models Leaderboard', fontweight='bold', pad=20, fontsize=12)\n",
        "\n",
        "        # ============================================================================\n",
        "        # 5. PHASE PERFORMANCE COMPARISON (middle right)\n",
        "        # ============================================================================\n",
        "        ax5 = fig.add_subplot(gs[1, 2:])\n",
        "\n",
        "        phase_stats = {}\n",
        "        for phase in ['V1', 'V2', 'VP']:\n",
        "            phase_df = df[df['Phase'] == phase]\n",
        "            if len(phase_df) > 0:\n",
        "                phase_stats[phase] = {\n",
        "                    'count': len(phase_df),\n",
        "                    'winners': phase_df['Beats_Baseline'].sum(),\n",
        "                    'avg_mae': phase_df['MAE'].mean(),\n",
        "                    'best_mae': phase_df['MAE'].min(),\n",
        "                    'win_rate': (phase_df['Beats_Baseline'].sum() / len(phase_df)) * 100\n",
        "                }\n",
        "\n",
        "        if phase_stats:\n",
        "            phases = list(phase_stats.keys())\n",
        "            x_pos = np.arange(len(phases))\n",
        "            width = 0.35\n",
        "\n",
        "            # Win rate bars\n",
        "            win_rates = [phase_stats[p]['win_rate'] for p in phases]\n",
        "            counts = [phase_stats[p]['count'] for p in phases]\n",
        "\n",
        "            bars1 = ax5.bar(x_pos - width/2, win_rates, width, label='Win Rate %',\n",
        "                          color=['skyblue', 'lightgreen', 'lightcoral'], alpha=0.7)\n",
        "            bars2 = ax5.bar(x_pos + width/2, counts, width, label='Model Count',\n",
        "                          color=['navy', 'darkgreen', 'darkred'], alpha=0.7)\n",
        "\n",
        "            ax5.set_xlabel('Phase', fontweight='bold')\n",
        "            ax5.set_ylabel('Percentage / Count', fontweight='bold')\n",
        "            ax5.set_title('Phase Performance Analysis', fontweight='bold')\n",
        "            ax5.set_xticks(x_pos)\n",
        "            ax5.set_xticklabels(phases)\n",
        "            ax5.legend()\n",
        "            ax5.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "            # Add value labels\n",
        "            for bar, rate in zip(bars1, win_rates):\n",
        "                height = bar.get_height()\n",
        "                ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                        f'{rate:.0f}%', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "            for bar, count in zip(bars2, counts):\n",
        "                height = bar.get_height()\n",
        "                ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                        str(count), ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "        # ============================================================================\n",
        "        # 6. MODEL TYPE PERFORMANCE (bottom left)\n",
        "        # ============================================================================\n",
        "        ax6 = fig.add_subplot(gs[2, :2])\n",
        "\n",
        "        model_types = {\n",
        "            'Tree': ['RandomForest', 'ExtraTrees', 'DecisionTree'],\n",
        "            'Boosting': ['GradientBoosting', 'XGBoost', 'LightGBM', 'CatBoost'],\n",
        "            'Linear': ['Ridge', 'Lasso', 'ElasticNet'],\n",
        "            'Neural': ['SimpleNN', 'MLP'],\n",
        "            'Other': ['SVR', 'Robust', 'Stacking']\n",
        "        }\n",
        "\n",
        "        type_performance = []\n",
        "        for category, keywords in model_types.items():\n",
        "            category_models = df[df['Model'].str.contains('|'.join(keywords), case=False, na=False)]\n",
        "            if len(category_models) > 0:\n",
        "                type_performance.append({\n",
        "                    'Type': category,\n",
        "                    'Count': len(category_models),\n",
        "                    'Winners': category_models['Beats_Baseline'].sum(),\n",
        "                    'Win_Rate': (category_models['Beats_Baseline'].sum() / len(category_models)) * 100,\n",
        "                    'Best_MAE': category_models['MAE'].min(),\n",
        "                    'Avg_Improvement': category_models[category_models['Beats_Baseline']]['MAE_Improvement'].mean()\n",
        "                })\n",
        "\n",
        "        if type_performance:\n",
        "            type_df = pd.DataFrame(type_performance)\n",
        "            type_df = type_df.sort_values('Win_Rate', ascending=False)\n",
        "\n",
        "            y_pos = np.arange(len(type_df))\n",
        "            colors_bar = ['green' if x == 100 else 'yellow' if x >= 80 else 'orange' if x >= 50 else 'red'\n",
        "                         for x in type_df['Win_Rate']]\n",
        "\n",
        "            bars = ax6.barh(y_pos, type_df['Win_Rate'], color=colors_bar, alpha=0.7)\n",
        "            ax6.set_yticks(y_pos)\n",
        "            ax6.set_yticklabels(type_df['Type'])\n",
        "            ax6.set_xlabel('Success Rate %', fontweight='bold')\n",
        "            ax6.set_title('Performance by Model Type', fontweight='bold')\n",
        "            ax6.axvline(x=50, color='black', linestyle='--', alpha=0.3)\n",
        "\n",
        "            # Add value labels\n",
        "            for bar, (_, row) in zip(bars, type_df.iterrows()):\n",
        "                width = bar.get_width()\n",
        "                label = f\"{row['Win_Rate']:.0f}% ({row['Winners']}/{row['Count']})\"\n",
        "                ax6.text(width + 2, bar.get_y() + bar.get_height()/2,\n",
        "                        label, va='center', fontsize=8)\n",
        "\n",
        "        # ============================================================================\n",
        "        # 7. DEPLOYMENT READINESS (bottom right)\n",
        "        # ============================================================================\n",
        "        ax7 = fig.add_subplot(gs[2, 2:])\n",
        "\n",
        "        # Deployment criteria assessment\n",
        "        criteria = {\n",
        "            'Model Performance': win_rate >= 50,\n",
        "            'Champion > Baseline': best_model['Beats_Baseline'],\n",
        "            'Significant Improvement': best_model['MAE_Improvement'] > 20,\n",
        "            'Multiple Options': winners >= 5,\n",
        "            'Stable Across Phases': all(phase_stats.get(p, {}).get('win_rate', 0) > 50 for p in ['V1', 'V2', 'VP']) if phase_stats else False\n",
        "        }\n",
        "\n",
        "        readiness_score = sum(criteria.values()) / len(criteria) * 100\n",
        "\n",
        "        # Create readiness visualization\n",
        "        criteria_names = list(criteria.keys())\n",
        "        criteria_values = [100 if v else 0 for v in criteria.values()]\n",
        "        colors_criteria = ['green' if v else 'red' for v in criteria.values()]\n",
        "\n",
        "        y_pos = np.arange(len(criteria_names))\n",
        "        bars = ax7.barh(y_pos, criteria_values, color=colors_criteria, alpha=0.7)\n",
        "\n",
        "        ax7.set_yticks(y_pos)\n",
        "        ax7.set_yticklabels(criteria_names)\n",
        "        ax7.set_xlim(0, 120)\n",
        "        ax7.set_xlabel('Pass/Fail', fontweight='bold')\n",
        "        ax7.set_title(f'Deployment Readiness Score: {readiness_score:.0f}%', fontweight='bold')\n",
        "\n",
        "        # Add status labels\n",
        "        for i, (bar, status) in enumerate(zip(bars, criteria.values())):\n",
        "            label = 'PASS' if status else 'FAIL'\n",
        "            color = 'green' if status else 'red'\n",
        "            ax7.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2,\n",
        "                    label, va='center', fontweight='bold', color=color)\n",
        "\n",
        "        # Overall recommendation\n",
        "        if readiness_score >= 80:\n",
        "            recommendation = \"READY FOR DEPLOYMENT\"\n",
        "            rec_color = 'green'\n",
        "        elif readiness_score >= 60:\n",
        "            recommendation = \"CONDITIONALLY READY\"\n",
        "            rec_color = 'orange'\n",
        "        else:\n",
        "            recommendation = \"NOT RECOMMENDED\"\n",
        "            rec_color = 'red'\n",
        "\n",
        "        ax7.text(60, -1.5, recommendation, ha='center', fontsize=12,\n",
        "                fontweight='bold', color=rec_color,\n",
        "                bbox=dict(boxstyle='round,pad=0.5', facecolor=rec_color, alpha=0.2))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def generate_text_report(self):\n",
        "        \"\"\"Generate comprehensive text report\"\"\"\n",
        "\n",
        "        report = []\n",
        "        report.append(\"=\" * 80)\n",
        "        report.append(\"MACHINE LEARNING FORECASTING PROJECT - EXECUTIVE REPORT\")\n",
        "        report.append(\"=\" * 80)\n",
        "        report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        # Executive Summary\n",
        "        total_models = len(self.results_df)\n",
        "        winners = self.results_df['Beats_Baseline'].sum()\n",
        "        win_rate = (winners / total_models) * 100\n",
        "        best_model = self.results_df.iloc[0]\n",
        "\n",
        "        report.append(\"EXECUTIVE SUMMARY\")\n",
        "        report.append(\"-\" * 40)\n",
        "        report.append(f\"• Baseline Performance (Seasonal Naive): MAE = {self.baseline_mae:.1f}\")\n",
        "        report.append(f\"• Total Models Evaluated: {total_models}\")\n",
        "        report.append(f\"• Models Beating Baseline: {winners} ({win_rate:.1f}%)\")\n",
        "        report.append(f\"• Champion Model: {best_model['Model']}\")\n",
        "        report.append(f\"• Champion Performance: MAE = {best_model['MAE']:.1f} ({best_model['MAE_Improvement']:.1f}% improvement)\")\n",
        "\n",
        "        # Phase Analysis\n",
        "        report.append(\"\\nPHASE ANALYSIS\")\n",
        "        report.append(\"-\" * 40)\n",
        "        for phase in ['V1', 'V2', 'VP']:\n",
        "            phase_df = self.results_df[self.results_df['Phase'] == phase]\n",
        "            if len(phase_df) > 0:\n",
        "                phase_winners = phase_df['Beats_Baseline'].sum()\n",
        "                phase_rate = (phase_winners / len(phase_df)) * 100\n",
        "                best_phase = phase_df.iloc[0]\n",
        "                report.append(f\"• {phase}: {phase_winners}/{len(phase_df)} models beat baseline ({phase_rate:.1f}%)\")\n",
        "                report.append(f\"  Best: {best_phase['Model'][:40]} (MAE: {best_phase['MAE']:.1f})\")\n",
        "\n",
        "        # Top 5 Models\n",
        "        report.append(\"\\nTOP 5 MODELS\")\n",
        "        report.append(\"-\" * 40)\n",
        "        for i, (_, row) in enumerate(self.results_df.head(5).iterrows(), 1):\n",
        "            report.append(f\"{i}. {row['Model'][:40]}\")\n",
        "            report.append(f\"   Phase: {row['Phase']} | MAE: {row['MAE']:.1f} | Improvement: {row['MAE_Improvement']:.1f}%\")\n",
        "\n",
        "        # Recommendations\n",
        "        report.append(\"\\nRECOMMENDATIONS\")\n",
        "        report.append(\"-\" * 40)\n",
        "\n",
        "        if win_rate >= 80:\n",
        "            report.append(\"✓ Strong Performance: ML significantly outperforms baseline\")\n",
        "            report.append(f\"✓ Deploy: {best_model['Model']}\")\n",
        "            report.append(\"✓ Consider ensemble of top 3-5 models for additional robustness\")\n",
        "        elif win_rate >= 50:\n",
        "            report.append(\"✓ Moderate Performance: ML adds value over baseline\")\n",
        "            report.append(f\"✓ Deploy: {best_model['Model']} with monitoring\")\n",
        "            report.append(\"✓ Continue model refinement and feature engineering\")\n",
        "        else:\n",
        "            report.append(\"⚠ Limited Performance: Few models beat baseline\")\n",
        "            report.append(\"⚠ Consider keeping seasonal naive as backup\")\n",
        "            report.append(\"⚠ Investigate data quality and feature engineering\")\n",
        "\n",
        "        # Next Steps\n",
        "        report.append(\"\\nNEXT STEPS\")\n",
        "        report.append(\"-\" * 40)\n",
        "        report.append(\"1. Deploy champion model in staging environment\")\n",
        "        report.append(\"2. Set up A/B testing against current system\")\n",
        "        report.append(\"3. Monitor performance metrics daily\")\n",
        "        report.append(\"4. Implement model retraining pipeline\")\n",
        "        report.append(\"5. Document model specifications and dependencies\")\n",
        "\n",
        "        report.append(\"\\n\" + \"=\" * 80)\n",
        "        report.append(\"END OF REPORT\")\n",
        "        report.append(\"=\" * 80)\n",
        "\n",
        "        return \"\\n\".join(report)\n",
        "\n",
        "# ============================================================================\n",
        "# GENERATE EXECUTIVE SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"📊 GENERATING EXECUTIVE SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create summary generator\n",
        "summary_generator = ExecutiveSummaryGenerator(results_df, seasonal_naive_mae, seasonal_naive_rmse)\n",
        "\n",
        "# Generate executive dashboard\n",
        "print(\"\\nCreating executive dashboard...\")\n",
        "exec_fig = summary_generator.create_executive_dashboard()\n",
        "plt.show()\n",
        "\n",
        "# Generate text report\n",
        "print(\"\\nGenerating text report...\")\n",
        "text_report = summary_generator.generate_text_report()\n",
        "print(text_report)\n",
        "\n",
        "# ============================================================================\n",
        "# EXPORT RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"📁 EXPORTING RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save comprehensive results\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# 1. Save detailed results CSV\n",
        "detailed_filename = f'ml_forecasting_detailed_results_{timestamp}.csv'\n",
        "results_df.to_csv(detailed_filename, index=False)\n",
        "print(f\"✓ Detailed results saved to: {detailed_filename}\")\n",
        "\n",
        "# 2. Save executive summary CSV\n",
        "summary_data = {\n",
        "    'Metric': ['Baseline MAE', 'Total Models', 'Models Beating Baseline', 'Success Rate',\n",
        "               'Champion Model', 'Champion MAE', 'Champion Improvement'],\n",
        "    'Value': [seasonal_naive_mae, len(results_df), results_df['Beats_Baseline'].sum(),\n",
        "             f\"{(results_df['Beats_Baseline'].sum()/len(results_df))*100:.1f}%\",\n",
        "             results_df.iloc[0]['Model'], results_df.iloc[0]['MAE'],\n",
        "             f\"{results_df.iloc[0]['MAE_Improvement']:.1f}%\"]\n",
        "}\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_filename = f'ml_forecasting_executive_summary_{timestamp}.csv'\n",
        "summary_df.to_csv(summary_filename, index=False)\n",
        "print(f\"✓ Executive summary saved to: {summary_filename}\")\n",
        "\n",
        "# 3. Save text report\n",
        "report_filename = f'ml_forecasting_report_{timestamp}.txt'\n",
        "with open(report_filename, 'w') as f:\n",
        "    f.write(text_report)\n",
        "print(f\"✓ Text report saved to: {report_filename}\")\n",
        "\n",
        "# 4. Save executive dashboard\n",
        "dashboard_filename = f'ml_forecasting_dashboard_{timestamp}.png'\n",
        "exec_fig.savefig(dashboard_filename, dpi=300, bbox_inches='tight')\n",
        "print(f\"✓ Dashboard saved to: {dashboard_filename}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL STATUS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✅ ML FORECASTING PIPELINE COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n🏆 FINAL RESULTS:\")\n",
        "print(f\"   Champion Model: {results_df.iloc[0]['Model']}\")\n",
        "print(f\"   Performance: MAE = {results_df.iloc[0]['MAE']:.1f}\")\n",
        "print(f\"   Improvement: {results_df.iloc[0]['MAE_Improvement']:.1f}% better than baseline\")\n",
        "print(f\"   Success Rate: {(results_df['Beats_Baseline'].sum()/len(results_df))*100:.1f}% of models beat baseline\")\n",
        "\n",
        "print(f\"\\n📁 ALL FILES SAVED:\")\n",
        "print(f\"   • Detailed results: {detailed_filename}\")\n",
        "print(f\"   • Executive summary: {summary_filename}\")\n",
        "print(f\"   • Text report: {report_filename}\")\n",
        "print(f\"   • Dashboard: {dashboard_filename}\")\n",
        "\n",
        "print(\"\\n🎉 PROJECT SUCCESSFULLY COMPLETED!\")"
      ],
      "metadata": {
        "id": "AuhuQnW48Lv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}