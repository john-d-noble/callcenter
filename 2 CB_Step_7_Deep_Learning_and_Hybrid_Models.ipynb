{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/2%20CB_Step_7_Deep_Learning_and_Hybrid_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn tensorflow prophet xgboost neuralprophet"
      ],
      "metadata": {
        "id": "MZjcmIH6-2yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f7d1c04"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Inspect column names in the CSV file\n",
        "df_inspect = pd.read_csv('all_combined_data.csv')\n",
        "print(df_inspect.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from prophet import Prophet\n",
        "from xgboost import XGBRegressor\n",
        "from neuralprophet import NeuralProphet\n",
        "import torch  # Added for safe_globals\n",
        "# Fix for UnpicklingError: Import the actual class and add to safe_globals\n",
        "from neuralprophet.configure import ConfigSeasonality\n",
        "torch.serialization.add_safe_globals([ConfigSeasonality])\n",
        "\n",
        "# Step 1: Load the data from CSV file with explicit date format to suppress parsing warning\n",
        "df = pd.read_csv('all_combined_data.csv', parse_dates=['Unnamed: 0'], date_format='%m/%d/%y', index_col='Unnamed: 0')\n",
        "\n",
        "# Assume 'Calls' is the target column\n",
        "target = 'Calls'\n",
        "\n",
        "# Prepare data: Sort by date if not already\n",
        "df = df.sort_index()\n",
        "\n",
        "# Feature Engineering (similar to ML: lags, rollings, dummies)\n",
        "df['Lag1'] = df[target].shift(1)\n",
        "df['Lag7'] = df[target].shift(7)\n",
        "df['Rolling_Mean_7'] = df[target].rolling(window=7).mean()\n",
        "df['Rolling_Std_7'] = df[target].rolling(window=7).std()\n",
        "df = pd.get_dummies(df, columns=['DayOfWeek'], drop_first=True)\n",
        "\n",
        "# Drop NaNs\n",
        "df = df.dropna()\n",
        "\n",
        "# Get the list of feature columns AFTER feature engineering and dummy variable creation\n",
        "feature_columns = df.drop(columns=[target]).columns.tolist()\n",
        "\n",
        "# Time series cross-validation: 5 splits\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Function to calculate metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100  # As percentage\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
        "\n",
        "# Dictionary to store average metrics for each model\n",
        "model_metrics = {}\n",
        "\n",
        "# Helper to create sequences for LSTM (timesteps=7)\n",
        "def create_sequences(data, timesteps=7):\n",
        "    X_seq, y_seq = [], []\n",
        "    # Features are all columns except the target\n",
        "    feature_cols_lstm = [col for col in data.columns if col != target]\n",
        "    for i in range(len(data) - timesteps):\n",
        "        X_seq.append(data[feature_cols_lstm].iloc[i:i+timesteps].values)\n",
        "        y_seq.append(data[target].iloc[i+timesteps])  # Predict next value\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# 1. LSTM Network\n",
        "lstm_preds_inv = []\n",
        "lstm_trues_inv = []\n",
        "for train_idx, test_idx in tscv.split(df):\n",
        "    train_df = df.iloc[train_idx]\n",
        "    test_df = df.iloc[test_idx]\n",
        "\n",
        "    # Scalers: Fit on train only\n",
        "    feature_scaler = MinMaxScaler()\n",
        "    feature_scaler.fit(train_df[feature_columns])\n",
        "\n",
        "    target_scaler = MinMaxScaler()\n",
        "    target_scaler.fit(train_df[[target]])  # Fit as 2D array\n",
        "\n",
        "    # Scale train and test\n",
        "    train_scaled = pd.DataFrame(feature_scaler.transform(train_df[feature_columns]),\n",
        "                                index=train_df.index, columns=feature_columns)\n",
        "    train_scaled[target] = target_scaler.transform(train_df[[target]])\n",
        "\n",
        "    test_scaled = pd.DataFrame(feature_scaler.transform(test_df[feature_columns]),\n",
        "                               index=test_df.index, columns=feature_columns)\n",
        "    test_scaled[target] = target_scaler.transform(test_df[[target]])\n",
        "\n",
        "    # Create sequences\n",
        "    X_train_seq, y_train_seq = create_sequences(train_scaled)\n",
        "    X_test_seq, y_test_seq = create_sequences(test_scaled)\n",
        "\n",
        "    # Build LSTM model with Input layer to suppress warning\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(7, X_train_seq.shape[2])))  # Use shape of X_train_seq\n",
        "    model.add(LSTM(50, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Fit\n",
        "    model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predict (scaled)\n",
        "    pred_scaled = model.predict(X_test_seq, verbose=0)\n",
        "\n",
        "    # Inverse scale predictions and trues\n",
        "    pred_inv = target_scaler.inverse_transform(pred_scaled)\n",
        "    true_inv = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
        "\n",
        "    lstm_preds_inv.extend(pred_inv.flatten())\n",
        "    lstm_trues_inv.extend(true_inv.flatten())\n",
        "\n",
        "lstm_metrics = calculate_metrics(lstm_trues_inv, lstm_preds_inv)\n",
        "model_metrics['LSTM'] = lstm_metrics\n",
        "\n",
        "# 2. Neural Prophet (with matplotlib backend to avoid Plotly error)\n",
        "np_preds = []\n",
        "np_trues = []\n",
        "for train_idx, test_idx in tscv.split(df):\n",
        "    train_temp = df.iloc[train_idx].reset_index()\n",
        "    date_col = train_temp.columns[0]  # The added index column (date)\n",
        "    train_df = train_temp.rename(columns={date_col: 'ds', 'Calls': 'y'})\n",
        "\n",
        "    test_temp = df.iloc[test_idx].reset_index()\n",
        "    test_df = test_temp.rename(columns={date_col: 'ds', 'Calls': 'y'})  # Use same date_col name assuming consistent\n",
        "\n",
        "    # Fit NeuralProphet with matplotlib and explicit seasonalities\n",
        "    model = NeuralProphet(epochs=50, batch_size=32, learning_rate=0.01,  # Manual LR to avoid finder warning\n",
        "                          yearly_seasonality=False, daily_seasonality=False)  # Explicit to suppress auto warnings\n",
        "    model.set_plotting_backend('matplotlib')\n",
        "    model.fit(train_df[['ds', 'y']], freq='D')\n",
        "\n",
        "    # Make future dataframe\n",
        "    future = model.make_future_dataframe(train_df[['ds', 'y']], periods=len(test_df))\n",
        "\n",
        "    # Predict\n",
        "    forecast = model.predict(future)\n",
        "    pred = forecast['yhat1'].tail(len(test_df)).values\n",
        "    np_preds.extend(pred)\n",
        "    np_trues.extend(test_df['y'])\n",
        "\n",
        "np_metrics = calculate_metrics(np_trues, np_preds)\n",
        "model_metrics['Neural Prophet'] = np_metrics\n",
        "\n",
        "# 3. Hybrid: Prophet + XGBoost (Prophet for trend/seasonal, XGBoost on residuals)\n",
        "hybrid_preds = []\n",
        "hybrid_trues = []\n",
        "for train_idx, test_idx in tscv.split(df):\n",
        "    train_df_hybrid = df.iloc[train_idx]\n",
        "    test_df_hybrid = df.iloc[test_idx]\n",
        "\n",
        "    # Step 1: Fit Prophet\n",
        "    # Prepare data for Prophet\n",
        "    prophet_train_temp = train_df_hybrid.reset_index()\n",
        "    date_col = prophet_train_temp.columns[0]  # The added index column (date)\n",
        "    prophet_train_df = prophet_train_temp.rename(columns={date_col: 'ds', 'Calls': 'y'})\n",
        "\n",
        "    prophet_model = Prophet(weekly_seasonality=True)\n",
        "    prophet_model.fit(prophet_train_df[['ds', 'y']])\n",
        "\n",
        "    # Predict on train for residuals\n",
        "    train_future_prophet = prophet_model.make_future_dataframe(periods=0)\n",
        "    train_forecast_prophet = prophet_model.predict(train_future_prophet)\n",
        "    train_residuals = train_df_hybrid['Calls'] - train_forecast_prophet['yhat'].values\n",
        "\n",
        "    # Predict on test for base forecast\n",
        "    test_future_prophet = prophet_model.make_future_dataframe(periods=len(test_df_hybrid))\n",
        "    test_forecast_prophet = prophet_model.predict(test_future_prophet)\n",
        "    prophet_test_pred = test_forecast_prophet['yhat'].tail(len(test_df_hybrid)).values\n",
        "\n",
        "    # Step 2: Fit XGBoost on residuals using features\n",
        "    # Select features for XGBoost after feature engineering\n",
        "    xgb_features = [col for col in train_df_hybrid.columns if col != target]\n",
        "    X_train = train_df_hybrid[xgb_features]\n",
        "    y_res_train = train_residuals\n",
        "    xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "    xgb_model.fit(X_train, y_res_train)\n",
        "\n",
        "    # Predict residuals on test\n",
        "    X_test = test_df_hybrid[xgb_features]\n",
        "    res_pred = xgb_model.predict(X_test)\n",
        "\n",
        "    # Combine: Prophet pred + residual pred\n",
        "    pred = prophet_test_pred + res_pred\n",
        "    hybrid_preds.extend(pred)\n",
        "    hybrid_trues.extend(test_df_hybrid[target])\n",
        "\n",
        "hybrid_metrics = calculate_metrics(hybrid_trues, hybrid_preds)\n",
        "model_metrics['Prophet + XGBoost Hybrid'] = hybrid_metrics\n",
        "\n",
        "# Summarize performance\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "metrics_df = pd.DataFrame(model_metrics).T\n",
        "print(metrics_df)\n",
        "\n",
        "# Pick winner: Lowest MAE (primary metric)\n",
        "winner = metrics_df['MAE'].idxmin()\n",
        "print(f\"\\nChampion DL/Hybrid Model: {winner}\")\n",
        "print(f\"Metrics: {metrics_df.loc[winner].to_dict()}\")"
      ],
      "metadata": {
        "id": "lG6FdV3tW6Yy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}