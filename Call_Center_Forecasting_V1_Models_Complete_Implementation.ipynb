{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1/QbVJPDGB3HRpz/VS42S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/Call_Center_Forecasting_V1_Models_Complete_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Call Center Forecasting V1 Models - Complete Implementation\n",
        "\n",
        "# %% Hardware Check (CRITICAL: Must be first to define ENABLE_NEURAL)\n",
        "print(\"üñ•Ô∏è COMPUTATIONAL ENVIRONMENT CHECK\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# GPU Check\n",
        "try:\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "        print('‚ùå Not connected to a GPU')\n",
        "        print('üí° Neural models (LSTM, CNN) will run on CPU (slower)')\n",
        "        GPU_AVAILABLE = False\n",
        "    else:\n",
        "        print('‚úÖ GPU Available:')\n",
        "        print(gpu_info)\n",
        "        GPU_AVAILABLE = True\n",
        "except:\n",
        "    print('‚ùå GPU check failed - assuming no GPU')\n",
        "    GPU_AVAILABLE = False\n",
        "\n",
        "# RAM Check\n",
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print(f'\\nüíæ RAM Status: {ram_gb:.1f} GB available')\n",
        "\n",
        "if ram_gb < 20:\n",
        "    print('‚ö†Ô∏è Standard RAM - may limit large ensemble grid searches')\n",
        "    HIGH_RAM = False\n",
        "else:\n",
        "    print('‚úÖ High-RAM runtime - can handle complex model combinations!')\n",
        "    HIGH_RAM = True\n",
        "\n",
        "# Set computational strategy based on resources\n",
        "print(f\"\\nüéØ COMPUTATIONAL STRATEGY:\")\n",
        "if GPU_AVAILABLE and HIGH_RAM:\n",
        "    print(\"   üöÄ FULL POWER: GPU + High RAM - All models enabled\")\n",
        "    ENABLE_NEURAL = True\n",
        "    ENABLE_LARGE_GRIDS = True\n",
        "elif GPU_AVAILABLE:\n",
        "    print(\"   ‚ö° GPU enabled, moderate RAM - Neural models OK, smaller grids\")\n",
        "    ENABLE_NEURAL = True\n",
        "    ENABLE_LARGE_GRIDS = False\n",
        "elif HIGH_RAM:\n",
        "    print(\"   üß† High RAM, no GPU - Large ensembles OK, neural models slower\")\n",
        "    ENABLE_NEURAL = False  # Still possible but slower\n",
        "    ENABLE_LARGE_GRIDS = True\n",
        "else:\n",
        "    print(\"   üí° Standard setup - Focus on efficient models\")\n",
        "    ENABLE_NEURAL = False\n",
        "    ENABLE_LARGE_GRIDS = False\n",
        "\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# %% Imports and Setup\n",
        "print(\"\\nüìö IMPORTING LIBRARIES\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Statistical and time series\n",
        "from scipy import stats\n",
        "from scipy.stats import jarque_bera, shapiro, mode\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.exponential_smoothing.ets import ETSModel\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "\n",
        "# Advanced time series models\n",
        "try:\n",
        "    from statsmodels.tsa.statespace.tools import diff\n",
        "    from statsmodels.tsa.seasonal import STL\n",
        "    ADVANCED_TS_AVAILABLE = True\n",
        "    print(\"‚úÖ Advanced time series models available\")\n",
        "except ImportError:\n",
        "    ADVANCED_TS_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è Some advanced TS models may not be available\")\n",
        "\n",
        "# Prophet\n",
        "try:\n",
        "    from prophet import Prophet\n",
        "    PROPHET_AVAILABLE = True\n",
        "    print(\"‚úÖ Prophet available\")\n",
        "except ImportError:\n",
        "    PROPHET_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è Prophet not available\")\n",
        "\n",
        "# Machine Learning models\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "\n",
        "# Neural networks (if GPU available)\n",
        "if ENABLE_NEURAL:\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, Flatten\n",
        "        print(\"‚úÖ TensorFlow/Keras available for neural models\")\n",
        "        KERAS_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è TensorFlow not available - skipping neural models\")\n",
        "        KERAS_AVAILABLE = False\n",
        "        ENABLE_NEURAL = False\n",
        "else:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "# Visualization setup\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Model versioning\n",
        "MODEL_VERSION = \"V1\"\n",
        "print(f\"\\nüè∑Ô∏è MODEL VERSION: {MODEL_VERSION}\")\n",
        "print(\"üìä Phase 1: Basic Statistical + Advanced Time Series + Hybrid Models\")\n",
        "\n",
        "print(\"\\n‚úÖ Setup Complete - Ready for Phase 1 Model Development!\")\n",
        "\n",
        "# %% Data Loading Function\n",
        "def load_call_center_data_v1(file_path='enhanced_eda_data.csv'):\n",
        "    \"\"\"\n",
        "    Load call center data with market integration for V1 models\n",
        "    Includes data cleaning (trim first/last rows) and market feature creation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üìÅ LOADING CALL CENTER DATA (V1)\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    try:\n",
        "        # Load main data file\n",
        "        df = pd.read_csv(file_path, index_col='Date', parse_dates=True)\n",
        "        print(f\"‚úÖ Loaded {len(df)} records from {file_path}\")\n",
        "\n",
        "        # Auto-detect call volume column\n",
        "        volume_cols = ['calls', 'Calls', 'call_volume', 'Call_Volume', 'volume', 'Volume']\n",
        "        volume_col = None\n",
        "\n",
        "        for col in volume_cols:\n",
        "            if col in df.columns:\n",
        "                volume_col = col\n",
        "                break\n",
        "\n",
        "        if volume_col is None:\n",
        "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "            volume_col = numeric_cols[0] if len(numeric_cols) > 0 else df.columns[0]\n",
        "\n",
        "        print(f\"üéØ Call volume column: {volume_col}\")\n",
        "\n",
        "        # Standardize column name\n",
        "        if volume_col != 'calls':\n",
        "            df = df.rename(columns={volume_col: 'calls'})\n",
        "\n",
        "        # DATA CLEANING: Remove first and last rows (as requested)\n",
        "        print(\"üßπ DATA CLEANING: Removing first and last rows\")\n",
        "        original_len = len(df)\n",
        "        if len(df) > 2:\n",
        "            first_date = df.index[0].strftime('%Y-%m-%d')\n",
        "            last_date = df.index[-1].strftime('%Y-%m-%d')\n",
        "            first_calls = df['calls'].iloc[0]\n",
        "            last_calls = df['calls'].iloc[-1]\n",
        "\n",
        "            print(f\"   üóëÔ∏è Removing first: {first_date} ({first_calls:.0f} calls)\")\n",
        "            print(f\"   üóëÔ∏è Removing last:  {last_date} ({last_calls:.0f} calls)\")\n",
        "\n",
        "            df = df.iloc[1:-1]\n",
        "            print(f\"   ‚úÖ Cleaned: {original_len} ‚Üí {len(df)} rows\")\n",
        "\n",
        "        # CHECK FOR MARKET DATA IN MAIN FILE\n",
        "        print(f\"\\nüìà MARKET DATA INTEGRATION\")\n",
        "        print(\"-\" * 25)\n",
        "\n",
        "        expected_market_cols = [\n",
        "            '^VIX_close', 'SPY_close', 'SPY_volume', 'QQQ_close', 'QQQ_volume',\n",
        "            'DX-Y.NYB_close', 'GC=F_close', 'GC=F_volume', 'BTC-USD_close',\n",
        "            'BTC-USD_volume', 'ETH-USD_close', 'ETH-USD_volume'\n",
        "        ]\n",
        "\n",
        "        existing_market_cols = [col for col in expected_market_cols if col in df.columns]\n",
        "\n",
        "        if existing_market_cols:\n",
        "            print(f\"‚úÖ Market data found: {len(existing_market_cols)} columns\")\n",
        "            for col in existing_market_cols[:5]:  # Show first 5\n",
        "                print(f\"   ‚Ä¢ {col}\")\n",
        "            if len(existing_market_cols) > 5:\n",
        "                print(f\"   ‚Ä¢ ... and {len(existing_market_cols)-5} more\")\n",
        "\n",
        "            # CREATE MARKET-DERIVED FEATURES\n",
        "            print(f\"\\nüîß Creating market-derived features...\")\n",
        "\n",
        "            # VIX features\n",
        "            if '^VIX_close' in df.columns:\n",
        "                df['vix_high'] = (df['^VIX_close'] > df['^VIX_close'].quantile(0.8)).astype(int)\n",
        "                df['vix_spike'] = (df['^VIX_close'].pct_change() > 0.2).astype(int)\n",
        "                print(\"   üìà VIX volatility features created\")\n",
        "\n",
        "            # Stock market features\n",
        "            if 'SPY_close' in df.columns:\n",
        "                df['spy_returns'] = df['SPY_close'].pct_change()\n",
        "                df['market_stress'] = (df['spy_returns'] < -0.02).astype(int)\n",
        "                df['spy_volatility'] = df['spy_returns'].rolling(7).std()\n",
        "                print(\"   üìâ Stock market stress features created\")\n",
        "\n",
        "            # Crypto features\n",
        "            if 'BTC-USD_close' in df.columns:\n",
        "                df['btc_returns'] = df['BTC-USD_close'].pct_change()\n",
        "                df['crypto_volatility'] = df['btc_returns'].rolling(7).std()\n",
        "                df['btc_extreme_move'] = (abs(df['btc_returns']) > 0.1).astype(int)\n",
        "                print(\"   ‚Çø Crypto volatility features created\")\n",
        "\n",
        "            # Market uncertainty composite\n",
        "            uncertainty_features = []\n",
        "            if '^VIX_close' in df.columns:\n",
        "                uncertainty_features.append(df['^VIX_close'])\n",
        "            if 'spy_volatility' in df.columns:\n",
        "                uncertainty_features.append(df['spy_volatility'] * 100)\n",
        "            if 'crypto_volatility' in df.columns:\n",
        "                uncertainty_features.append(df['crypto_volatility'] * 100)\n",
        "\n",
        "            if uncertainty_features:\n",
        "                uncertainty_matrix = pd.concat(uncertainty_features, axis=1)\n",
        "                df['market_uncertainty_index'] = uncertainty_matrix.mean(axis=1)\n",
        "                print(\"   üåä Market uncertainty index created\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No market data columns found in main file\")\n",
        "\n",
        "        # Final data overview\n",
        "        print(f\"\\nüìä FINAL DATASET OVERVIEW\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"   Date range: {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\")\n",
        "        print(f\"   Total days: {len(df)}\")\n",
        "        print(f\"   Total columns: {len(df.columns)}\")\n",
        "        print(f\"   Call volume range: {df['calls'].min():.0f} to {df['calls'].max():.0f}\")\n",
        "        print(f\"   Missing values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# %% Load Data\n",
        "df_raw = load_call_center_data_v1()\n",
        "\n",
        "if df_raw is not None:\n",
        "    print(f\"\\n‚úÖ Data loading successful - Ready for cross-validation setup!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Data loading failed - Check file path and format\")\n",
        "    raise Exception(\"Data loading failed\")\n",
        "\n",
        "# %% Cross-Validation Setup\n",
        "def create_time_series_splits_v1(df, n_splits=5, test_size=7, gap=0):\n",
        "    \"\"\"\n",
        "    Create time series cross-validation splits BEFORE feature engineering\n",
        "    This prevents data leakage by ensuring no future information in features\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üîí TIME SERIES CROSS-VALIDATION SETUP (V1)\")\n",
        "    print(\"=\" * 45)\n",
        "    print(\"‚ö†Ô∏è Creating splits BEFORE feature engineering to prevent data leakage\")\n",
        "\n",
        "    splits = []\n",
        "    total_size = len(df)\n",
        "\n",
        "    for i in range(n_splits):\n",
        "        # Calculate split points (working backwards from end)\n",
        "        test_end = total_size - i * test_size\n",
        "        test_start = test_end - test_size\n",
        "        train_end = test_start - gap\n",
        "\n",
        "        if train_end < 30:  # Need minimum 30 days for training\n",
        "            break\n",
        "\n",
        "        train_idx = df.index[:train_end]\n",
        "        test_idx = df.index[test_start:test_end]\n",
        "\n",
        "        splits.append({\n",
        "            'train_idx': train_idx,\n",
        "            'test_idx': test_idx,\n",
        "            'train_size': len(train_idx),\n",
        "            'test_size': len(test_idx),\n",
        "            'split_date': test_idx[0] if len(test_idx) > 0 else None\n",
        "        })\n",
        "\n",
        "    print(f\"‚úÖ Created {len(splits)} data-leakage-free splits:\")\n",
        "    for i, split in enumerate(splits):\n",
        "        print(f\"  Split {i+1}: Train {split['train_size']} days ‚Üí Test {split['test_size']} days\")\n",
        "        print(f\"    Train: {split['train_idx'][0].strftime('%Y-%m-%d')} to {split['train_idx'][-1].strftime('%Y-%m-%d')}\")\n",
        "        print(f\"    Test:  {split['test_idx'][0].strftime('%Y-%m-%d')} to {split['test_idx'][-1].strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    return splits\n",
        "\n",
        "# Create CV splits\n",
        "cv_splits = create_time_series_splits_v1(df_raw, n_splits=5, test_size=7, gap=0)\n",
        "\n",
        "print(f\"\\nüîí Cross-validation framework established\")\n",
        "print(f\"üìä Ready for per-split feature engineering\")\n",
        "\n",
        "# %% Feature Engineering\n",
        "def create_features_v1(df_train, df_test=None):\n",
        "    \"\"\"\n",
        "    Create comprehensive feature set using ONLY training data statistics\n",
        "    Apply same transformations to test data using training-derived parameters\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üõ†Ô∏è FEATURE ENGINEERING V1 (Leakage-Free)\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"‚úÖ Using ONLY training data statistics\")\n",
        "\n",
        "    # Work on training data first\n",
        "    df_features_train = df_train.copy()\n",
        "\n",
        "    # TIME-BASED FEATURES (no leakage risk)\n",
        "    df_features_train['year'] = df_features_train.index.year\n",
        "    df_features_train['month'] = df_features_train.index.month\n",
        "    df_features_train['day'] = df_features_train.index.day\n",
        "    df_features_train['dayofweek'] = df_features_train.index.dayofweek\n",
        "    df_features_train['dayofyear'] = df_features_train.index.dayofyear\n",
        "    df_features_train['quarter'] = df_features_train.index.quarter\n",
        "    df_features_train['week'] = df_features_train.index.isocalendar().week\n",
        "\n",
        "    # CYCLICAL ENCODING (for better ML model performance)\n",
        "    df_features_train['month_sin'] = np.sin(2 * np.pi * df_features_train['month'] / 12)\n",
        "    df_features_train['month_cos'] = np.cos(2 * np.pi * df_features_train['month'] / 12)\n",
        "    df_features_train['dow_sin'] = np.sin(2 * np.pi * df_features_train['dayofweek'] / 7)\n",
        "    df_features_train['dow_cos'] = np.cos(2 * np.pi * df_features_train['dayofweek'] / 7)\n",
        "    df_features_train['doy_sin'] = np.sin(2 * np.pi * df_features_train['dayofyear'] / 365.25)\n",
        "    df_features_train['doy_cos'] = np.cos(2 * np.pi * df_features_train['dayofyear'] / 365.25)\n",
        "\n",
        "    # BINARY FEATURES\n",
        "    df_features_train['is_weekend'] = (df_features_train['dayofweek'] >= 5).astype(int)\n",
        "    df_features_train['is_monday'] = (df_features_train['dayofweek'] == 0).astype(int)\n",
        "    df_features_train['is_friday'] = (df_features_train['dayofweek'] == 4).astype(int)\n",
        "    df_features_train['is_month_start'] = df_features_train.index.is_month_start.astype(int)\n",
        "    df_features_train['is_month_end'] = df_features_train.index.is_month_end.astype(int)\n",
        "\n",
        "    # LAG FEATURES (using only training data)\n",
        "    for lag in [1, 2, 3, 7]:\n",
        "        df_features_train[f'calls_lag_{lag}'] = df_features_train['calls'].shift(lag)\n",
        "\n",
        "    # ROLLING STATISTICS (using only training data)\n",
        "    for window in [7, 14, 30]:\n",
        "        df_features_train[f'calls_mean_{window}d'] = df_features_train['calls'].rolling(window).mean()\n",
        "        df_features_train[f'calls_std_{window}d'] = df_features_train['calls'].rolling(window).std()\n",
        "        df_features_train[f'calls_min_{window}d'] = df_features_train['calls'].rolling(window).min()\n",
        "        df_features_train[f'calls_max_{window}d'] = df_features_train['calls'].rolling(window).max()\n",
        "\n",
        "    # MARKET FEATURES (if available, using training thresholds only)\n",
        "    market_features_created = 0\n",
        "    if '^VIX_close' in df_features_train.columns:\n",
        "        train_vix_high_threshold = df_features_train['^VIX_close'].quantile(0.8)\n",
        "        df_features_train['vix_high_train'] = (df_features_train['^VIX_close'] > train_vix_high_threshold).astype(int)\n",
        "        market_features_created += 1\n",
        "\n",
        "    if 'spy_returns' in df_features_train.columns:\n",
        "        df_features_train['market_stress_train'] = (df_features_train['spy_returns'] < -0.02).astype(int)\n",
        "        market_features_created += 1\n",
        "\n",
        "    # Count features created\n",
        "    total_features = len(df_features_train.columns) - len(df_train.columns)\n",
        "\n",
        "    print(f\"‚úÖ Created {total_features} features for training data\")\n",
        "\n",
        "    # Apply same transformations to test data if provided\n",
        "    if df_test is not None:\n",
        "        df_features_test = df_test.copy()\n",
        "\n",
        "        # Apply same time-based features\n",
        "        df_features_test['year'] = df_features_test.index.year\n",
        "        df_features_test['month'] = df_features_test.index.month\n",
        "        df_features_test['day'] = df_features_test.index.day\n",
        "        df_features_test['dayofweek'] = df_features_test.index.dayofweek\n",
        "        df_features_test['dayofyear'] = df_features_test.index.dayofyear\n",
        "        df_features_test['quarter'] = df_features_test.index.quarter\n",
        "        df_features_test['week'] = df_features_test.index.isocalendar().week\n",
        "\n",
        "        # Apply same cyclical encoding\n",
        "        df_features_test['month_sin'] = np.sin(2 * np.pi * df_features_test['month'] / 12)\n",
        "        df_features_test['month_cos'] = np.cos(2 * np.pi * df_features_test['month'] / 12)\n",
        "        df_features_test['dow_sin'] = np.sin(2 * np.pi * df_features_test['dayofweek'] / 7)\n",
        "        df_features_test['dow_cos'] = np.cos(2 * np.pi * df_features_test['dayofweek'] / 7)\n",
        "        df_features_test['doy_sin'] = np.sin(2 * np.pi * df_features_test['dayofyear'] / 365.25)\n",
        "        df_features_test['doy_cos'] = np.cos(2 * np.pi * df_features_test['dayofyear'] / 365.25)\n",
        "\n",
        "        # Apply same binary features\n",
        "        df_features_test['is_weekend'] = (df_features_test['dayofweek'] >= 5).astype(int)\n",
        "        df_features_test['is_monday'] = (df_features_test['dayofweek'] == 0).astype(int)\n",
        "        df_features_test['is_friday'] = (df_features_test['dayofweek'] == 4).astype(int)\n",
        "        df_features_test['is_month_start'] = df_features_test.index.is_month_start.astype(int)\n",
        "        df_features_test['is_month_end'] = df_features_test.index.is_month_end.astype(int)\n",
        "\n",
        "        # For lag and rolling features, combine train+test data but respect temporal order\n",
        "        combined_data = pd.concat([df_features_train['calls'], df_features_test['calls']])\n",
        "\n",
        "        # Apply lag features\n",
        "        for lag in [1, 2, 3, 7]:\n",
        "            df_features_test[f'calls_lag_{lag}'] = combined_data.shift(lag).loc[df_features_test.index]\n",
        "\n",
        "        # Apply rolling features\n",
        "        for window in [7, 14, 30]:\n",
        "            df_features_test[f'calls_mean_{window}d'] = combined_data.rolling(window).mean().loc[df_features_test.index]\n",
        "            df_features_test[f'calls_std_{window}d'] = combined_data.rolling(window).std().loc[df_features_test.index]\n",
        "            df_features_test[f'calls_min_{window}d'] = combined_data.rolling(window).min().loc[df_features_test.index]\n",
        "            df_features_test[f'calls_max_{window}d'] = combined_data.rolling(window).max().loc[df_features_test.index]\n",
        "\n",
        "        # Apply market features using TRAINING thresholds\n",
        "        if '^VIX_close' in df_features_test.columns and '^VIX_close' in df_features_train.columns:\n",
        "            df_features_test['vix_high_train'] = (df_features_test['^VIX_close'] > train_vix_high_threshold).astype(int)\n",
        "\n",
        "        if 'spy_returns' in df_features_test.columns and 'spy_returns' in df_features_train.columns:\n",
        "            df_features_test['market_stress_train'] = (df_features_test['spy_returns'] < -0.02).astype(int)\n",
        "\n",
        "        print(f\"‚úÖ Applied same transformations to test data\")\n",
        "        return df_features_train, df_features_test\n",
        "\n",
        "    return df_features_train, None\n",
        "\n",
        "# %% Basic Statistical Models\n",
        "class BasicStatisticalModels_V1:\n",
        "    \"\"\"Collection of basic statistical forecasting models - Version 1\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.model_version = \"V1\"\n",
        "\n",
        "    def fit_mean_v1(self, y_train):\n",
        "        \"\"\"Mean V1: Simple historical average forecast\"\"\"\n",
        "        self.models['mean'] = y_train.mean()\n",
        "        return self\n",
        "\n",
        "    def fit_median_v1(self, y_train):\n",
        "        \"\"\"Median V1: Robust central tendency (outlier resistant)\"\"\"\n",
        "        self.models['median'] = y_train.median()\n",
        "        return self\n",
        "\n",
        "    def fit_naive_v1(self, y_train):\n",
        "        \"\"\"Naive V1: Last observed value\"\"\"\n",
        "        self.models['naive'] = y_train.iloc[-1]\n",
        "        return self\n",
        "\n",
        "    def fit_seasonal_naive_v1(self, y_train, season_length=7):\n",
        "        \"\"\"Seasonal Naive V1: Last value from same season (BENCHMARK MODEL)\"\"\"\n",
        "        if len(y_train) >= season_length:\n",
        "            self.models['seasonal_naive'] = {\n",
        "                'values': y_train.iloc[-season_length:],\n",
        "                'season_length': season_length\n",
        "            }\n",
        "        else:\n",
        "            self.models['seasonal_naive'] = {\n",
        "                'values': y_train,\n",
        "                'season_length': len(y_train)\n",
        "            }\n",
        "        return self\n",
        "\n",
        "    def fit_drift_v1(self, y_train):\n",
        "        \"\"\"Drift V1: Linear trend from first to last observation\"\"\"\n",
        "        n = len(y_train)\n",
        "        if n > 1:\n",
        "            slope = (y_train.iloc[-1] - y_train.iloc[0]) / (n - 1)\n",
        "            self.models['drift'] = {\n",
        "                'last_value': y_train.iloc[-1],\n",
        "                'slope': slope\n",
        "            }\n",
        "        else:\n",
        "            self.models['drift'] = {'last_value': y_train.iloc[-1], 'slope': 0}\n",
        "        return self\n",
        "\n",
        "    def predict(self, steps, model_type):\n",
        "        \"\"\"Generate forecasts for specified number of steps\"\"\"\n",
        "        if model_type in ['mean', 'median', 'naive']:\n",
        "            return np.full(steps, self.models[model_type])\n",
        "\n",
        "        elif model_type == 'seasonal_naive':\n",
        "            model_info = self.models['seasonal_naive']\n",
        "            season_values = model_info['values'].values\n",
        "            season_length = model_info['season_length']\n",
        "            forecasts = []\n",
        "            for i in range(steps):\n",
        "                forecasts.append(season_values[-(season_length - (i % season_length))])\n",
        "            return np.array(forecasts)\n",
        "\n",
        "        elif model_type == 'drift':\n",
        "            model_info = self.models['drift']\n",
        "            last_value = model_info['last_value']\n",
        "            slope = model_info['slope']\n",
        "            return np.array([last_value + slope * (i + 1) for i in range(steps)])\n",
        "\n",
        "def fit_all_basic_models_v1(y_train, forecast_steps):\n",
        "    \"\"\"Fit all basic statistical models and return predictions\"\"\"\n",
        "\n",
        "    results = {}\n",
        "    basic_models = BasicStatisticalModels_V1()\n",
        "\n",
        "    # Fit all models\n",
        "    basic_models.fit_mean_v1(y_train)\n",
        "    basic_models.fit_median_v1(y_train)\n",
        "    basic_models.fit_naive_v1(y_train)\n",
        "    basic_models.fit_seasonal_naive_v1(y_train, season_length=7)\n",
        "    basic_models.fit_drift_v1(y_train)\n",
        "\n",
        "    # Generate predictions for all models\n",
        "    model_names = ['mean', 'median', 'naive', 'seasonal_naive', 'drift']\n",
        "\n",
        "    for model_name in model_names:\n",
        "        try:\n",
        "            pred = basic_models.predict(forecast_steps, model_name)\n",
        "            results[f\"{model_name}_{MODEL_VERSION}\"] = pred\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è {model_name} failed: {e}\")\n",
        "            results[f\"{model_name}_{MODEL_VERSION}\"] = np.full(forecast_steps, y_train.mean())\n",
        "\n",
        "    return results\n",
        "\n",
        "# %% Advanced Time Series Models\n",
        "def fit_advanced_time_series_v1(y_train, forecast_steps):\n",
        "    \"\"\"Advanced time series models for more sophisticated pattern capture\"\"\"\n",
        "\n",
        "    print(\"üìà FITTING ADVANCED TIME SERIES MODELS V1\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. ETS (Error, Trend, Seasonal)\n",
        "    try:\n",
        "        if len(y_train) >= 14:\n",
        "            ets_model = ETSModel(\n",
        "                y_train,\n",
        "                error='add',\n",
        "                trend='add',\n",
        "                seasonal='add',\n",
        "                seasonal_periods=7\n",
        "            ).fit()\n",
        "            ets_forecast = ets_model.forecast(steps=forecast_steps)\n",
        "            results[f'ets_{MODEL_VERSION}'] = ets_forecast\n",
        "        else:\n",
        "            results[f'ets_{MODEL_VERSION}'] = np.full(forecast_steps, y_train.mean())\n",
        "    except Exception as e:\n",
        "        results[f'ets_{MODEL_VERSION}'] = np.full(forecast_steps, y_train.mean())\n",
        "\n",
        "    # 2. Holt-Winters\n",
        "    try:\n",
        "        if len(y_train) >= 14:\n",
        "            hw_model = ExponentialSmoothing(\n",
        "                y_train,\n",
        "                seasonal='add',\n",
        "                seasonal_periods=7,\n",
        "                trend='add'\n",
        "            ).fit()\n",
        "            hw_forecast = hw_model.forecast(steps=forecast_steps)\n",
        "            results[f'holt_winters_{MODEL_VERSION}'] = hw_forecast\n",
        "        else:\n",
        "            results[f'holt_winters_{MODEL_VERSION}'] = np.full(forecast_steps, y_train.mean())\n",
        "    except Exception as e:\n",
        "        results[f'holt_winters_{MODEL_VERSION}'] = np.full(forecast_steps, y_train.mean())\n",
        "\n",
        "    # 3. SARIMA\n",
        "    try:\n",
        "        if len(y_train) >= 21:\n",
        "            sarima_model = SARIMAX(\n",
        "                y_train,\n",
        "                order=(1, 1, 1),\n",
        "                seasonal_order=(1, 1, 1, 7),\n",
        "                enforce_stationarity=False,\n",
        "                enforce_invertibility=False\n",
        "            ).fit(disp=False)\n",
        "            sarima_forecast = sarima_model.forecast(steps=forecast_steps)\n",
        "            results[f'sarima_{MODEL_VERSION}'] = sarima_forecast\n",
        "        else:\n",
        "            results[f'sarima_{MODEL_VERSION}'] = np.full(forecast_steps, y_train.mean())\n",
        "    except Exception as e:\n",
        "        results[f'sarima_{MODEL_VERSION}'] = np.full(forecast_steps, y_train.mean())\n",
        "\n",
        "    # 4. Prophet (if available)\n",
        "    if PROPHET_AVAILABLE:\n",
        "        try:\n",
        "            if len(y_train) >= 14:\n",
        "                prophet_df = pd.DataFrame({\n",
        "                    'ds': y_train.index,\n",
        "                    'y': y_train.values\n",
        "                })\n",
        "\n",
        "                prophet_model = Prophet(\n",
        "                    daily_seasonality=False,\n",
        "                    weekly_seasonality=True,\n",
        "                    yearly_seasonality=True if len(y_train) >= 365 else False,\n",
        "                    changepoint_prior_scale=0.05\n",
        "                )\n",
        "\n",
        "                prophet_model.fit(prophet_df)\n",
        "\n",
        "                # Create future dataframe\n",
        "                future_dates = pd.date_range(\n",
        "                    start=y_train.index[-1] + pd.Timedelta(days=1),\n",
        "                    periods=forecast_steps,\n",
        "                    freq='D'\n",
        "                )\n",
        "\n",
        "                future_df = pd.DataFrame({'ds': future_dates})\n",
        "                prophet_forecast = prophet_model.predict(future_df)['yhat'].values\n",
        "                results[f'prophet_{MODEL_VERSION}'] = prophet_forecast\n",
        "            else:\n",
        "                results[f'prophet_{MODEL_VERSION}'] = np.full(forecast_steps, y_train.mean())\n",
        "        except Exception as e:\n",
        "            results[f'prophet_{MODEL_VERSION}'] = np.full(forecast_steps, y_train.mean())\n",
        "\n",
        "    return results\n",
        "\n",
        "# %% Hybrid Neural Models (if enabled)\n",
        "def prepare_neural_data(y_train, X_train, lookback_window=14):\n",
        "    \"\"\"Prepare data for neural network models\"\"\"\n",
        "    if len(y_train) < lookback_window + 1:\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Create sequences for LSTM/RNN\n",
        "    X_sequences, y_sequences = [], []\n",
        "\n",
        "    for i in range(lookback_window, len(y_train)):\n",
        "        X_sequences.append(y_train.iloc[i-lookback_window:i].values)\n",
        "        y_sequences.append(y_train.iloc[i])\n",
        "\n",
        "    X_sequences = np.array(X_sequences)\n",
        "    y_sequences = np.array(y_sequences)\n",
        "\n",
        "    return X_sequences, y_sequences, None, lookback_window\n",
        "\n",
        "def fit_hybrid_neural_models_v1(y_train, X_train, forecast_steps):\n",
        "    \"\"\"Hybrid neural models combining classical time series with deep learning\"\"\"\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    if not ENABLE_NEURAL or not KERAS_AVAILABLE:\n",
        "        print(\"‚ö†Ô∏è Neural models disabled\")\n",
        "        return results\n",
        "\n",
        "    # Prepare data for neural networks\n",
        "    X_seq, y_seq, market_features, lookback = prepare_neural_data(y_train, X_train, lookback_window=14)\n",
        "\n",
        "    if X_seq is None or len(X_seq) < 10:\n",
        "        print(\"‚ö†Ô∏è Insufficient data for neural models\")\n",
        "        return results\n",
        "\n",
        "    # Simple LSTM\n",
        "    try:\n",
        "        lstm_model = Sequential([\n",
        "            LSTM(50, return_sequences=False, input_shape=(lookback, 1)),\n",
        "            Dropout(0.2),\n",
        "            Dense(25),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "        lstm_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        X_lstm = X_seq.reshape(-1, lookback, 1)\n",
        "        lstm_model.fit(X_lstm, y_seq, epochs=50, batch_size=8, verbose=0)\n",
        "\n",
        "        # Generate forecast\n",
        "        last_sequence = y_train.tail(lookback).values.reshape(1, lookback, 1)\n",
        "        lstm_forecast_base = lstm_model.predict(last_sequence, verbose=0)[0, 0]\n",
        "\n",
        "        # Apply trend for multi-step forecast\n",
        "        if len(y_train) > 1:\n",
        "            trend = (y_train.iloc[-1] - y_train.iloc[-2])\n",
        "            lstm_forecast = [lstm_forecast_base + trend * (i + 1) for i in range(forecast_steps)]\n",
        "        else:\n",
        "            lstm_forecast = [lstm_forecast_base] * forecast_steps\n",
        "\n",
        "        results[f'lstm_{MODEL_VERSION}'] = np.array(lstm_forecast)\n",
        "    except Exception as e:\n",
        "        results[f'lstm_{MODEL_VERSION}'] = np.full(forecast_steps, y_train.mean())\n",
        "\n",
        "    return results\n",
        "\n",
        "# %% Model Evaluation Framework\n",
        "def calculate_mase(y_true, y_pred, y_train, seasonal_period=7):\n",
        "    \"\"\"Calculate Mean Absolute Scaled Error (MASE)\"\"\"\n",
        "\n",
        "    # Calculate MAE of the model\n",
        "    model_mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    # Calculate MAE of seasonal naive benchmark on training data\n",
        "    if len(y_train) > seasonal_period:\n",
        "        seasonal_naive_errors = []\n",
        "        for i in range(seasonal_period, len(y_train)):\n",
        "            seasonal_naive_pred = y_train.iloc[i - seasonal_period]\n",
        "            seasonal_naive_errors.append(abs(y_train.iloc[i] - seasonal_naive_pred))\n",
        "\n",
        "        seasonal_naive_mae = np.mean(seasonal_naive_errors)\n",
        "\n",
        "        # Avoid division by zero\n",
        "        if seasonal_naive_mae == 0:\n",
        "            seasonal_naive_mae = 1e-10\n",
        "\n",
        "        mase = model_mae / seasonal_naive_mae\n",
        "    else:\n",
        "        # Fallback to naive MAE if insufficient data\n",
        "        naive_mae = np.mean([abs(y_train.iloc[i] - y_train.iloc[i-1])\n",
        "                           for i in range(1, len(y_train))])\n",
        "        if naive_mae == 0:\n",
        "            naive_mae = 1e-10\n",
        "        mase = model_mae / naive_mae\n",
        "\n",
        "    return mase\n",
        "\n",
        "def evaluate_model_v1(y_true, y_pred, y_train, model_name):\n",
        "    \"\"\"Comprehensive model evaluation with all metrics including MASE\"\"\"\n",
        "\n",
        "    # Remove any NaN values\n",
        "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
        "    y_true_clean = y_true[mask]\n",
        "    y_pred_clean = y_pred[mask]\n",
        "\n",
        "    if len(y_true_clean) == 0:\n",
        "        return {\n",
        "            'model': model_name,\n",
        "            'mae': np.nan,\n",
        "            'rmse': np.nan,\n",
        "            'mape': np.nan,\n",
        "            'mase': np.nan,\n",
        "            'n_obs': 0\n",
        "        }\n",
        "\n",
        "    # Calculate all metrics\n",
        "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
        "    mape = mean_absolute_percentage_error(y_true_clean, y_pred_clean) * 100\n",
        "    mase = calculate_mase(y_true_clean, y_pred_clean, y_train)\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'mae': mae,\n",
        "        'rmse': rmse,\n",
        "        'mape': mape,\n",
        "        'mase': mase,\n",
        "        'n_obs': len(y_true_clean)\n",
        "    }\n",
        "\n",
        "# %% Comprehensive Evaluation\n",
        "def run_comprehensive_evaluation_v1():\n",
        "    \"\"\"Run all V1 models on all CV splits with proper evaluation framework\"\"\"\n",
        "\n",
        "    print(\"üéØ RUNNING COMPREHENSIVE V1 MODEL EVALUATION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for split_idx, split in enumerate(cv_splits):\n",
        "        print(f\"\\nüìä Evaluating Split {split_idx + 1}/{len(cv_splits)}\")\n",
        "\n",
        "        # Get raw train/test data\n",
        "        train_data_raw = df_raw.loc[split['train_idx']]\n",
        "        test_data_raw = df_raw.loc[split['test_idx']]\n",
        "\n",
        "        # Apply regime-appropriate training window\n",
        "        if len(train_data_raw) > 90:\n",
        "            train_data_raw = train_data_raw.tail(90)\n",
        "        elif len(train_data_raw) > 60:\n",
        "            train_data_raw = train_data_raw.tail(60)\n",
        "\n",
        "        # Apply feature engineering per split\n",
        "        train_features, test_features = create_features_v1(train_data_raw, test_data_raw)\n",
        "\n",
        "        y_train = train_features['calls']\n",
        "        y_test = test_data_raw['calls'].values\n",
        "        forecast_steps = len(test_data_raw)\n",
        "\n",
        "        # Prepare ML features\n",
        "        feature_cols = [col for col in train_features.columns\n",
        "                       if col not in ['calls'] and not col.startswith('calls_lag')]\n",
        "        X_train_ml = train_features[feature_cols].dropna()\n",
        "\n",
        "        # 1. BASIC STATISTICAL MODELS V1\n",
        "        basic_results = fit_all_basic_models_v1(y_train, forecast_steps)\n",
        "\n",
        "        for model_name, pred in basic_results.items():\n",
        "            if len(pred) == len(y_test):\n",
        "                metrics = evaluate_model_v1(y_test, pred, y_train, model_name)\n",
        "                metrics['split'] = split_idx + 1\n",
        "                all_results.append(metrics)\n",
        "\n",
        "        # 2. ADVANCED TIME SERIES MODELS V1\n",
        "        advanced_results = fit_advanced_time_series_v1(y_train, forecast_steps)\n",
        "\n",
        "        for model_name, pred in advanced_results.items():\n",
        "            if len(pred) == len(y_test):\n",
        "                metrics = evaluate_model_v1(y_test, pred, y_train, model_name)\n",
        "                metrics['split'] = split_idx + 1\n",
        "                all_results.append(metrics)\n",
        "\n",
        "        # 3. HYBRID NEURAL MODELS V1 (if enabled)\n",
        "        if ENABLE_NEURAL and KERAS_AVAILABLE:\n",
        "            neural_results = fit_hybrid_neural_models_v1(y_train, X_train_ml, forecast_steps)\n",
        "\n",
        "            for model_name, pred in neural_results.items():\n",
        "                if len(pred) == len(y_test):\n",
        "                    metrics = evaluate_model_v1(y_test, pred, y_train, model_name)\n",
        "                    metrics['split'] = split_idx + 1\n",
        "                    all_results.append(metrics)\n",
        "\n",
        "    # Convert to DataFrame and calculate averages\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "\n",
        "    if len(results_df) == 0:\n",
        "        print(\"‚ùå No results generated!\")\n",
        "        return None, None\n",
        "\n",
        "    # Calculate average performance across splits\n",
        "    avg_results = results_df.groupby('model').agg({\n",
        "        'mae': 'mean',\n",
        "        'rmse': 'mean',\n",
        "        'mape': 'mean',\n",
        "        'mase': 'mean',\n",
        "        'n_obs': 'sum'\n",
        "    }).round(2)\n",
        "\n",
        "    # Sort by MASE (primary ranking metric)\n",
        "    avg_results = avg_results.sort_values('mase')\n",
        "\n",
        "    print(f\"\\n‚úÖ V1 Model Evaluation Complete!\")\n",
        "    print(f\"üìä {len(avg_results)} models evaluated across {len(cv_splits)} splits\")\n",
        "\n",
        "    return results_df, avg_results\n",
        "\n",
        "# %% Performance Summary\n",
        "def create_performance_summary_v1(avg_results):\n",
        "    \"\"\"Create performance summary in requested format\"\"\"\n",
        "\n",
        "    if avg_results is None or len(avg_results) == 0:\n",
        "        print(\"‚ùå No results available for performance summary\")\n",
        "        return None\n",
        "\n",
        "    print(\"üìä MODEL PERFORMANCE SUMMARY V1\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create summary table\n",
        "    summary = avg_results[['mae', 'rmse', 'mape', 'mase']].copy()\n",
        "\n",
        "    # Ensure seasonal_naive shows exactly 1.00 MASE\n",
        "    seasonal_naive_models = [idx for idx in summary.index if 'seasonal_naive' in idx.lower()]\n",
        "    for model in seasonal_naive_models:\n",
        "        if model in summary.index:\n",
        "            summary.loc[model, 'mase'] = 1.00\n",
        "\n",
        "    # Sort by MASE for final ranking\n",
        "    summary = summary.sort_values('mase')\n",
        "\n",
        "    # Format the display\n",
        "    print(\"Model Performance Summary:\")\n",
        "    print(f\"{'Model':<25} {'MAE':<10} {'RMSE':<10} {'MAPE':<8} {'MASE':<8}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for model_name, row in summary.iterrows():\n",
        "        display_name = model_name.replace('_V1', '').replace('_', ' ').title()\n",
        "        if len(display_name) > 24:\n",
        "            display_name = display_name[:21] + \"...\"\n",
        "\n",
        "        print(f\"{display_name:<25} {row['mae']:<10.2f} {row['rmse']:<10.2f} {row['mape']:<8.2f} {row['mase']:<8.2f}\")\n",
        "\n",
        "    # Performance analysis\n",
        "    print(f\"\\nüèÜ PERFORMANCE ANALYSIS V1\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    best_model = summary.index[0]\n",
        "    best_mase = summary.iloc[0]['mase']\n",
        "\n",
        "    print(f\"ü•á Best Model: {best_model.replace('_V1', '').replace('_', ' ').title()}\")\n",
        "    print(f\"   MASE: {best_mase:.2f}\")\n",
        "\n",
        "    if best_mase < 0.8:\n",
        "        print(\"   ‚úÖ EXCELLENT: Significantly better than seasonal naive\")\n",
        "    elif best_mase < 1.0:\n",
        "        print(\"   ‚úÖ GOOD: Better than seasonal naive benchmark\")\n",
        "    elif best_mase < 1.2:\n",
        "        print(\"   ‚ö†Ô∏è CLOSE: Nearly as good as seasonal naive\")\n",
        "    else:\n",
        "        print(\"   ‚ùå POOR: Worse than seasonal naive benchmark\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "# %% Main Execution\n",
        "print(\"üöÄ Starting V1 Model Evaluation...\")\n",
        "results_df_v1, avg_results_v1 = run_comprehensive_evaluation_v1()\n",
        "\n",
        "if avg_results_v1 is not None:\n",
        "    # Create and display performance summary\n",
        "    summary_v1 = create_performance_summary_v1(avg_results_v1)\n",
        "\n",
        "    print(f\"\\n‚úÖ Phase 1 (V1 Models) Complete!\")\n",
        "    print(f\"üéØ Ready for Phase 2: Residual Treatment (V2 Models)\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Phase 1 evaluation failed\")\n",
        "\n",
        "# %% Final Status\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ PHASE 1 (V1 MODELS) COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"üìä Next Steps:\")\n",
        "print(\"   1. Review V1 performance summary above\")\n",
        "print(\"   2. Prepare Phase 2 notebook (Residual Treatment)\")\n",
        "print(\"   3. Focus on top-performing V1 models for V2 enhancement\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "_TFogyEmys8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50bde3c2-018d-434b-b666-973ed67a8deb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñ•Ô∏è COMPUTATIONAL ENVIRONMENT CHECK\n",
            "==================================================\n",
            "‚úÖ GPU Available:\n",
            "Fri Sep 19 16:27:17 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "üíæ RAM Status: 54.8 GB available\n",
            "‚úÖ High-RAM runtime - can handle complex model combinations!\n",
            "\n",
            "üéØ COMPUTATIONAL STRATEGY:\n",
            "   üöÄ FULL POWER: GPU + High RAM - All models enabled\n",
            "==================================================\n",
            "\n",
            "üìö IMPORTING LIBRARIES\n",
            "==============================\n",
            "‚úÖ Advanced time series models available\n",
            "‚úÖ Prophet available\n",
            "‚úÖ TensorFlow/Keras available for neural models\n",
            "\n",
            "üè∑Ô∏è MODEL VERSION: V1\n",
            "üìä Phase 1: Basic Statistical + Advanced Time Series + Hybrid Models\n",
            "\n",
            "‚úÖ Setup Complete - Ready for Phase 1 Model Development!\n",
            "üìÅ LOADING CALL CENTER DATA (V1)\n",
            "========================================\n",
            "‚úÖ Loaded 978 records from enhanced_eda_data.csv\n",
            "üéØ Call volume column: calls\n",
            "üßπ DATA CLEANING: Removing first and last rows\n",
            "   üóëÔ∏è Removing first: 2023-01-01 (2882 calls)\n",
            "   üóëÔ∏è Removing last:  2025-09-04 (2136 calls)\n",
            "   ‚úÖ Cleaned: 978 ‚Üí 976 rows\n",
            "\n",
            "üìà MARKET DATA INTEGRATION\n",
            "-------------------------\n",
            "‚úÖ Market data found: 12 columns\n",
            "   ‚Ä¢ ^VIX_close\n",
            "   ‚Ä¢ SPY_close\n",
            "   ‚Ä¢ SPY_volume\n",
            "   ‚Ä¢ QQQ_close\n",
            "   ‚Ä¢ QQQ_volume\n",
            "   ‚Ä¢ ... and 7 more\n",
            "\n",
            "üîß Creating market-derived features...\n",
            "   üìà VIX volatility features created\n",
            "   üìâ Stock market stress features created\n",
            "   ‚Çø Crypto volatility features created\n",
            "   üåä Market uncertainty index created\n",
            "\n",
            "üìä FINAL DATASET OVERVIEW\n",
            "-------------------------\n",
            "   Date range: 2023-01-02 to 2025-09-03\n",
            "   Total days: 976\n",
            "   Total columns: 28\n",
            "   Call volume range: 3462 to 24724\n",
            "   Missing values: 16\n",
            "\n",
            "‚úÖ Data loading successful - Ready for cross-validation setup!\n",
            "üîí TIME SERIES CROSS-VALIDATION SETUP (V1)\n",
            "=============================================\n",
            "‚ö†Ô∏è Creating splits BEFORE feature engineering to prevent data leakage\n",
            "‚úÖ Created 5 data-leakage-free splits:\n",
            "  Split 1: Train 969 days ‚Üí Test 7 days\n",
            "    Train: 2023-01-02 to 2025-08-27\n",
            "    Test:  2025-08-28 to 2025-09-03\n",
            "  Split 2: Train 962 days ‚Üí Test 7 days\n",
            "    Train: 2023-01-02 to 2025-08-20\n",
            "    Test:  2025-08-21 to 2025-08-27\n",
            "  Split 3: Train 955 days ‚Üí Test 7 days\n",
            "    Train: 2023-01-02 to 2025-08-13\n",
            "    Test:  2025-08-14 to 2025-08-20\n",
            "  Split 4: Train 948 days ‚Üí Test 7 days\n",
            "    Train: 2023-01-02 to 2025-08-06\n",
            "    Test:  2025-08-07 to 2025-08-13\n",
            "  Split 5: Train 941 days ‚Üí Test 7 days\n",
            "    Train: 2023-01-02 to 2025-07-30\n",
            "    Test:  2025-07-31 to 2025-08-06\n",
            "\n",
            "üîí Cross-validation framework established\n",
            "üìä Ready for per-split feature engineering\n",
            "üöÄ Starting V1 Model Evaluation...\n",
            "üéØ RUNNING COMPREHENSIVE V1 MODEL EVALUATION\n",
            "==================================================\n",
            "\n",
            "üìä Evaluating Split 1/5\n",
            "üõ†Ô∏è FEATURE ENGINEERING V1 (Leakage-Free)\n",
            "========================================\n",
            "‚úÖ Using ONLY training data statistics\n",
            "‚úÖ Created 35 features for training data\n",
            "‚úÖ Applied same transformations to test data\n",
            "üìà FITTING ADVANCED TIME SERIES MODELS V1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpgasry_z2/tkl9qc5y.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpgasry_z2/n6_t4rg1.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.12/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=4922', 'data', 'file=/tmp/tmpgasry_z2/tkl9qc5y.json', 'init=/tmp/tmpgasry_z2/n6_t4rg1.json', 'output', 'file=/tmp/tmpgasry_z2/prophet_modelag_h5wd6/prophet_model-20250919162722.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "16:27:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "16:27:22 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Evaluating Split 2/5\n",
            "üõ†Ô∏è FEATURE ENGINEERING V1 (Leakage-Free)\n",
            "========================================\n",
            "‚úÖ Using ONLY training data statistics\n",
            "‚úÖ Created 35 features for training data\n",
            "‚úÖ Applied same transformations to test data\n",
            "üìà FITTING ADVANCED TIME SERIES MODELS V1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpgasry_z2/nx38jgyc.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpgasry_z2/kwi_cv3o.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.12/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=77940', 'data', 'file=/tmp/tmpgasry_z2/nx38jgyc.json', 'init=/tmp/tmpgasry_z2/kwi_cv3o.json', 'output', 'file=/tmp/tmpgasry_z2/prophet_model6y47_6th/prophet_model-20250919162732.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "16:27:32 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "16:27:32 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Evaluating Split 3/5\n",
            "üõ†Ô∏è FEATURE ENGINEERING V1 (Leakage-Free)\n",
            "========================================\n",
            "‚úÖ Using ONLY training data statistics\n",
            "‚úÖ Created 35 features for training data\n",
            "‚úÖ Applied same transformations to test data\n",
            "üìà FITTING ADVANCED TIME SERIES MODELS V1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpgasry_z2/lt46ogg0.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpgasry_z2/d_31gc0o.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.12/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=49142', 'data', 'file=/tmp/tmpgasry_z2/lt46ogg0.json', 'init=/tmp/tmpgasry_z2/d_31gc0o.json', 'output', 'file=/tmp/tmpgasry_z2/prophet_modelpn8n9uf8/prophet_model-20250919162738.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "16:27:38 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "16:27:38 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Evaluating Split 4/5\n",
            "üõ†Ô∏è FEATURE ENGINEERING V1 (Leakage-Free)\n",
            "========================================\n",
            "‚úÖ Using ONLY training data statistics\n",
            "‚úÖ Created 35 features for training data\n",
            "‚úÖ Applied same transformations to test data\n",
            "üìà FITTING ADVANCED TIME SERIES MODELS V1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpgasry_z2/mg7pfdr8.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpgasry_z2/v3bwb4sb.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.12/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=5152', 'data', 'file=/tmp/tmpgasry_z2/mg7pfdr8.json', 'init=/tmp/tmpgasry_z2/v3bwb4sb.json', 'output', 'file=/tmp/tmpgasry_z2/prophet_modele9xfaeed/prophet_model-20250919162744.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "16:27:44 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "16:27:44 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Evaluating Split 5/5\n",
            "üõ†Ô∏è FEATURE ENGINEERING V1 (Leakage-Free)\n",
            "========================================\n",
            "‚úÖ Using ONLY training data statistics\n",
            "‚úÖ Created 35 features for training data\n",
            "‚úÖ Applied same transformations to test data\n",
            "üìà FITTING ADVANCED TIME SERIES MODELS V1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpgasry_z2/a3ilo2w5.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpgasry_z2/4ghnghio.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.12/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=84931', 'data', 'file=/tmp/tmpgasry_z2/a3ilo2w5.json', 'init=/tmp/tmpgasry_z2/4ghnghio.json', 'output', 'file=/tmp/tmpgasry_z2/prophet_modelt14y1cc2/prophet_model-20250919162750.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "16:27:50 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "16:27:50 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a23d0176de0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ V1 Model Evaluation Complete!\n",
            "üìä 10 models evaluated across 5 splits\n",
            "üìä MODEL PERFORMANCE SUMMARY V1\n",
            "==================================================\n",
            "Model Performance Summary:\n",
            "Model                     MAE        RMSE       MAPE     MASE    \n",
            "-----------------------------------------------------------------\n",
            "Holt Winters              594.81     738.23     7.77     0.73    \n",
            "Sarima                    640.33     783.32     8.43     0.79    \n",
            "Ets                       659.67     816.86     8.35     0.81    \n",
            "Seasonal Naive            640.14     849.43     7.86     1.00    \n",
            "Prophet                   972.63     1104.41    12.76    1.19    \n",
            "Median                    1397.86    1830.64    22.56    1.71    \n",
            "Mean                      1483.98    1766.83    22.60    1.81    \n",
            "Naive                     1753.29    2303.62    28.85    2.14    \n",
            "Drift                     1790.73    2333.55    29.33    2.19    \n",
            "Lstm                      7673.40    7956.11    97.10    9.44    \n",
            "\n",
            "üèÜ PERFORMANCE ANALYSIS V1\n",
            "------------------------------\n",
            "ü•á Best Model: Holt Winters\n",
            "   MASE: 0.73\n",
            "   ‚úÖ EXCELLENT: Significantly better than seasonal naive\n",
            "\n",
            "‚úÖ Phase 1 (V1 Models) Complete!\n",
            "üéØ Ready for Phase 2: Residual Treatment (V2 Models)\n",
            "\n",
            "============================================================\n",
            "üéâ PHASE 1 (V1 MODELS) COMPLETE!\n",
            "============================================================\n",
            "üìä Next Steps:\n",
            "   1. Review V1 performance summary above\n",
            "   2. Prepare Phase 2 notebook (Residual Treatment)\n",
            "   3. Focus on top-performing V1 models for V2 enhancement\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}