{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/FRESH_START.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Market Data Could Help:**\n",
        "\n",
        "VIX spikes â†’ Customer anxiety â†’ More service calls\n",
        "Stock crashes â†’ Portfolio concerns â†’ Support calls surge\n",
        "Crypto volatility â†’ Trading platform issues â†’ Call spikes\n",
        "Dollar movements â†’ International customer impacts\n",
        "Gold rallies â†’ Economic uncertainty â†’ Increased activity"
      ],
      "metadata": {
        "id": "8PoEcb46jvPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tommorow Ideas - have a weekly / daily toggle"
      ],
      "metadata": {
        "id": "HfWS6hkSl0ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "k-mM9CdiXekA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Example: Move a tensor to the GPU\n",
        "x = torch.randn(10, 10).to(device)\n",
        "\n",
        "# Example: Move a model to the GPU\n",
        "# model = YourModel().to(device)"
      ],
      "metadata": {
        "id": "A2-hwYzFXeiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 1: Import all required libraries and configure GPU\n",
        "\"\"\"\n",
        "\n",
        "!pip install catboost optuna\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Core libraries\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import pickle\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import holidays\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Gradient Boosting\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Prophet\n",
        "from prophet import Prophet\n",
        "\n",
        "# Hyperparameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import optuna\n",
        "\n",
        "# Configure GPU\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"âœ… GPU Available: {len(gpus)} GPU(s) detected\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"âš ï¸ GPU initialization error: {e}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU detected, using CPU\")\n",
        "\n",
        "# Set seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "qBnMvAT_XegS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call Center Volume Forecasting - Fresh Start Approach\n",
        "# This notebook provides a systematic approach to forecasting call center volumes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Statistical and time series\n",
        "from scipy import stats\n",
        "from scipy.stats import jarque_bera, shapiro\n",
        "from scipy.signal import find_peaks\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "\n",
        "# Prophet\n",
        "try:\n",
        "    from prophet import Prophet\n",
        "    PROPHET_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Prophet not available - install with: pip install prophet\")\n",
        "    PROPHET_AVAILABLE = False\n",
        "\n",
        "# ML models\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"ðŸ“ž Call Center Forecasting Notebook - Ready to Roll!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: REGIME CHANGE IMPOSSIBILITY PROOF - Independent Validation\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nðŸ”¬ STEP 1: VALIDATING FORECAST IMPOSSIBILITY\")\n",
        "print(\"=\" * 60)\n",
        "print(\"ðŸ“Š Objective: Prove that frequent regime changes make forecasting impossible\")\n",
        "print(\"ðŸŽ¯ Hypothesis: '37 changes in 36 days = impossible forecasting'\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def load_data_for_impossibility_analysis():\n",
        "    \"\"\"Load data specifically for regime change analysis with market data integration\"\"\"\n",
        "    try:\n",
        "        print(\"ðŸ“ Loading call center data...\")\n",
        "        df = pd.read_csv('enhanced_eda_data.csv', index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Auto-detect volume column\n",
        "        volume_cols = ['calls', 'Calls', 'call_volume', 'Call_Volume', 'volume', 'Volume']\n",
        "        volume_col = None\n",
        "\n",
        "        for col in volume_cols:\n",
        "            if col in df.columns:\n",
        "                volume_col = col\n",
        "                break\n",
        "\n",
        "        if volume_col is None:\n",
        "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "            volume_col = numeric_cols[0] if len(numeric_cols) > 0 else df.columns[0]\n",
        "\n",
        "        df = df.rename(columns={volume_col: 'calls'})\n",
        "        df = df[['calls']].sort_index()\n",
        "\n",
        "        # TRIM FIRST AND LAST ROWS (bad call volume data)\n",
        "        print(\"ðŸ§¹ Trimming first and last rows (bad call volume data)...\")\n",
        "        if len(df) > 2:\n",
        "            df = df.iloc[1:-1]  # Remove first and last rows\n",
        "            print(f\"   Trimmed to {len(df)} rows\")\n",
        "\n",
        "        print(f\"âœ… Loaded {len(df)} days of call data ({df.index.min()} to {df.index.max()})\")\n",
        "\n",
        "        # LOAD MARKET DATA\n",
        "        print(\"\\nðŸ“ˆ Loading market data...\")\n",
        "        market_tickers = {\n",
        "            '^VIX': ['^VIX_close'],\n",
        "            'SPY': ['SPY_close', 'SPY_volume'],\n",
        "            'QQQ': ['QQQ_close', 'QQQ_volume'],\n",
        "            'DX-Y.NYB': ['DX-Y.NYB_close'],\n",
        "            'GC=F': ['GC=F_close', 'GC=F_volume'],\n",
        "            'BTC-USD': ['BTC-USD_close', 'BTC-USD_volume'],\n",
        "            'ETH-USD': ['ETH-USD_close', 'ETH-USD_volume']\n",
        "        }\n",
        "\n",
        "        market_data = {}\n",
        "\n",
        "        for ticker, columns in market_tickers.items():\n",
        "            try:\n",
        "                # Try different possible filenames\n",
        "                possible_files = [\n",
        "                    f'{ticker}_data.csv',\n",
        "                    f'{ticker.replace(\"^\", \"\").replace(\"=\", \"_\").replace(\"-\", \"_\")}_data.csv',\n",
        "                    f'market_data_{ticker.replace(\"^\", \"\").replace(\"=\", \"_\").replace(\"-\", \"_\")}.csv'\n",
        "                ]\n",
        "\n",
        "                ticker_df = None\n",
        "                for filename in possible_files:\n",
        "                    try:\n",
        "                        ticker_df = pd.read_csv(filename, index_col='Date', parse_dates=True)\n",
        "                        print(f\"   âœ… Loaded {ticker} from {filename}\")\n",
        "                        break\n",
        "                    except FileNotFoundError:\n",
        "                        continue\n",
        "\n",
        "                if ticker_df is not None:\n",
        "                    # Trim first and last rows for market data too\n",
        "                    if len(ticker_df) > 2:\n",
        "                        ticker_df = ticker_df.iloc[1:-1]\n",
        "\n",
        "                    # Select only the columns we want\n",
        "                    available_cols = [col for col in columns if col in ticker_df.columns]\n",
        "                    if available_cols:\n",
        "                        market_data[ticker] = ticker_df[available_cols]\n",
        "                        print(f\"     Columns: {available_cols}\")\n",
        "                    else:\n",
        "                        print(f\"   âš ï¸ No matching columns found for {ticker}\")\n",
        "                else:\n",
        "                    print(f\"   âŒ Could not load {ticker} data\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   âŒ Error loading {ticker}: {e}\")\n",
        "\n",
        "        # Combine market data with call data\n",
        "        if market_data:\n",
        "            print(f\"\\nðŸ”— Integrating {len(market_data)} market datasets...\")\n",
        "\n",
        "            for ticker, ticker_df in market_data.items():\n",
        "                # Align dates and merge\n",
        "                aligned_data = ticker_df.reindex(df.index, method='ffill')  # Forward fill for weekends\n",
        "                df = pd.concat([df, aligned_data], axis=1)\n",
        "\n",
        "            print(f\"   âœ… Combined dataset: {df.shape[1]} columns, {len(df)} rows\")\n",
        "            print(f\"   ðŸ“Š Market columns added: {[col for ticker_df in market_data.values() for col in ticker_df.columns]}\")\n",
        "        else:\n",
        "            print(\"   âš ï¸ No market data loaded - proceeding with call data only\")\n",
        "\n",
        "        # Basic data quality check\n",
        "        print(f\"\\nðŸ“Š Final Dataset Overview:\")\n",
        "        print(f\"   Date range: {df.index.min()} to {df.index.max()}\")\n",
        "        print(f\"   Total days: {len(df)}\")\n",
        "        print(f\"   Columns: {list(df.columns)}\")\n",
        "        print(f\"   Call volume missing: {df['calls'].isna().sum()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def detect_regime_changes_proof(df, method='multiple', window=7):\n",
        "    \"\"\"Advanced regime change detection for impossibility proof with market data awareness\"\"\"\n",
        "\n",
        "    print(f\"\\nðŸ” DETECTING REGIME CHANGES (Call Center + Market)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    df_analysis = df.copy()\n",
        "\n",
        "    # CALL CENTER REGIME CHANGES\n",
        "    df_analysis['rolling_mean'] = df_analysis['calls'].rolling(window).mean()\n",
        "    df_analysis['rolling_std'] = df_analysis['calls'].rolling(window).std()\n",
        "    df_analysis['rolling_var'] = df_analysis['calls'].rolling(window).var()\n",
        "\n",
        "    # Detect significant changes in call patterns\n",
        "    df_analysis['mean_change'] = df_analysis['rolling_mean'].diff().abs()\n",
        "    df_analysis['std_change'] = df_analysis['rolling_std'].diff().abs()\n",
        "    df_analysis['var_change'] = df_analysis['rolling_var'].diff().abs()\n",
        "\n",
        "    # Set thresholds (adaptive based on data)\n",
        "    mean_threshold = df_analysis['mean_change'].quantile(0.95)  # Top 5% of changes\n",
        "    std_threshold = df_analysis['std_change'].quantile(0.95)\n",
        "    var_threshold = df_analysis['var_change'].quantile(0.95)\n",
        "\n",
        "    # CUSUM Detection for calls\n",
        "    target = df_analysis['calls'].mean()\n",
        "    df_analysis['cusum'] = (df_analysis['calls'] - target).cumsum()\n",
        "    df_analysis['cusum_change'] = df_analysis['cusum'].diff().abs()\n",
        "    cusum_threshold = df_analysis['cusum_change'].quantile(0.95)\n",
        "\n",
        "    # MARKET REGIME CHANGES (if market data available)\n",
        "    market_regime_signals = []\n",
        "\n",
        "    # VIX regime changes (volatility spikes)\n",
        "    if '^VIX_close' in df_analysis.columns:\n",
        "        vix_changes = df_analysis['^VIX_close'].diff().abs()\n",
        "        vix_threshold = vix_changes.quantile(0.95)\n",
        "        vix_regime = vix_changes > vix_threshold\n",
        "        market_regime_signals.append(vix_regime)\n",
        "        print(f\"   ðŸ“Š VIX regime changes: {vix_regime.sum()}\")\n",
        "\n",
        "    # Stock market regime changes\n",
        "    if 'SPY_close' in df_analysis.columns:\n",
        "        spy_returns = df_analysis['SPY_close'].pct_change()\n",
        "        spy_vol_changes = spy_returns.rolling(window).std().diff().abs()\n",
        "        spy_threshold = spy_vol_changes.quantile(0.95)\n",
        "        spy_regime = spy_vol_changes > spy_threshold\n",
        "        market_regime_signals.append(spy_regime)\n",
        "        print(f\"   ðŸ“ˆ SPY regime changes: {spy_regime.sum()}\")\n",
        "\n",
        "    # Crypto regime changes\n",
        "    if 'BTC-USD_close' in df_analysis.columns:\n",
        "        btc_returns = df_analysis['BTC-USD_close'].pct_change()\n",
        "        btc_vol_changes = btc_returns.rolling(window).std().diff().abs()\n",
        "        btc_threshold = btc_vol_changes.quantile(0.95)\n",
        "        btc_regime = btc_vol_changes > btc_threshold\n",
        "        market_regime_signals.append(btc_regime)\n",
        "        print(f\"   â‚¿ BTC regime changes: {btc_regime.sum()}\")\n",
        "\n",
        "    # Combine call center and market regime detection\n",
        "    call_regime_mask = (\n",
        "        (df_analysis['mean_change'] > mean_threshold) |\n",
        "        (df_analysis['std_change'] > std_threshold) |\n",
        "        (df_analysis['var_change'] > var_threshold) |\n",
        "        (df_analysis['cusum_change'] > cusum_threshold)\n",
        "    )\n",
        "\n",
        "    # Add market regime signals\n",
        "    if market_regime_signals:\n",
        "        market_regime_mask = pd.concat(market_regime_signals, axis=1).any(axis=1)\n",
        "        print(f\"   ðŸŒ Market-driven regime changes: {market_regime_mask.sum()}\")\n",
        "\n",
        "        # Combined regime detection\n",
        "        combined_regime_mask = call_regime_mask | market_regime_mask\n",
        "        df_analysis['regime_source'] = 'none'\n",
        "        df_analysis.loc[call_regime_mask & ~market_regime_mask, 'regime_source'] = 'call_only'\n",
        "        df_analysis.loc[~call_regime_mask & market_regime_mask, 'regime_source'] = 'market_only'\n",
        "        df_analysis.loc[call_regime_mask & market_regime_mask, 'regime_source'] = 'both'\n",
        "\n",
        "        regime_mask = combined_regime_mask\n",
        "\n",
        "        # Analyze regime change sources\n",
        "        source_counts = df_analysis['regime_source'].value_counts()\n",
        "        print(f\"   ðŸ“Š Regime change attribution:\")\n",
        "        for source, count in source_counts.items():\n",
        "            if source != 'none':\n",
        "                print(f\"     {source}: {count} changes\")\n",
        "    else:\n",
        "        regime_mask = call_regime_mask\n",
        "        print(f\"   ðŸ“Š No market data - using call center changes only\")\n",
        "\n",
        "    regime_mask = regime_mask.fillna(False)\n",
        "    regime_changes = df_analysis[regime_mask].copy()\n",
        "\n",
        "    print(f\"\\nðŸ“Š DETECTION RESULTS:\")\n",
        "    print(f\"   Total regime changes detected: {len(regime_changes)}\")\n",
        "    print(f\"   Average changes per day: {len(regime_changes) / len(df):.3f}\")\n",
        "\n",
        "    # Focus on recent period (last 36 days)\n",
        "    if len(df) >= 36:\n",
        "        recent_data = df.tail(36)\n",
        "        recent_changes = regime_changes[regime_changes.index.isin(recent_data.index)]\n",
        "        print(f\"   ðŸš¨ CRITICAL: Recent changes (last 36 days): {len(recent_changes)}\")\n",
        "        print(f\"   ðŸš¨ CRITICAL: Recent change frequency: {len(recent_changes)/36:.3f} per day\")\n",
        "\n",
        "        # Analyze recent change sources if market data available\n",
        "        if market_regime_signals and len(recent_changes) > 0:\n",
        "            recent_sources = recent_changes['regime_source'].value_counts()\n",
        "            print(f\"   ðŸ“Š Recent change sources:\")\n",
        "            for source, count in recent_sources.items():\n",
        "                print(f\"     {source}: {count} ({count/len(recent_changes)*100:.1f}%)\")\n",
        "\n",
        "        if len(recent_changes) >= 30:\n",
        "            print(f\"   âš ï¸ WARNING: EXTREME INSTABILITY DETECTED!\")\n",
        "            if market_regime_signals:\n",
        "                market_driven = recent_changes['regime_source'].isin(['market_only', 'both']).sum()\n",
        "                print(f\"   ðŸŒ Market-influenced changes: {market_driven}/{len(recent_changes)} ({market_driven/len(recent_changes)*100:.1f}%)\")\n",
        "\n",
        "    return regime_changes, df_analysis\n",
        "\n",
        "def create_impossibility_proof_visualization(df, regime_changes):\n",
        "    \"\"\"Create the definitive proof that forecasting is impossible\"\"\"\n",
        "\n",
        "    print(f\"\\nðŸ“Š CREATING IMPOSSIBILITY PROOF\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('EMPIRICAL PROOF: Why Forecasting Is Impossible', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Recent period with regime changes\n",
        "    recent_data = df.tail(50) if len(df) >= 50 else df\n",
        "    recent_regime = regime_changes[regime_changes.index.isin(recent_data.index)]\n",
        "\n",
        "    axes[0,0].plot(recent_data.index, recent_data['calls'], 'b-', alpha=0.7, linewidth=2, label='Call Volume')\n",
        "    axes[0,0].scatter(recent_regime.index, recent_regime['calls'], color='red', s=60, alpha=0.9,\n",
        "               label=f'Regime Changes ({len(recent_regime)})', zorder=5)\n",
        "    axes[0,0].set_title(f'Last {len(recent_data)} Days: {len(recent_regime)} Structural Breaks')\n",
        "    axes[0,0].set_ylabel('Call Volume')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add impossibility annotation\n",
        "    if len(recent_regime) > len(recent_data) * 0.8:\n",
        "        axes[0,0].text(0.05, 0.95, 'CHAOS ZONE\\nNo Stable Patterns',\n",
        "                      transform=axes[0,0].transAxes, fontsize=12, fontweight='bold',\n",
        "                      color='red', va='top', bbox=dict(boxstyle=\"round,pad=0.3\",\n",
        "                      facecolor=\"yellow\", alpha=0.8))\n",
        "\n",
        "    # 2. Impossibility threshold analysis\n",
        "    if len(df) >= 36:\n",
        "        days = np.arange(1, min(51, len(df)))  # Up to 50 days\n",
        "        recent_changes_count = []\n",
        "\n",
        "        for day in days:\n",
        "            end_date = df.index[-1]\n",
        "            start_date = end_date - pd.Timedelta(days=day-1)\n",
        "            period_changes = len(regime_changes[\n",
        "                (regime_changes.index >= start_date) &\n",
        "                (regime_changes.index <= end_date)\n",
        "            ])\n",
        "            recent_changes_count.append(period_changes)\n",
        "\n",
        "        axes[0,1].plot(days, recent_changes_count, 'ro-', linewidth=3, markersize=6, label='Actual Changes')\n",
        "\n",
        "        # Theoretical impossibility thresholds\n",
        "        stable_threshold = days * 0.1      # 1 change per 10 days = stable\n",
        "        challenging_threshold = days * 0.3  # 1 change per 3 days = challenging\n",
        "        impossible_threshold = days * 0.8   # 4+ changes per 5 days = impossible\n",
        "\n",
        "        axes[0,1].plot(days, stable_threshold, 'g--', alpha=0.7, linewidth=2, label='Stable (10% change rate)')\n",
        "        axes[0,1].plot(days, challenging_threshold, 'y--', alpha=0.7, linewidth=2, label='Challenging (30% rate)')\n",
        "        axes[0,1].plot(days, impossible_threshold, 'r--', alpha=0.7, linewidth=2, label='Impossible (80% rate)')\n",
        "\n",
        "        # Highlight key finding\n",
        "        if len(days) >= 36:\n",
        "            changes_36_days = recent_changes_count[35]  # 36th day (0-indexed)\n",
        "            impossible_36 = impossible_threshold[35]\n",
        "\n",
        "            axes[0,1].axhline(y=changes_36_days, color='red', linewidth=4, alpha=0.8)\n",
        "            axes[0,1].axvline(x=36, color='blue', linewidth=2, alpha=0.5, linestyle=':')\n",
        "\n",
        "            if changes_36_days > impossible_36:\n",
        "                status = \"IMPOSSIBLE\"\n",
        "                color = \"red\"\n",
        "            elif changes_36_days > challenging_threshold[35]:\n",
        "                status = \"EXTREMELY DIFFICULT\"\n",
        "                color = \"orange\"\n",
        "            else:\n",
        "                status = \"FEASIBLE\"\n",
        "                color = \"green\"\n",
        "\n",
        "            axes[0,1].text(36, changes_36_days + 2, f'{changes_36_days} changes\\nin 36 days\\n= {status}',\n",
        "                          fontsize=11, fontweight='bold', ha='center',\n",
        "                          bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.3))\n",
        "\n",
        "        axes[0,1].set_xlabel('Time Window (Days)')\n",
        "        axes[0,1].set_ylabel('Number of Regime Changes')\n",
        "        axes[0,1].set_title('Forecasting Impossibility Threshold Analysis')\n",
        "        axes[0,1].legend(fontsize=9)\n",
        "        axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Forecast accuracy simulation\n",
        "    print(\"   ðŸ”¬ Running forecast accuracy simulation...\")\n",
        "\n",
        "    horizons = [1, 2, 3, 5, 7]\n",
        "    naive_errors = []\n",
        "\n",
        "    # Quick forecast simulation\n",
        "    min_train = 10\n",
        "    for horizon in horizons:\n",
        "        errors = []\n",
        "        for i in range(min_train, min(len(df) - horizon, min_train + 50)):  # Limit iterations for speed\n",
        "            train_data = df.iloc[max(0, i-14):i]['calls']  # Last 14 days only\n",
        "            test_data = df.iloc[i:i+horizon]['calls']\n",
        "\n",
        "            if len(train_data) > 0 and len(test_data) == horizon:\n",
        "                # Simple naive forecast\n",
        "                pred = [train_data.iloc[-1]] * horizon\n",
        "                mape = mean_absolute_percentage_error(test_data, pred)\n",
        "                errors.append(mape)\n",
        "\n",
        "        naive_errors.append(np.mean(errors) if errors else 100)\n",
        "\n",
        "    axes[1,0].plot(horizons, naive_errors, 'ro-', linewidth=3, markersize=8, label='Naive Forecast MAPE')\n",
        "    axes[1,0].axhline(y=50, color='orange', linestyle='--', alpha=0.7, label='50% Error (Poor)')\n",
        "    axes[1,0].axhline(y=100, color='red', linestyle='--', alpha=0.7, label='100% Error (Random)')\n",
        "    axes[1,0].set_xlabel('Forecast Horizon (Days)')\n",
        "    axes[1,0].set_ylabel('MAPE (%)')\n",
        "    axes[1,0].set_title('Forecast Accuracy Collapse')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    axes[1,0].set_ylim(0, min(150, max(naive_errors) + 10))\n",
        "\n",
        "    # 4. Change frequency heatmap\n",
        "    change_indicator = pd.Series(0, index=df.index)\n",
        "    change_indicator[regime_changes.index] = 1\n",
        "\n",
        "    # Create weekly change frequency\n",
        "    weekly_changes = change_indicator.groupby(change_indicator.index.to_period('W')).sum()\n",
        "\n",
        "    if len(weekly_changes) > 4:\n",
        "        # Reshape for heatmap (take last 20 weeks max)\n",
        "        recent_weeks = weekly_changes.tail(20)\n",
        "        weeks_per_row = 5\n",
        "        n_rows = len(recent_weeks) // weeks_per_row\n",
        "\n",
        "        if n_rows > 0:\n",
        "            heatmap_data = recent_weeks.iloc[:n_rows * weeks_per_row].values.reshape(n_rows, weeks_per_row)\n",
        "\n",
        "            im = axes[1,1].imshow(heatmap_data, cmap='Reds', aspect='auto')\n",
        "            axes[1,1].set_title('Weekly Regime Change Intensity\\n(Red = More Changes)')\n",
        "            axes[1,1].set_xlabel('Week (within row)')\n",
        "            axes[1,1].set_ylabel('Time Period')\n",
        "\n",
        "            # Add colorbar\n",
        "            plt.colorbar(im, ax=axes[1,1], label='Changes per Week')\n",
        "        else:\n",
        "            axes[1,1].text(0.5, 0.5, 'Insufficient data\\nfor heatmap',\n",
        "                          transform=axes[1,1].transAxes, ha='center', va='center')\n",
        "            axes[1,1].set_title('Change Frequency Analysis')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return recent_changes_count if len(df) >= 36 else None\n",
        "\n",
        "def deliver_scientific_verdict(df, regime_changes):\n",
        "    \"\"\"Deliver the final scientific verdict on forecast impossibility with market insights\"\"\"\n",
        "\n",
        "    print(f\"\\nâš–ï¸ SCIENTIFIC VERDICT\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if len(df) >= 36:\n",
        "        recent_36_changes = len(regime_changes.tail(36))\n",
        "        change_rate = recent_36_changes / 36\n",
        "\n",
        "        print(f\"ðŸ“Š EMPIRICAL EVIDENCE:\")\n",
        "        print(f\"   â€¢ Regime changes in last 36 days: {recent_36_changes}\")\n",
        "        print(f\"   â€¢ Change rate: {change_rate:.3f} changes per day\")\n",
        "        print(f\"   â€¢ Average time between changes: {1/change_rate:.1f} days\") if change_rate > 0 else print(f\"   â€¢ No recent changes detected\")\n",
        "\n",
        "        # MARKET CORRELATION ANALYSIS (if market data available)\n",
        "        market_columns = [col for col in df.columns if col not in ['calls']]\n",
        "        if market_columns:\n",
        "            print(f\"\\nðŸŒ MARKET CORRELATION ANALYSIS:\")\n",
        "\n",
        "            # Analyze correlation between market volatility and call spikes\n",
        "            if '^VIX_close' in df.columns:\n",
        "                call_returns = df['calls'].pct_change().abs()\n",
        "                vix_correlation = df['^VIX_close'].corr(call_returns)\n",
        "                print(f\"   ðŸ“Š VIX vs Call Volatility correlation: {vix_correlation:.3f}\")\n",
        "\n",
        "                # VIX spike days vs high call days\n",
        "                vix_spikes = df['^VIX_close'] > df['^VIX_close'].quantile(0.9)\n",
        "                call_spikes = df['calls'] > df['calls'].quantile(0.9)\n",
        "                spike_overlap = (vix_spikes & call_spikes).sum()\n",
        "                total_call_spikes = call_spikes.sum()\n",
        "                if total_call_spikes > 0:\n",
        "                    overlap_pct = spike_overlap / total_call_spikes * 100\n",
        "                    print(f\"   ðŸ”¥ High-VIX days with call spikes: {spike_overlap}/{total_call_spikes} ({overlap_pct:.1f}%)\")\n",
        "\n",
        "            # Analyze if market regime changes predict call center changes\n",
        "            if 'regime_source' in regime_changes.columns:\n",
        "                market_driven = regime_changes['regime_source'].isin(['market_only', 'both']).sum()\n",
        "                total_changes = len(regime_changes)\n",
        "                if total_changes > 0:\n",
        "                    market_pct = market_driven / total_changes * 100\n",
        "                    print(f\"   ðŸ“ˆ Market-influenced regime changes: {market_driven}/{total_changes} ({market_pct:.1f}%)\")\n",
        "\n",
        "                    if market_pct > 50:\n",
        "                        print(f\"   ðŸ’¡ INSIGHT: Majority of instability is market-driven!\")\n",
        "                    elif market_pct > 25:\n",
        "                        print(f\"   ðŸ’¡ INSIGHT: Significant market influence on call patterns\")\n",
        "\n",
        "            # Economic uncertainty impact\n",
        "            if 'market_uncertainty_index' in df.columns:\n",
        "                uncertainty_call_corr = df['market_uncertainty_index'].corr(df['calls'])\n",
        "                print(f\"   ðŸŒŠ Market uncertainty vs calls correlation: {uncertainty_call_corr:.3f}\")\n",
        "\n",
        "        # Scientific thresholds based on forecasting literature\n",
        "        if change_rate > 0.8:  # More than 4 changes per 5 days\n",
        "            verdict = \"MATHEMATICALLY IMPOSSIBLE\"\n",
        "            confidence = \"99.9%\"\n",
        "            color = \"ðŸ”´\"\n",
        "            recommendation = \"ABANDON FORECASTING - FOCUS ON REAL-TIME ADAPTATION\"\n",
        "        elif change_rate > 0.5:  # More than 1 change per 2 days\n",
        "            verdict = \"EXTREMELY DIFFICULT\"\n",
        "            confidence = \"95%\"\n",
        "            color = \"ðŸŸ \"\n",
        "            recommendation = \"USE ONLY 1-DAY FORECASTS WITH HOURLY UPDATES\"\n",
        "        elif change_rate > 0.2:  # More than 1 change per 5 days\n",
        "            verdict = \"CHALLENGING BUT POSSIBLE\"\n",
        "            confidence = \"80%\"\n",
        "            color = \"ðŸŸ¡\"\n",
        "            recommendation = \"SHORT-TERM FORECASTS ONLY (1-3 DAYS)\"\n",
        "        else:\n",
        "            verdict = \"FEASIBLE\"\n",
        "            confidence = \"High\"\n",
        "            color = \"ðŸŸ¢\"\n",
        "            recommendation = \"STANDARD FORECASTING APPROACHES CAN WORK\"\n",
        "\n",
        "        print(f\"\\n{color} FINAL CONCLUSION:\")\n",
        "        print(f\"   FORECASTING IS {verdict}\")\n",
        "        print(f\"   Confidence Level: {confidence}\")\n",
        "        print(f\"   Scientific Basis: Change frequency exceeds model adaptation capacity\")\n",
        "\n",
        "        print(f\"\\nðŸ’¡ STRATEGIC RECOMMENDATION:\")\n",
        "        print(f\"   {recommendation}\")\n",
        "\n",
        "        # Enhanced recommendations based on market data\n",
        "        if market_columns:\n",
        "            print(f\"\\nðŸŽ¯ MARKET-INFORMED STRATEGY:\")\n",
        "            if '^VIX_close' in df.columns:\n",
        "                print(f\"   â€¢ Monitor VIX for early warning of call volume spikes\")\n",
        "                print(f\"   â€¢ Implement VIX-based staffing alerts (VIX >25 = prepare for volume)\")\n",
        "            if 'SPY_close' in df.columns:\n",
        "                print(f\"   â€¢ Track market drops for customer service demand surges\")\n",
        "                print(f\"   â€¢ Set up automated alerts for market stress indicators\")\n",
        "            if 'BTC-USD_close' in df.columns:\n",
        "                print(f\"   â€¢ Monitor crypto volatility for trading platform support load\")\n",
        "\n",
        "            print(f\"   â€¢ Use market regime changes as leading indicators\")\n",
        "            print(f\"   â€¢ Implement market-based capacity planning\")\n",
        "\n",
        "        if change_rate > 0.5:\n",
        "            print(f\"\\nðŸ§ª SCIENTIFIC REASONING:\")\n",
        "            print(f\"   â€¢ Forecasting models require stable patterns lasting longer than forecast horizon\")\n",
        "            print(f\"   â€¢ With changes every {1/change_rate:.1f} days, patterns expire before use\")\n",
        "            print(f\"   â€¢ Model training time exceeds pattern stability duration\")\n",
        "            print(f\"   â€¢ Signal-to-noise ratio insufficient for meaningful prediction\")\n",
        "\n",
        "            if market_columns:\n",
        "                print(f\"   â€¢ External market forces create unpredictable regime changes\")\n",
        "                print(f\"   â€¢ Market volatility cascades into operational instability\")\n",
        "\n",
        "            print(f\"\\nâš¡ BUSINESS IMPLICATIONS:\")\n",
        "            print(f\"   â€¢ Traditional accuracy metrics become meaningless\")\n",
        "            print(f\"   â€¢ Focus must shift from prediction to adaptation speed\")\n",
        "            print(f\"   â€¢ Operational flexibility more valuable than forecast precision\")\n",
        "            print(f\"   â€¢ Real-time monitoring and rapid response capabilities essential\")\n",
        "\n",
        "            if market_columns:\n",
        "                print(f\"   â€¢ Market data provides better leading indicators than historical calls\")\n",
        "                print(f\"   â€¢ Implement market-driven operational triggers\")\n",
        "\n",
        "    return change_rate if len(df) >= 36 else 0\n",
        "\n",
        "# Execute the impossibility proof analysis\n",
        "print(\"ðŸš€ Starting Impossibility Proof Analysis...\")\n",
        "\n",
        "# Load data\n",
        "df_proof = load_data_for_impossibility_analysis()\n",
        "\n",
        "if df_proof is not None:\n",
        "    # Detect regime changes\n",
        "    regime_changes_proof, df_analysis_proof = detect_regime_changes_proof(df_proof)\n",
        "\n",
        "    # Create proof visualization\n",
        "    recent_changes_count = create_impossibility_proof_visualization(df_proof, regime_changes_proof)\n",
        "\n",
        "    # Deliver scientific verdict\n",
        "    change_rate_proof = deliver_scientific_verdict(df_proof, regime_changes_proof)\n",
        "\n",
        "    print(f\"\\nâœ… IMPOSSIBILITY PROOF COMPLETE\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"ðŸŽ¯ Evidence gathered - proceeding with adaptive modeling approach...\")\n",
        "    print(\"ðŸ“ˆ The following analysis will focus on rapid adaptation rather than accuracy\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Cannot proceed with proof analysis - check data file\")\n",
        "    change_rate_proof = 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PROCEEDING TO FULL ADAPTIVE FORECASTING ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 2: Data Loading and Initial Setup\n",
        "# ============================================================================\n",
        "\n",
        "def load_and_prepare_data(file_path='enhanced_eda_data.csv', volume_col=None, include_market_data=True):\n",
        "    \"\"\"\n",
        "    Load call center data with market data integration and data cleaning\n",
        "\n",
        "    Enhanced to include market data and trim problematic first/last rows\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        print(f\"ðŸ“ Loading call center data from {file_path}...\")\n",
        "\n",
        "        # Load call center data with Date as index\n",
        "        df = pd.read_csv(file_path, index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Auto-detect the volume column if not specified\n",
        "        if volume_col is None:\n",
        "            possible_cols = ['calls', 'Calls', 'call_volume', 'Call_Volume', 'volume', 'Volume',\n",
        "                           'call_count', 'Call_Count', 'total_calls', 'Total_Calls']\n",
        "\n",
        "            volume_col = None\n",
        "            for col in possible_cols:\n",
        "                if col in df.columns:\n",
        "                    volume_col = col\n",
        "                    break\n",
        "\n",
        "            if volume_col is None:\n",
        "                numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "                if len(numeric_cols) > 0:\n",
        "                    volume_col = numeric_cols[0]\n",
        "                    print(f\"ðŸ” Auto-detected volume column: '{volume_col}'\")\n",
        "                else:\n",
        "                    raise ValueError(\"No numeric columns found for call volume\")\n",
        "            else:\n",
        "                print(f\"ðŸ” Found volume column: '{volume_col}'\")\n",
        "\n",
        "        # Rename to standardized 'calls' column\n",
        "        if volume_col != 'calls':\n",
        "            df = df.rename(columns={volume_col: 'calls'})\n",
        "\n",
        "        # Keep only the calls column initially\n",
        "        df = df[['calls']].sort_index()\n",
        "\n",
        "        # TRIM FIRST AND LAST ROWS (problematic call volume data)\n",
        "        print(\"ðŸ§¹ DATA CLEANING: Removing problematic first and last rows\")\n",
        "        print(\"   âš ï¸  As requested - first and last days have bad call volume data\")\n",
        "        original_len = len(df)\n",
        "        if len(df) > 2:\n",
        "            first_row_date = df.index[0].strftime('%Y-%m-%d')\n",
        "            last_row_date = df.index[-1].strftime('%Y-%m-%d')\n",
        "            first_row_calls = df['calls'].iloc[0]\n",
        "            last_row_calls = df['calls'].iloc[-1]\n",
        "\n",
        "            print(f\"   ðŸ—‘ï¸  Removing first row: {first_row_date} ({first_row_calls:.0f} calls)\")\n",
        "            print(f\"   ðŸ—‘ï¸  Removing last row:  {last_row_date} ({last_row_calls:.0f} calls)\")\n",
        "\n",
        "            df = df.iloc[1:-1]  # Remove first and last rows\n",
        "\n",
        "            print(f\"   âœ… Cleaned dataset: {original_len} â†’ {len(df)} rows ({original_len-len(df)} removed)\")\n",
        "        else:\n",
        "            print(\"   âš ï¸  Dataset too small to trim (â‰¤2 rows)\")\n",
        "\n",
        "        print(f\"   ðŸ“… Final date range: {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        # INTEGRATE MARKET DATA\n",
        "        if include_market_data:\n",
        "            print(f\"\\nðŸ“ˆ Loading and integrating market data...\")\n",
        "\n",
        "            market_tickers = {\n",
        "                '^VIX': ['^VIX_close'],\n",
        "                'SPY': ['SPY_close', 'SPY_volume'],\n",
        "                'QQQ': ['QQQ_close', 'QQQ_volume'],\n",
        "                'DX-Y.NYB': ['DX-Y.NYB_close'],\n",
        "                'GC=F': ['GC=F_close', 'GC=F_volume'],\n",
        "                'BTC-USD': ['BTC-USD_close', 'BTC-USD_volume'],\n",
        "                'ETH-USD': ['ETH-USD_close', 'ETH-USD_volume']\n",
        "            }\n",
        "\n",
        "            market_data_loaded = {}\n",
        "\n",
        "            for ticker, columns in market_tickers.items():\n",
        "                try:\n",
        "                    # Try different possible filenames\n",
        "                    possible_files = [\n",
        "                        f'{ticker}_data.csv',\n",
        "                        f'{ticker.replace(\"^\", \"\").replace(\"=\", \"_\").replace(\"-\", \"_\")}_data.csv',\n",
        "                        f'market_data_{ticker.replace(\"^\", \"\").replace(\"=\", \"_\").replace(\"-\", \"_\")}.csv',\n",
        "                        f'{ticker.replace(\"^\", \"VIX_\").replace(\"=F\", \"\").replace(\"-\", \"_\")}.csv'\n",
        "                    ]\n",
        "\n",
        "                    ticker_df = None\n",
        "                    for filename in possible_files:\n",
        "                        try:\n",
        "                            ticker_df = pd.read_csv(filename, index_col='Date', parse_dates=True)\n",
        "                            print(f\"   âœ… Loaded {ticker} from {filename}\")\n",
        "                            break\n",
        "                        except FileNotFoundError:\n",
        "                            continue\n",
        "\n",
        "                    if ticker_df is not None:\n",
        "                        # Trim first and last rows for consistency with call data\n",
        "                        original_market_len = len(ticker_df)\n",
        "                        if len(ticker_df) > 2:\n",
        "                            ticker_df = ticker_df.iloc[1:-1]\n",
        "                            print(f\"     ðŸ§¹ Trimmed {ticker} from {original_market_len} to {len(ticker_df)} rows\")\n",
        "\n",
        "                        # Select only the columns we want\n",
        "                        available_cols = [col for col in columns if col in ticker_df.columns]\n",
        "                        if available_cols:\n",
        "                            market_data_loaded[ticker] = ticker_df[available_cols]\n",
        "                            print(f\"     ðŸ“Š Columns: {available_cols}\")\n",
        "                        else:\n",
        "                            print(f\"   âš ï¸ No matching columns found for {ticker}\")\n",
        "                    else:\n",
        "                        print(f\"   âŒ Could not find data files for {ticker}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"   âŒ Error loading {ticker}: {e}\")\n",
        "\n",
        "            # Merge market data with call data\n",
        "            if market_data_loaded:\n",
        "                print(f\"\\nðŸ”— Integrating {len(market_data_loaded)} market datasets...\")\n",
        "\n",
        "                market_columns_added = []\n",
        "                for ticker, ticker_df in market_data_loaded.items():\n",
        "                    # Align dates (forward fill for weekends/holidays)\n",
        "                    aligned_data = ticker_df.reindex(df.index, method='ffill')\n",
        "\n",
        "                    # Merge with main dataset\n",
        "                    df = pd.concat([df, aligned_data], axis=1)\n",
        "                    market_columns_added.extend(aligned_data.columns.tolist())\n",
        "\n",
        "                print(f\"   âœ… Market integration complete!\")\n",
        "                print(f\"   ðŸ“Š Added columns: {market_columns_added}\")\n",
        "                print(f\"   ðŸ“ Final dataset: {df.shape[1]} columns Ã— {len(df)} rows\")\n",
        "\n",
        "                # Create market-derived features\n",
        "                print(f\"\\nðŸ”§ Creating market-based features...\")\n",
        "\n",
        "                # VIX-based features (fear/volatility)\n",
        "                if '^VIX_close' in df.columns:\n",
        "                    df['vix_high'] = (df['^VIX_close'] > df['^VIX_close'].quantile(0.8)).astype(int)\n",
        "                    df['vix_spike'] = (df['^VIX_close'].pct_change() > 0.2).astype(int)\n",
        "                    print(\"   ðŸ“ˆ VIX volatility features created\")\n",
        "\n",
        "                # Stock market stress features\n",
        "                if 'SPY_close' in df.columns:\n",
        "                    df['spy_returns'] = df['SPY_close'].pct_change()\n",
        "                    df['market_stress'] = (df['spy_returns'] < -0.02).astype(int)  # 2%+ daily drop\n",
        "                    df['spy_volatility'] = df['spy_returns'].rolling(7).std()\n",
        "                    print(\"   ðŸ“‰ Stock market stress features created\")\n",
        "\n",
        "                # Crypto volatility features\n",
        "                if 'BTC-USD_close' in df.columns:\n",
        "                    df['btc_returns'] = df['BTC-USD_close'].pct_change()\n",
        "                    df['crypto_volatility'] = df['btc_returns'].rolling(7).std()\n",
        "                    df['btc_extreme_move'] = (abs(df['btc_returns']) > 0.1).astype(int)  # 10%+ moves\n",
        "                    print(\"   â‚¿ Crypto volatility features created\")\n",
        "\n",
        "                # Economic uncertainty composite\n",
        "                uncertainty_features = []\n",
        "                if '^VIX_close' in df.columns:\n",
        "                    uncertainty_features.append(df['^VIX_close'])\n",
        "                if 'spy_volatility' in df.columns:\n",
        "                    uncertainty_features.append(df['spy_volatility'] * 100)  # Scale to match VIX\n",
        "                if 'crypto_volatility' in df.columns:\n",
        "                    uncertainty_features.append(df['crypto_volatility'] * 100)\n",
        "\n",
        "                if uncertainty_features:\n",
        "                    uncertainty_matrix = pd.concat(uncertainty_features, axis=1)\n",
        "                    df['market_uncertainty_index'] = uncertainty_matrix.mean(axis=1)\n",
        "                    print(\"   ðŸŒŠ Market uncertainty composite index created\")\n",
        "\n",
        "            else:\n",
        "                print(\"   âš ï¸ No market data files found - proceeding with call data only\")\n",
        "                print(\"   ðŸ’¡ Expected files: VIX_data.csv, SPY_data.csv, etc.\")\n",
        "\n",
        "        # Final data quality checks\n",
        "        print(f\"\\nðŸ“Š Data Overview:\")\n",
        "        print(f\"   Date range: {df.index.min()} to {df.index.max()}\")\n",
        "        print(f\"   Total days: {len(df)}\")\n",
        "        print(f\"   Total columns: {len(df.columns)}\")\n",
        "        print(f\"   Call volume missing: {df['calls'].isna().sum()}\")\n",
        "\n",
        "        if len(df.columns) > 1:\n",
        "            market_missing = df.drop('calls', axis=1).isna().sum().sum()\n",
        "            print(f\"   Market data missing: {market_missing} values\")\n",
        "\n",
        "        # Check for any obvious data issues\n",
        "        if df['calls'].min() < 0:\n",
        "            print(\"âš ï¸  Warning: Negative call volumes detected\")\n",
        "\n",
        "        if df['calls'].isna().sum() > 0:\n",
        "            print(f\"âš ï¸  Warning: {df['calls'].isna().sum()} missing values in call volume\")\n",
        "\n",
        "        # Check for duplicated dates\n",
        "        if df.index.duplicated().any():\n",
        "            print(\"âš ï¸  Warning: Duplicate dates found - removing duplicates\")\n",
        "            df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ File '{file_path}' not found!\")\n",
        "        print(\"ðŸ’¡ Make sure the file is in the same directory as this notebook\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading data: {e}\")\n",
        "        print(\"ðŸ’¡ Check your file format and column names\")\n",
        "        return None\n",
        "\n",
        "# Load your actual data with market integration\n",
        "df = load_and_prepare_data(\n",
        "    file_path='enhanced_eda_data.csv',\n",
        "    volume_col=None,  # Will auto-detect\n",
        "    include_market_data=True  # Set to False if no market data available\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 3: Exploratory Data Analysis\n",
        "# ============================================================================\n",
        "\n",
        "def comprehensive_eda(df):\n",
        "    \"\"\"Comprehensive exploratory data analysis for call center data\"\"\"\n",
        "\n",
        "    print(\"ðŸ” EXPLORATORY DATA ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 1. Time Series Plot\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Main time series\n",
        "    axes[0,0].plot(df.index, df['calls'], alpha=0.7, linewidth=0.8)\n",
        "    axes[0,0].set_title('Call Volume Over Time')\n",
        "    axes[0,0].set_ylabel('Number of Calls')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Distribution\n",
        "    axes[0,1].hist(df['calls'], bins=50, alpha=0.7, edgecolor='black')\n",
        "    axes[0,1].set_title('Call Volume Distribution')\n",
        "    axes[0,1].set_xlabel('Number of Calls')\n",
        "    axes[0,1].set_ylabel('Frequency')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Day of week pattern\n",
        "    df_dow = df.copy()\n",
        "    df_dow['dow'] = df_dow.index.dayofweek\n",
        "    dow_means = df_dow.groupby('dow')['calls'].mean()\n",
        "    dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "    axes[1,0].bar(range(7), dow_means.values, alpha=0.7)\n",
        "    axes[1,0].set_xticks(range(7))\n",
        "    axes[1,0].set_xticklabels(dow_names)\n",
        "    axes[1,0].set_title('Average Calls by Day of Week')\n",
        "    axes[1,0].set_ylabel('Average Calls')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Monthly pattern\n",
        "    df_month = df.copy()\n",
        "    df_month['month'] = df_month.index.month\n",
        "    month_means = df_month.groupby('month')['calls'].mean()\n",
        "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "    axes[1,1].bar(range(1, 13), month_means.values, alpha=0.7)\n",
        "    axes[1,1].set_xticks(range(1, 13))\n",
        "    axes[1,1].set_xticklabels(month_names, rotation=45)\n",
        "    axes[1,1].set_title('Average Calls by Month')\n",
        "    axes[1,1].set_ylabel('Average Calls')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Statistical Summary\n",
        "    print(\"\\nðŸ“ˆ Statistical Summary:\")\n",
        "    print(f\"   Mean: {df['calls'].mean():.1f}\")\n",
        "    print(f\"   Median: {df['calls'].median():.1f}\")\n",
        "    print(f\"   Std Dev: {df['calls'].std():.1f}\")\n",
        "    print(f\"   CV: {df['calls'].std()/df['calls'].mean():.3f}\")\n",
        "    print(f\"   Skewness: {stats.skew(df['calls']):.3f}\")\n",
        "    print(f\"   Kurtosis: {stats.kurtosis(df['calls']):.3f}\")\n",
        "\n",
        "    # 3. Stationarity Test\n",
        "    print(\"\\nðŸ”¬ Stationarity Tests:\")\n",
        "    adf_result = adfuller(df['calls'])\n",
        "    print(f\"   ADF Statistic: {adf_result[0]:.4f}\")\n",
        "    print(f\"   p-value: {adf_result[1]:.4f}\")\n",
        "    print(f\"   Critical Values: {adf_result[4]}\")\n",
        "\n",
        "    if adf_result[1] <= 0.05:\n",
        "        print(\"   âœ… Series appears stationary\")\n",
        "    else:\n",
        "        print(\"   âŒ Series appears non-stationary\")\n",
        "\n",
        "    # 4. Seasonal Decomposition\n",
        "    print(\"\\nðŸ”„ Seasonal Decomposition:\")\n",
        "    decomposition = seasonal_decompose(df['calls'], model='additive', period=7)\n",
        "\n",
        "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
        "    decomposition.observed.plot(ax=axes[0], title='Original')\n",
        "    decomposition.trend.plot(ax=axes[1], title='Trend')\n",
        "    decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
        "    decomposition.resid.plot(ax=axes[3], title='Residual')\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 5. Autocorrelation Analysis\n",
        "    print(\"\\nðŸ“Š Autocorrelation Analysis:\")\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    plot_acf(df['calls'], ax=axes[0], lags=40)\n",
        "    plot_pacf(df['calls'], ax=axes[1], lags=40)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 6. Outlier Analysis\n",
        "    print(\"\\nðŸš¨ Outlier Analysis:\")\n",
        "    Q1 = df['calls'].quantile(0.25)\n",
        "    Q3 = df['calls'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = df[(df['calls'] < lower_bound) | (df['calls'] > upper_bound)]\n",
        "    print(f\"   Outliers detected: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
        "    print(f\"   Outlier range: {outliers['calls'].min():.0f} - {outliers['calls'].max():.0f}\")\n",
        "\n",
        "    if len(outliers) > 0:\n",
        "        print(f\"   Top 5 outlier dates:\")\n",
        "        for date, row in outliers.nlargest(5, 'calls').iterrows():\n",
        "            print(f\"     {date.strftime('%Y-%m-%d')}: {row['calls']:.0f} calls\")\n",
        "\n",
        "    return decomposition, outliers\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 3.5: Regime Change Analysis (Added for Structural Break Detection)\n",
        "# ============================================================================\n",
        "\n",
        "def detect_regime_changes(df, window=7, threshold=2.0):\n",
        "    \"\"\"\n",
        "    Detect potential regime changes in call volume data\n",
        "\n",
        "    Parameters:\n",
        "    - window: Rolling window for calculating statistics\n",
        "    - threshold: Standard deviations for change detection\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ðŸ”„ REGIME CHANGE ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Calculate rolling statistics\n",
        "    df_regime = df.copy()\n",
        "    df_regime['rolling_mean'] = df_regime['calls'].rolling(window).mean()\n",
        "    df_regime['rolling_std'] = df_regime['calls'].rolling(window).std()\n",
        "\n",
        "    # Detect significant changes in mean\n",
        "    df_regime['mean_change'] = df_regime['rolling_mean'].diff().abs()\n",
        "    df_regime['std_change'] = df_regime['rolling_std'].diff().abs()\n",
        "\n",
        "    # Flag potential regime changes\n",
        "    mean_threshold = df_regime['mean_change'].std() * threshold\n",
        "    std_threshold = df_regime['std_change'].std() * threshold\n",
        "\n",
        "    regime_changes = df_regime[\n",
        "        (df_regime['mean_change'] > mean_threshold) |\n",
        "        (df_regime['std_change'] > std_threshold)\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"ðŸ“Š Detected {len(regime_changes)} potential regime changes\")\n",
        "\n",
        "    if len(regime_changes) > 0:\n",
        "        # Focus on recent changes (last 60 days)\n",
        "        recent_changes = regime_changes.tail(60)\n",
        "        print(f\"ðŸ“… Recent changes (last 60 days): {len(recent_changes)}\")\n",
        "\n",
        "        # Plot regime changes\n",
        "        fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "        # Main time series with change points\n",
        "        axes[0].plot(df.index, df['calls'], alpha=0.7, linewidth=1, label='Call Volume')\n",
        "        axes[0].scatter(regime_changes.index, regime_changes['calls'],\n",
        "                       color='red', s=50, alpha=0.7, label=f'Regime Changes ({len(regime_changes)})')\n",
        "        axes[0].set_title('Call Volume with Detected Regime Changes')\n",
        "        axes[0].set_ylabel('Number of Calls')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Recent period zoom\n",
        "        if len(df) > 60:\n",
        "            recent_data = df.tail(60)\n",
        "            recent_regime = regime_changes.tail(60)\n",
        "\n",
        "            axes[1].plot(recent_data.index, recent_data['calls'], alpha=0.7, linewidth=2, label='Call Volume')\n",
        "            if len(recent_regime) > 0:\n",
        "                axes[1].scatter(recent_regime.index, recent_regime['calls'],\n",
        "                               color='red', s=60, alpha=0.8, label=f'Recent Changes ({len(recent_regime)})')\n",
        "            axes[1].set_title('Recent Period (Last 60 Days) - Regime Changes')\n",
        "            axes[1].set_ylabel('Number of Calls')\n",
        "            axes[1].legend()\n",
        "            axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print recent change dates\n",
        "        if len(recent_changes) > 0:\n",
        "            print(f\"\\nðŸ“‹ Most Recent Regime Changes:\")\n",
        "            for date, row in recent_changes.tail(10).iterrows():\n",
        "                print(f\"   {date.strftime('%Y-%m-%d')}: {row['calls']:.0f} calls \"\n",
        "                      f\"(mean Î”: {row['mean_change']:.1f}, std Î”: {row['std_change']:.1f})\")\n",
        "\n",
        "    return regime_changes\n",
        "\n",
        "# Analyze regime changes in your data\n",
        "regime_changes = detect_regime_changes(df, window=7, threshold=1.5)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 3.6: Adaptive Modeling Strategy for High-Change Data\n",
        "# ============================================================================\n",
        "\n",
        "def recommend_adaptive_strategy(regime_changes, df):\n",
        "    \"\"\"Recommend modeling strategy based on regime change frequency\"\"\"\n",
        "\n",
        "    print(f\"\\nðŸ’¡ ADAPTIVE MODELING RECOMMENDATIONS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    recent_changes = len(regime_changes.tail(36))  # Last 36 days\n",
        "    change_frequency = recent_changes / 36\n",
        "\n",
        "    print(f\"ðŸ“Š Regime Change Frequency: {recent_changes} changes in 36 days ({change_frequency:.2f}/day)\")\n",
        "\n",
        "    if change_frequency > 0.5:  # More than 1 change every 2 days\n",
        "        print(\"ðŸš¨ EXTREMELY HIGH VOLATILITY - Traditional forecasting not recommended\")\n",
        "        print(\"\\nâœ… RECOMMENDED APPROACHES:\")\n",
        "        print(\"   1. ONLINE LEARNING models (update with each new observation)\")\n",
        "        print(\"   2. VERY SHORT training windows (3-7 days max)\")\n",
        "        print(\"   3. ENSEMBLE of simple models with different lookback periods\")\n",
        "        print(\"   4. MOVING WINDOW forecasts (retrain daily)\")\n",
        "        print(\"   5. REGIME-SWITCHING models\")\n",
        "\n",
        "        print(\"\\nâŒ AVOID:\")\n",
        "        print(\"   â€¢ SARIMA/Prophet with long training periods\")\n",
        "        print(\"   â€¢ Complex seasonal patterns (too unstable)\")\n",
        "        print(\"   â€¢ Any model assuming stationarity\")\n",
        "\n",
        "    elif change_frequency > 0.2:  # 1 change every 5 days\n",
        "        print(\"âš ï¸ HIGH VOLATILITY - Need adaptive approaches\")\n",
        "        print(\"\\nâœ… RECOMMENDED APPROACHES:\")\n",
        "        print(\"   1. SHORT training windows (7-14 days)\")\n",
        "        print(\"   2. WEIGHTED recent observations more heavily\")\n",
        "        print(\"   3. ENSEMBLE methods with model combination\")\n",
        "        print(\"   4. EXPONENTIAL SMOOTHING with high alpha\")\n",
        "\n",
        "    else:\n",
        "        print(\"âœ… MODERATE VOLATILITY - Standard approaches may work\")\n",
        "        print(\"   â€¢ Use normal forecasting approaches but monitor closely\")\n",
        "        print(\"   â€¢ Consider shorter cross-validation windows\")\n",
        "\n",
        "    print(f\"\\nðŸ”§ IMMEDIATE ACTIONS:\")\n",
        "    print(\"   1. Reduce training window to 7-14 days maximum\")\n",
        "    print(\"   2. Implement daily model retraining\")\n",
        "    print(\"   3. Use simple, adaptive models (exponential smoothing)\")\n",
        "    print(\"   4. Focus on 1-3 day forecasts only\")\n",
        "    print(\"   5. Set up change point detection alerts\")\n",
        "\n",
        "# Get recommendations based on your data\n",
        "recommend_adaptive_strategy(regime_changes, df)\n",
        "\n",
        "# Run EDA\n",
        "decomposition, outliers = comprehensive_eda(df)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 5: Time Series Cross-Validation Framework (BEFORE Feature Engineering)\n",
        "# ============================================================================\n",
        "\n",
        "def create_time_series_splits_early(df, n_splits=5, test_size=7, gap=0):\n",
        "    \"\"\"\n",
        "    Create time series cross-validation splits BEFORE feature engineering\n",
        "    This prevents data leakage by ensuring no future information in features\n",
        "\n",
        "    Parameters:\n",
        "    - n_splits: Number of splits\n",
        "    - test_size: Size of test set in days\n",
        "    - gap: Gap between train and test (to prevent data leakage)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ðŸ”’ PREVENTING DATA LEAKAGE\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"âš ï¸  Creating splits BEFORE feature engineering to prevent future data leakage\")\n",
        "\n",
        "    splits = []\n",
        "    total_size = len(df)\n",
        "\n",
        "    for i in range(n_splits):\n",
        "        # Calculate split points\n",
        "        test_end = total_size - i * test_size\n",
        "        test_start = test_end - test_size\n",
        "        train_end = test_start - gap\n",
        "\n",
        "        if train_end < test_size:  # Need minimum training size\n",
        "            break\n",
        "\n",
        "        train_idx = df.index[:train_end]\n",
        "        test_idx = df.index[test_start:test_end]\n",
        "\n",
        "        splits.append({\n",
        "            'train_idx': train_idx,\n",
        "            'test_idx': test_idx,\n",
        "            'train_size': len(train_idx),\n",
        "            'test_size': len(test_idx),\n",
        "            'split_date': test_idx[0] if len(test_idx) > 0 else None\n",
        "        })\n",
        "\n",
        "    print(f\"âœ… Created {len(splits)} data-leakage-free splits:\")\n",
        "    for i, split in enumerate(splits):\n",
        "        print(f\"  Split {i+1}: Train {split['train_size']} days â†’ Test {split['test_size']} days\")\n",
        "        print(f\"    Train: {split['train_idx'][0].strftime('%Y-%m-%d')} to {split['train_idx'][-1].strftime('%Y-%m-%d')}\")\n",
        "        print(f\"    Test:  {split['test_idx'][0].strftime('%Y-%m-%d')} to {split['test_idx'][-1].strftime('%Y-%m-%d')}\")\n",
        "        print()\n",
        "\n",
        "    return splits\n",
        "\n",
        "def evaluate_forecast_relative(y_true, y_pred, seasonal_naive_pred, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Calculate forecast evaluation metrics RELATIVE to Seasonal Naive baseline\n",
        "    This makes results much easier to explain to business stakeholders\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove any NaN values\n",
        "    mask = ~(np.isnan(y_true) | np.isnan(y_pred) | np.isnan(seasonal_naive_pred))\n",
        "    y_true_clean = y_true[mask]\n",
        "    y_pred_clean = y_pred[mask]\n",
        "    seasonal_naive_clean = seasonal_naive_pred[mask]\n",
        "\n",
        "    if len(y_true_clean) == 0:\n",
        "        return {\n",
        "            'model': model_name,\n",
        "            'mae': np.nan,\n",
        "            'mape': np.nan,\n",
        "            'rmse': np.nan,\n",
        "            'r2': np.nan,\n",
        "            'mae_vs_baseline': np.nan,\n",
        "            'mape_vs_baseline': np.nan,\n",
        "            'rmse_vs_baseline': np.nan,\n",
        "            'improvement_pct': np.nan,\n",
        "            'n_obs': 0\n",
        "        }\n",
        "\n",
        "    # Calculate absolute metrics\n",
        "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
        "    mape = np.mean(np.abs((y_true_clean - y_pred_clean) / y_true_clean)) * 100\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
        "    r2 = r2_score(y_true_clean, y_pred_clean)\n",
        "\n",
        "    # Calculate baseline metrics\n",
        "    baseline_mae = mean_absolute_error(y_true_clean, seasonal_naive_clean)\n",
        "    baseline_mape = np.mean(np.abs((y_true_clean - seasonal_naive_clean) / y_true_clean)) * 100\n",
        "    baseline_rmse = np.sqrt(mean_squared_error(y_true_clean, seasonal_naive_clean))\n",
        "\n",
        "    # Calculate relative performance (positive = better than baseline)\n",
        "    mae_improvement = (baseline_mae - mae) / baseline_mae * 100\n",
        "    mape_improvement = (baseline_mape - mape) / baseline_mape * 100\n",
        "    rmse_improvement = (baseline_rmse - rmse) / baseline_rmse * 100\n",
        "\n",
        "    # Overall improvement score (based on MAPE)\n",
        "    improvement_pct = mape_improvement\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'mae': mae,\n",
        "        'mape': mape,\n",
        "        'rmse': rmse,\n",
        "        'r2': r2,\n",
        "        'baseline_mae': baseline_mae,\n",
        "        'baseline_mape': baseline_mape,\n",
        "        'baseline_rmse': baseline_rmse,\n",
        "        'mae_vs_baseline': mae_improvement,\n",
        "        'mape_vs_baseline': mape_improvement,\n",
        "        'rmse_vs_baseline': rmse_improvement,\n",
        "        'improvement_pct': improvement_pct,\n",
        "        'n_obs': len(y_true_clean)\n",
        "    }\n",
        "\n",
        "# Create cross-validation splits FIRST (before any feature engineering)\n",
        "if df is not None:\n",
        "    print(\"\\nðŸ”’ STEP 1: CREATE DATA SPLITS (Preventing Data Leakage)\")\n",
        "    print(\"=\" * 70)\n",
        "    cv_splits = create_time_series_splits_early(df, n_splits=5, test_size=7, gap=0)\n",
        "else:\n",
        "    print(\"âŒ Cannot create splits - no data loaded\")\n",
        "    cv_splits = []\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 6: Feature Engineering (Applied ONLY to Training Data)\n",
        "# ============================================================================\n",
        "\n",
        "def create_features_no_leakage(df_train, df_test=None):\n",
        "    \"\"\"\n",
        "    Create features using ONLY training data to prevent data leakage\n",
        "    Apply the same transformations to test data using training statistics\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ðŸ› ï¸ FEATURE ENGINEERING (No Data Leakage)\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"âœ… Using ONLY training data statistics for feature creation\")\n",
        "\n",
        "    # Work on training data\n",
        "    df_features_train = df_train.copy()\n",
        "\n",
        "    # Time-based features (no leakage risk)\n",
        "    df_features_train['year'] = df_features_train.index.year\n",
        "    df_features_train['month'] = df_features_train.index.month\n",
        "    df_features_train['day'] = df_features_train.index.day\n",
        "    df_features_train['dayofweek'] = df_features_train.index.dayofweek\n",
        "    df_features_train['dayofyear'] = df_features_train.index.dayofyear\n",
        "    df_features_train['quarter'] = df_features_train.index.quarter\n",
        "    df_features_train['week'] = df_features_train.index.isocalendar().week\n",
        "\n",
        "    # Cyclical encoding\n",
        "    df_features_train['month_sin'] = np.sin(2 * np.pi * df_features_train['month'] / 12)\n",
        "    df_features_train['month_cos'] = np.cos(2 * np.pi * df_features_train['month'] / 12)\n",
        "    df_features_train['dow_sin'] = np.sin(2 * np.pi * df_features_train['dayofweek'] / 7)\n",
        "    df_features_train['dow_cos'] = np.cos(2 * np.pi * df_features_train['dayofweek'] / 7)\n",
        "    df_features_train['doy_sin'] = np.sin(2 * np.pi * df_features_train['dayofyear'] / 365.25)\n",
        "    df_features_train['doy_cos'] = np.cos(2 * np.pi * df_features_train['dayofyear'] / 365.25)\n",
        "\n",
        "    # Binary features\n",
        "    df_features_train['is_weekend'] = (df_features_train['dayofweek'] >= 5).astype(int)\n",
        "    df_features_train['is_monday'] = (df_features_train['dayofweek'] == 0).astype(int)\n",
        "    df_features_train['is_friday'] = (df_features_train['dayofweek'] == 4).astype(int)\n",
        "    df_features_train['is_month_start'] = df_features_train.index.is_month_start.astype(int)\n",
        "    df_features_train['is_month_end'] = df_features_train.index.is_month_end.astype(int)\n",
        "\n",
        "    # Lag features (only use training data)\n",
        "    for lag in [1, 2, 3, 7]:\n",
        "        df_features_train[f'calls_lag_{lag}'] = df_features_train['calls'].shift(lag)\n",
        "\n",
        "    # Rolling statistics (only on training data)\n",
        "    for window in [7, 14]:\n",
        "        df_features_train[f'calls_mean_{window}d'] = df_features_train['calls'].rolling(window).mean()\n",
        "        df_features_train[f'calls_std_{window}d'] = df_features_train['calls'].rolling(window).std()\n",
        "\n",
        "    # Market features (if available, using training statistics only)\n",
        "    market_features = []\n",
        "    if '^VIX_close' in df_features_train.columns:\n",
        "        # VIX thresholds based on training data\n",
        "        train_vix_high_threshold = df_features_train['^VIX_close'].quantile(0.8)\n",
        "        df_features_train['vix_high'] = (df_features_train['^VIX_close'] > train_vix_high_threshold).astype(int)\n",
        "        df_features_train['vix_spike'] = (df_features_train['^VIX_close'].pct_change() > 0.2).astype(int)\n",
        "        market_features.extend(['vix_high', 'vix_spike'])\n",
        "\n",
        "    if 'SPY_close' in df_features_train.columns:\n",
        "        df_features_train['spy_returns'] = df_features_train['SPY_close'].pct_change()\n",
        "        df_features_train['market_stress'] = (df_features_train['spy_returns'] < -0.02).astype(int)\n",
        "        market_features.extend(['spy_returns', 'market_stress'])\n",
        "\n",
        "    print(f\"âœ… Created {len(df_features_train.columns)-len(df_train.columns)} new features for training data\")\n",
        "    print(f\"   Market features: {len(market_features)}\")\n",
        "\n",
        "    # Apply same transformations to test data (if provided)\n",
        "    if df_test is not None:\n",
        "        df_features_test = df_test.copy()\n",
        "\n",
        "        # Apply same time-based features\n",
        "        df_features_test['year'] = df_features_test.index.year\n",
        "        df_features_test['month'] = df_features_test.index.month\n",
        "        df_features_test['day'] = df_features_test.index.day\n",
        "        df_features_test['dayofweek'] = df_features_test.index.dayofweek\n",
        "        df_features_test['dayofyear'] = df_features_test.index.dayofyear\n",
        "        df_features_test['quarter'] = df_features_test.index.quarter\n",
        "        df_features_test['week'] = df_features_test.index.isocalendar().week\n",
        "\n",
        "        # Cyclical encoding\n",
        "        df_features_test['month_sin'] = np.sin(2 * np.pi * df_features_test['month'] / 12)\n",
        "        df_features_test['month_cos'] = np.cos(2 * np.pi * df_features_test['month'] / 12)\n",
        "        df_features_test['dow_sin'] = np.sin(2 * np.pi * df_features_test['dayofweek'] / 7)\n",
        "        df_features_test['dow_cos'] = np.cos(2 * np.pi * df_features_test['dayofweek'] / 7)\n",
        "        df_features_test['doy_sin'] = np.sin(2 * np.pi * df_features_test['dayofyear'] / 365.25)\n",
        "        df_features_test['doy_cos'] = np.cos(2 * np.pi * df_features_test['dayofyear'] / 365.25)\n",
        "\n",
        "        # Binary features\n",
        "        df_features_test['is_weekend'] = (df_features_test['dayofweek'] >= 5).astype(int)\n",
        "        df_features_test['is_monday'] = (df_features_test['dayofweek'] == 0).astype(int)\n",
        "        df_features_test['is_friday'] = (df_features_test['dayofweek'] == 4).astype(int)\n",
        "        df_features_test['is_month_start'] = df_features_test.index.is_month_start.astype(int)\n",
        "        df_features_test['is_month_end'] = df_features_test.index.is_month_end.astype(int)\n",
        "\n",
        "        # For lag features in test data, we need to be careful\n",
        "        # We can only use data up to the prediction point\n",
        "        combined_data = pd.concat([df_features_train['calls'], df_features_test['calls']])\n",
        "        for lag in [1, 2, 3, 7]:\n",
        "            df_features_test[f'calls_lag_{lag}'] = combined_data.shift(lag).loc[df_features_test.index]\n",
        "\n",
        "        # Rolling features for test (using expanded window)\n",
        "        for window in [7, 14]:\n",
        "            df_features_test[f'calls_mean_{window}d'] = combined_data.rolling(window).mean().loc[df_features_test.index]\n",
        "            df_features_test[f'calls_std_{window}d'] = combined_data.rolling(window).std().loc[df_features_test.index]\n",
        "\n",
        "        # Market features for test (using training thresholds)\n",
        "        if '^VIX_close' in df_features_test.columns and 'vix_high' in market_features:\n",
        "            df_features_test['vix_high'] = (df_features_test['^VIX_close'] > train_vix_high_threshold).astype(int)\n",
        "            df_features_test['vix_spike'] = (df_features_test['^VIX_close'].pct_change() > 0.2).astype(int)\n",
        "\n",
        "        if 'SPY_close' in df_features_test.columns and 'spy_returns' in market_features:\n",
        "            df_features_test['spy_returns'] = df_features_test['SPY_close'].pct_change()\n",
        "            df_features_test['market_stress'] = (df_features_test['spy_returns'] < -0.02).astype(int)\n",
        "\n",
        "        print(f\"âœ… Applied same transformations to test data\")\n",
        "        return df_features_train, df_features_test\n",
        "\n",
        "    return df_features_train, None\n",
        "\n",
        "print(\"ðŸ”’ Feature engineering will be applied to each CV split individually\")\n",
        "print(\"ðŸ“Š This prevents any future information from leaking into the models\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 6: Baseline Models\n",
        "# ============================================================================\n",
        "\n",
        "class BaselineModels:\n",
        "    \"\"\"Collection of simple baseline forecasting models\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "\n",
        "    def fit_naive(self, y_train):\n",
        "        \"\"\"Naive forecast: last value\"\"\"\n",
        "        self.models['naive'] = y_train.iloc[-1]\n",
        "        return self\n",
        "\n",
        "    def fit_seasonal_naive(self, y_train, season_length=7):\n",
        "        \"\"\"Seasonal naive: last value from same season\"\"\"\n",
        "        self.models['seasonal_naive'] = {\n",
        "            'values': y_train.iloc[-season_length:],\n",
        "            'season_length': season_length\n",
        "        }\n",
        "        return self\n",
        "\n",
        "    def fit_mean(self, y_train):\n",
        "        \"\"\"Mean forecast: historical average\"\"\"\n",
        "        self.models['mean'] = y_train.mean()\n",
        "        return self\n",
        "\n",
        "    def fit_drift(self, y_train):\n",
        "        \"\"\"Drift forecast: linear trend from first to last\"\"\"\n",
        "        n = len(y_train)\n",
        "        if n > 1:\n",
        "            slope = (y_train.iloc[-1] - y_train.iloc[0]) / (n - 1)\n",
        "            self.models['drift'] = {\n",
        "                'last_value': y_train.iloc[-1],\n",
        "                'slope': slope\n",
        "            }\n",
        "        else:\n",
        "            self.models['drift'] = {'last_value': y_train.iloc[-1], 'slope': 0}\n",
        "        return self\n",
        "\n",
        "    def predict(self, steps, model_type='naive'):\n",
        "        \"\"\"Generate forecasts\"\"\"\n",
        "        if model_type == 'naive':\n",
        "            return np.full(steps, self.models['naive'])\n",
        "\n",
        "        elif model_type == 'seasonal_naive':\n",
        "            model_info = self.models['seasonal_naive']\n",
        "            season_values = model_info['values'].values\n",
        "            season_length = model_info['season_length']\n",
        "            forecasts = []\n",
        "            for i in range(steps):\n",
        "                forecasts.append(season_values[i % season_length])\n",
        "            return np.array(forecasts)\n",
        "\n",
        "        elif model_type == 'mean':\n",
        "            return np.full(steps, self.models['mean'])\n",
        "\n",
        "        elif model_type == 'drift':\n",
        "            model_info = self.models['drift']\n",
        "            last_value = model_info['last_value']\n",
        "            slope = model_info['slope']\n",
        "            return np.array([last_value + slope * (i + 1) for i in range(steps)])\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "def fit_traditional_models(y_train, forecast_steps):\n",
        "    \"\"\"Fit traditional time series models\"\"\"\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. Exponential Smoothing (Holt-Winters)\n",
        "    try:\n",
        "        hw_model = ExponentialSmoothing(\n",
        "            y_train,\n",
        "            seasonal='add',\n",
        "            seasonal_periods=7,\n",
        "            trend='add'\n",
        "        ).fit()\n",
        "        hw_forecast = hw_model.forecast(steps=forecast_steps)\n",
        "        results['holt_winters'] = hw_forecast\n",
        "    except Exception as e:\n",
        "        print(f\"Holt-Winters failed: {e}\")\n",
        "        results['holt_winters'] = np.full(forecast_steps, y_train.mean())\n",
        "\n",
        "    # 2. SARIMA\n",
        "    try:\n",
        "        # Simple SARIMA(1,1,1)(1,1,1,7) - adjust based on your ACF/PACF analysis\n",
        "        sarima_model = SARIMAX(\n",
        "            y_train,\n",
        "            order=(1, 1, 1),\n",
        "            seasonal_order=(1, 1, 1, 7),\n",
        "            enforce_stationarity=False,\n",
        "            enforce_invertibility=False\n",
        "        ).fit(disp=False)\n",
        "        sarima_forecast = sarima_model.forecast(steps=forecast_steps)\n",
        "        results['sarima'] = sarima_forecast\n",
        "    except Exception as e:\n",
        "        print(f\"SARIMA failed: {e}\")\n",
        "        results['sarima'] = np.full(forecast_steps, y_train.mean())\n",
        "\n",
        "    # 3. Prophet (if available)\n",
        "    if PROPHET_AVAILABLE:\n",
        "        try:\n",
        "            prophet_df = pd.DataFrame({\n",
        "                'ds': y_train.index,\n",
        "                'y': y_train.values\n",
        "            })\n",
        "\n",
        "            prophet_model = Prophet(\n",
        "                daily_seasonality=False,\n",
        "                weekly_seasonality=True,\n",
        "                yearly_seasonality=True,\n",
        "                changepoint_prior_scale=0.05\n",
        "            )\n",
        "\n",
        "            prophet_model.fit(prophet_df)\n",
        "\n",
        "            future_dates = pd.date_range(\n",
        "                start=y_train.index[-1] + pd.Timedelta(days=1),\n",
        "                periods=forecast_steps,\n",
        "                freq='D'\n",
        "            )\n",
        "\n",
        "            future_df = pd.DataFrame({'ds': future_dates})\n",
        "            prophet_forecast = prophet_model.predict(future_df)['yhat'].values\n",
        "            results['prophet'] = prophet_forecast\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Prophet failed: {e}\")\n",
        "            results['prophet'] = np.full(forecast_steps, y_train.mean())\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 7: Machine Learning Models\n",
        "# ============================================================================\n",
        "\n",
        "def prepare_ml_features(df_features, target_col='calls'):\n",
        "    \"\"\"Prepare features for ML models\"\"\"\n",
        "\n",
        "    # Select feature columns (exclude target and non-predictive columns)\n",
        "    feature_cols = [col for col in df_features.columns\n",
        "                   if col != target_col and not col.startswith('calls_lag')]\n",
        "\n",
        "    # Add lag features back (they're predictive)\n",
        "    lag_cols = [col for col in df_features.columns if col.startswith('calls_lag')]\n",
        "    feature_cols.extend(lag_cols)\n",
        "\n",
        "    X = df_features[feature_cols].copy()\n",
        "    y = df_features[target_col].copy()\n",
        "\n",
        "    # Remove rows with any NaN values\n",
        "    complete_mask = ~(X.isna().any(axis=1) | y.isna())\n",
        "    X_clean = X[complete_mask]\n",
        "    y_clean = y[complete_mask]\n",
        "\n",
        "    return X_clean, y_clean, feature_cols\n",
        "\n",
        "def fit_ml_models(X_train, y_train, X_test):\n",
        "    \"\"\"Fit machine learning models\"\"\"\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # 1. Linear Regression\n",
        "    try:\n",
        "        lr_model = LinearRegression()\n",
        "        lr_model.fit(X_train_scaled, y_train)\n",
        "        lr_pred = lr_model.predict(X_test_scaled)\n",
        "        results['linear_regression'] = lr_pred\n",
        "    except Exception as e:\n",
        "        print(f\"Linear Regression failed: {e}\")\n",
        "        results['linear_regression'] = np.full(len(X_test), y_train.mean())\n",
        "\n",
        "    # 2. Random Forest\n",
        "    try:\n",
        "        rf_model = RandomForestRegressor(\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        rf_pred = rf_model.predict(X_test)\n",
        "        results['random_forest'] = rf_pred\n",
        "    except Exception as e:\n",
        "        print(f\"Random Forest failed: {e}\")\n",
        "        results['random_forest'] = np.full(len(X_test), y_train.mean())\n",
        "\n",
        "    # 3. Gradient Boosting\n",
        "    try:\n",
        "        gb_model = GradientBoostingRegressor(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42\n",
        "        )\n",
        "        gb_model.fit(X_train, y_train)\n",
        "        gb_pred = gb_model.predict(X_test)\n",
        "        results['gradient_boosting'] = gb_pred\n",
        "    except Exception as e:\n",
        "        print(f\"Gradient Boosting failed: {e}\")\n",
        "        results['gradient_boosting'] = np.full(len(X_test), y_train.mean())\n",
        "\n",
        "    return results, scaler\n",
        "\n",
        "print(\"ðŸ¤– MODEL DEFINITIONS READY\")\n",
        "print(\"=\" * 50)\n",
        "print(\"âœ… Baseline models: Naive, Seasonal Naive, Mean, Drift\")\n",
        "print(\"âœ… Traditional models: Holt-Winters, SARIMA, Prophet\")\n",
        "print(\"âœ… ML models: Linear Regression, Random Forest, Gradient Boosting\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 7.5: Adaptive Models for High Regime Change Data\n",
        "# ============================================================================\n",
        "\n",
        "class AdaptiveModels:\n",
        "    \"\"\"Models specifically designed for high regime change environments\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "\n",
        "    def fit_exponential_smoothing_adaptive(self, y_train, alpha=0.8):\n",
        "        \"\"\"High-alpha exponential smoothing for rapid adaptation\"\"\"\n",
        "        if len(y_train) < 2:\n",
        "            self.models['exp_smooth_adaptive'] = y_train.iloc[-1]\n",
        "            return self\n",
        "\n",
        "        # Simple exponential smoothing with high alpha\n",
        "        smoothed = [y_train.iloc[0]]\n",
        "        for i in range(1, len(y_train)):\n",
        "            smoothed.append(alpha * y_train.iloc[i] + (1 - alpha) * smoothed[-1])\n",
        "\n",
        "        self.models['exp_smooth_adaptive'] = smoothed[-1]\n",
        "        return self\n",
        "\n",
        "    def fit_moving_window_mean(self, y_train, window=3):\n",
        "        \"\"\"Very short moving window mean\"\"\"\n",
        "        if len(y_train) < window:\n",
        "            self.models['moving_window'] = y_train.mean()\n",
        "        else:\n",
        "            self.models['moving_window'] = y_train.tail(window).mean()\n",
        "        return self\n",
        "\n",
        "    def fit_weighted_recent(self, y_train, weights=None):\n",
        "        \"\"\"Weighted average with heavy emphasis on recent data\"\"\"\n",
        "        if weights is None:\n",
        "            # Exponentially decaying weights\n",
        "            n = min(7, len(y_train))  # Use last 7 days max\n",
        "            weights = np.exp(np.linspace(-2, 0, n))\n",
        "            weights = weights / weights.sum()\n",
        "\n",
        "        if len(y_train) >= len(weights):\n",
        "            recent_data = y_train.tail(len(weights))\n",
        "            self.models['weighted_recent'] = np.average(recent_data, weights=weights)\n",
        "        else:\n",
        "            self.models['weighted_recent'] = y_train.mean()\n",
        "        return self\n",
        "\n",
        "    def fit_linear_trend_short(self, y_train, window=5):\n",
        "        \"\"\"Linear trend on very short window\"\"\"\n",
        "        if len(y_train) < window:\n",
        "            window = len(y_train)\n",
        "\n",
        "        recent_data = y_train.tail(window)\n",
        "        if len(recent_data) > 1:\n",
        "            x = np.arange(len(recent_data))\n",
        "            slope, intercept = np.polyfit(x, recent_data.values, 1)\n",
        "            # Forecast one step ahead\n",
        "            self.models['linear_trend_short'] = slope * len(recent_data) + intercept\n",
        "        else:\n",
        "            self.models['linear_trend_short'] = recent_data.iloc[-1]\n",
        "        return self\n",
        "\n",
        "    def predict(self, steps, model_type):\n",
        "        \"\"\"Generate adaptive forecasts\"\"\"\n",
        "        base_value = self.models.get(model_type, 0)\n",
        "\n",
        "        if model_type == 'linear_trend_short':\n",
        "            # For linear trend, we already computed one step ahead\n",
        "            return np.full(steps, base_value)\n",
        "        else:\n",
        "            # For other models, repeat the fitted value\n",
        "            return np.full(steps, base_value)\n",
        "\n",
        "def fit_adaptive_models(y_train, forecast_steps):\n",
        "    \"\"\"Fit adaptive models designed for regime change environments\"\"\"\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. High-alpha exponential smoothing\n",
        "    adaptive = AdaptiveModels()\n",
        "    adaptive.fit_exponential_smoothing_adaptive(y_train, alpha=0.9)\n",
        "    results['adaptive_exp_smooth'] = adaptive.predict(forecast_steps, 'exp_smooth_adaptive')\n",
        "\n",
        "    # 2. Very short moving window\n",
        "    adaptive.fit_moving_window_mean(y_train, window=3)\n",
        "    results['adaptive_moving_3d'] = adaptive.predict(forecast_steps, 'moving_window')\n",
        "\n",
        "    # 3. Weighted recent observations\n",
        "    adaptive.fit_weighted_recent(y_train)\n",
        "    results['adaptive_weighted'] = adaptive.predict(forecast_steps, 'weighted_recent')\n",
        "\n",
        "    # 4. Short linear trend\n",
        "    adaptive.fit_linear_trend_short(y_train, window=5)\n",
        "    results['adaptive_linear_trend'] = adaptive.predict(forecast_steps, 'linear_trend_short')\n",
        "\n",
        "    # 5. Damped trend (conservative)\n",
        "    try:\n",
        "        from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "        damped_model = ExponentialSmoothing(\n",
        "            y_train.tail(14),  # Use only last 14 days\n",
        "            trend='add',\n",
        "            damped_trend=True\n",
        "        ).fit()\n",
        "        results['adaptive_damped_trend'] = damped_model.forecast(steps=forecast_steps)\n",
        "    except:\n",
        "        results['adaptive_damped_trend'] = np.full(forecast_steps, y_train.tail(7).mean())\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"ðŸ”„ ADAPTIVE MODELS READY\")\n",
        "print(\"=\" * 50)\n",
        "print(\"âœ… Adaptive models for high regime change environments:\")\n",
        "print(\"   â€¢ High-alpha exponential smoothing (Î±=0.9)\")\n",
        "print(\"   â€¢ Very short moving windows (3 days)\")\n",
        "print(\"   â€¢ Weighted recent observations\")\n",
        "print(\"   â€¢ Short-term linear trends (5 days)\")\n",
        "print(\"   â€¢ Damped trend models\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 8: Model Evaluation and Comparison\n",
        "# ============================================================================\n",
        "\n",
        "def run_comprehensive_evaluation():\n",
        "    \"\"\"Run all models on all CV splits with proper data leakage prevention and relative performance\"\"\"\n",
        "\n",
        "    print(\"ðŸŽ¯ RUNNING LEAKAGE-FREE MODEL EVALUATION\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"ðŸ”’ Features engineered separately for each split\")\n",
        "    print(\"ðŸ“Š Results shown relative to Seasonal Naive baseline\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for split_idx, split in enumerate(cv_splits):\n",
        "        print(f\"\\nðŸ“Š Evaluating Split {split_idx + 1}/{len(cv_splits)}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Get raw train/test data (before feature engineering)\n",
        "        train_data_raw = df.loc[split['train_idx']]\n",
        "        test_data_raw = df.loc[split['test_idx']]\n",
        "\n",
        "        # Limit training data for LONG-TERM regime changes (not daily volatility)\n",
        "        if len(train_data_raw) > 90:  # Use only last 90 days for structural changes\n",
        "            train_data_raw = train_data_raw.tail(90)\n",
        "            print(f\"  âš ï¸  Limited training to last 90 days due to long-term regime changes\")\n",
        "        elif len(train_data_raw) > 60:  # Fallback to 60 days\n",
        "            train_data_raw = train_data_raw.tail(60)\n",
        "            print(f\"  âš ï¸  Limited training to last 60 days due to regime changes\")\n",
        "\n",
        "        print(f\"  ðŸ“… Using {len(train_data_raw)} days for training (recent regime only)\")\n",
        "\n",
        "        # Apply feature engineering ONLY to this split's data\n",
        "        train_features, test_features = create_features_no_leakage(train_data_raw, test_data_raw)\n",
        "\n",
        "        y_train = train_features['calls']\n",
        "        y_test = test_data_raw['calls'].values\n",
        "        forecast_steps = len(test_data_raw)\n",
        "\n",
        "        # 1. BASELINE MODELS (including Seasonal Naive for reference)\n",
        "        print(\"  ðŸ”µ Fitting baseline models...\")\n",
        "        baseline = BaselineModels()\n",
        "        baseline.fit_naive(y_train)\n",
        "        baseline.fit_seasonal_naive(y_train, season_length=7)\n",
        "        baseline.fit_mean(y_train)\n",
        "        baseline.fit_drift(y_train)\n",
        "\n",
        "        # Get seasonal naive prediction for relative comparison\n",
        "        seasonal_naive_pred = baseline.predict(forecast_steps, 'seasonal_naive')\n",
        "\n",
        "        # Evaluate all baseline models\n",
        "        baseline_models = ['naive', 'seasonal_naive', 'mean', 'drift']\n",
        "        for model_name in baseline_models:\n",
        "            pred = baseline.predict(forecast_steps, model_name)\n",
        "            metrics = evaluate_forecast_relative(y_test, pred, seasonal_naive_pred, f\"baseline_{model_name}\")\n",
        "            metrics['split'] = split_idx + 1\n",
        "            all_results.append(metrics)\n",
        "\n",
        "        # 2. ADAPTIVE MODELS (designed for regime changes)\n",
        "        print(\"  ðŸŸ  Fitting adaptive models...\")\n",
        "        adaptive_results = fit_adaptive_models(y_train, forecast_steps)\n",
        "\n",
        "        for model_name, pred in adaptive_results.items():\n",
        "            metrics = evaluate_forecast_relative(y_test, pred, seasonal_naive_pred, model_name)\n",
        "            metrics['split'] = split_idx + 1\n",
        "            all_results.append(metrics)\n",
        "\n",
        "        # 3. TRADITIONAL MODELS (with short windows)\n",
        "        print(\"  ðŸŸ¢ Fitting traditional models (short window)...\")\n",
        "        try:\n",
        "            traditional_results = fit_traditional_models(y_train, forecast_steps)\n",
        "\n",
        "            for model_name, pred in traditional_results.items():\n",
        "                metrics = evaluate_forecast_relative(y_test, pred, seasonal_naive_pred, f\"{model_name}_short\")\n",
        "                metrics['split'] = split_idx + 1\n",
        "                all_results.append(metrics)\n",
        "        except Exception as e:\n",
        "            print(f\"    Traditional models failed: {e}\")\n",
        "\n",
        "        # 4. ML MODELS (with market-enhanced features)\n",
        "        print(\"  ðŸŸ¡ Fitting market-enhanced ML models...\")\n",
        "        try:\n",
        "            # Prepare ML features INCLUDING MARKET DATA (FIXED VERSION)\n",
        "            all_feature_cols = [col for col in train_features.columns if col not in ['calls']]\n",
        "\n",
        "            # Show what features are available\n",
        "            market_cols = [col for col in all_feature_cols if any(market in col for market in\n",
        "                          ['^VIX', 'SPY', 'QQQ', 'BTC', 'ETH', 'DX-Y', 'GC=F', 'vix_', 'market_', 'spy_', 'btc_', 'crypto_'])]\n",
        "\n",
        "            print(f\"    ðŸ“Š Available features: {len(all_feature_cols)} total\")\n",
        "            print(f\"    ðŸ“ˆ Market features found: {len(market_cols)}\")\n",
        "            if market_cols:\n",
        "                print(f\"    ðŸŽ¯ Market features: {market_cols[:5]}{'...' if len(market_cols) > 5 else ''}\")\n",
        "\n",
        "            # Use ALL available features (including market data)\n",
        "            X_train_ml = train_features[all_feature_cols].dropna()\n",
        "            y_train_ml = train_features.loc[X_train_ml.index, 'calls']\n",
        "            X_test_ml = test_features[all_feature_cols].reindex(test_data_raw.index).dropna()\n",
        "\n",
        "            print(f\"    ðŸ”§ ML training data: {X_train_ml.shape} (features: {X_train_ml.shape[1]})\")\n",
        "\n",
        "            if len(X_train_ml) > 5 and len(X_test_ml) > 0:\n",
        "                y_test_aligned = test_data_raw.loc[X_test_ml.index, 'calls'].values\n",
        "                seasonal_naive_aligned = seasonal_naive_pred[:len(y_test_aligned)]\n",
        "\n",
        "                ml_results, scaler = fit_ml_models(X_train_ml, y_train_ml, X_test_ml)\n",
        "\n",
        "                for model_name, pred in ml_results.items():\n",
        "                    if len(pred) == len(y_test_aligned):\n",
        "                        # Clear naming to show market enhancement\n",
        "                        enhanced_model_name = f\"{model_name}_MARKET_ENHANCED\"\n",
        "                        metrics = evaluate_forecast_relative(y_test_aligned, pred, seasonal_naive_aligned, enhanced_model_name)\n",
        "                        metrics['split'] = split_idx + 1\n",
        "                        all_results.append(metrics)\n",
        "\n",
        "                print(f\"    âœ… Market-enhanced ML models: {len(ml_results)} completed\")\n",
        "            else:\n",
        "                print(f\"    âŒ Insufficient ML data: train={len(X_train_ml)}, test={len(X_test_ml)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    âŒ Market-enhanced ML failed: {e}\")\n",
        "            import traceback\n",
        "            print(f\"    ðŸ” Error details: {traceback.format_exc()[-200:]}\")  # Last 200 chars of error\n",
        "\n",
        "    # Convert to DataFrame and analyze\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "\n",
        "    if len(results_df) == 0:\n",
        "        print(\"âŒ No results generated!\")\n",
        "        return None, None\n",
        "\n",
        "    # Calculate average performance across splits\n",
        "    avg_results = results_df.groupby('model').agg({\n",
        "        'mae': ['mean', 'std'],\n",
        "        'mape': ['mean', 'std'],\n",
        "        'rmse': ['mean', 'std'],\n",
        "        'r2': ['mean', 'std'],\n",
        "        'improvement_pct': ['mean', 'std'],\n",
        "        'mape_vs_baseline': ['mean', 'std']\n",
        "    }).round(3)\n",
        "\n",
        "    # Flatten column names\n",
        "    avg_results.columns = [f\"{metric}_{stat}\" for metric, stat in avg_results.columns]\n",
        "\n",
        "    # Sort by improvement over baseline\n",
        "    avg_results = avg_results.sort_values('improvement_pct_mean', ascending=False)\n",
        "\n",
        "    print(\"\\nðŸ† FINAL MODEL RANKINGS (Relative to Seasonal Naive)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Business-friendly results table\n",
        "    display_results = pd.DataFrame({\n",
        "        'Model': avg_results.index,\n",
        "        'MAPE (%)': avg_results['mape_mean'].round(1),\n",
        "        'vs Seasonal Naive': avg_results['improvement_pct_mean'].round(1).astype(str) + '%',\n",
        "        'Consistency (Â± std)': avg_results['improvement_pct_std'].round(1).astype(str) + '%',\n",
        "        'Status': ['Better' if x > 0 else 'Worse' for x in avg_results['improvement_pct_mean']]\n",
        "    })\n",
        "\n",
        "    print(display_results.to_string(index=False))\n",
        "\n",
        "    # Highlight key findings\n",
        "    print(f\"\\nðŸ“‹ KEY BUSINESS INSIGHTS:\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    # Find best model\n",
        "    best_model = avg_results.index[0]\n",
        "    best_improvement = avg_results.iloc[0]['improvement_pct_mean']\n",
        "\n",
        "    if best_improvement > 10:\n",
        "        print(f\"âœ… WINNER: {best_model}\")\n",
        "        print(f\"   â€¢ {best_improvement:.1f}% better than seasonal naive\")\n",
        "        print(f\"   â€¢ Clear performance advantage\")\n",
        "    elif best_improvement > 0:\n",
        "        print(f\"âœ… MODEST WINNER: {best_model}\")\n",
        "        print(f\"   â€¢ {best_improvement:.1f}% better than seasonal naive\")\n",
        "        print(f\"   â€¢ Small but consistent improvement\")\n",
        "    else:\n",
        "        print(f\"âŒ NO CLEAR WINNER\")\n",
        "        print(f\"   â€¢ Best model: {best_model} ({best_improvement:.1f}% vs baseline)\")\n",
        "        print(f\"   â€¢ Seasonal naive is hard to beat!\")\n",
        "\n",
        "    # Count how many models beat baseline\n",
        "    better_models = (avg_results['improvement_pct_mean'] > 0).sum()\n",
        "    total_models = len(avg_results)\n",
        "\n",
        "    print(f\"\\nðŸ“Š OVERALL ASSESSMENT:\")\n",
        "    print(f\"   â€¢ Models beating seasonal naive: {better_models}/{total_models}\")\n",
        "    print(f\"   â€¢ Average improvement range: {avg_results['improvement_pct_mean'].min():.1f}% to {avg_results['improvement_pct_mean'].max():.1f}%\")\n",
        "\n",
        "    if better_models < total_models / 2:\n",
        "        print(f\"   âš ï¸ WARNING: Most complex models underperform simple baseline\")\n",
        "        print(f\"   ðŸ’¡ INSIGHT: High regime changes favor simple adaptive models\")\n",
        "\n",
        "    # Adaptive models performance\n",
        "    adaptive_models = [idx for idx in avg_results.index if 'adaptive' in idx]\n",
        "    if adaptive_models:\n",
        "        adaptive_improvements = [avg_results.loc[model, 'improvement_pct_mean'] for model in adaptive_models]\n",
        "        avg_adaptive_improvement = np.mean(adaptive_improvements)\n",
        "        print(f\"\\nðŸ”„ ADAPTIVE MODELS ASSESSMENT:\")\n",
        "        print(f\"   â€¢ Average improvement: {avg_adaptive_improvement:.1f}%\")\n",
        "\n",
        "        if avg_adaptive_improvement > 5:\n",
        "            print(f\"   âœ… Adaptive models are working well for regime changes\")\n",
        "        elif avg_adaptive_improvement > 0:\n",
        "            print(f\"   âš ï¸ Adaptive models show modest improvement\")\n",
        "        else:\n",
        "            print(f\"   âŒ Even adaptive models struggle with this data\")\n",
        "\n",
        "    return results_df, avg_results\n",
        "\n",
        "def display_business_summary(avg_results):\n",
        "    \"\"\"Create a business-friendly summary of results\"\"\"\n",
        "\n",
        "    print(f\"\\nðŸ’¼ EXECUTIVE SUMMARY\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    if avg_results is None:\n",
        "        print(\"âŒ No results to summarize\")\n",
        "        return\n",
        "\n",
        "    best_model = avg_results.index[0]\n",
        "    best_improvement = avg_results.iloc[0]['improvement_pct_mean']\n",
        "    best_mape = avg_results.iloc[0]['mape_mean']\n",
        "\n",
        "    # Find seasonal naive performance for context\n",
        "    seasonal_naive_row = avg_results[avg_results.index.str.contains('seasonal_naive', case=False)]\n",
        "    if len(seasonal_naive_row) > 0:\n",
        "        baseline_mape = seasonal_naive_row.iloc[0]['mape_mean']\n",
        "    else:\n",
        "        baseline_mape = \"Unknown\"\n",
        "\n",
        "    print(f\"ðŸ“Š BOTTOM LINE:\")\n",
        "    print(f\"   â€¢ Best performing model: {best_model}\")\n",
        "    print(f\"   â€¢ Performance: {best_mape:.1f}% MAPE\")\n",
        "    print(f\"   â€¢ Improvement over simple baseline: {best_improvement:.1f}%\")\n",
        "    print(f\"   â€¢ Seasonal naive baseline: {baseline_mape:.1f}% MAPE\" if baseline_mape != \"Unknown\" else \"\")\n",
        "\n",
        "    print(f\"\\nðŸŽ¯ BUSINESS RECOMMENDATION:\")\n",
        "    if best_improvement > 15:\n",
        "        print(f\"   âœ… IMPLEMENT {best_model}\")\n",
        "        print(f\"   â€¢ Significant improvement over baseline\")\n",
        "        print(f\"   â€¢ Justifies implementation complexity\")\n",
        "    elif best_improvement > 5:\n",
        "        print(f\"   âš ï¸ CONSIDER {best_model}\")\n",
        "        print(f\"   â€¢ Modest improvement over baseline\")\n",
        "        print(f\"   â€¢ Evaluate implementation cost vs benefit\")\n",
        "    elif best_improvement > 0:\n",
        "        print(f\"   âš ï¸ STICK WITH SEASONAL NAIVE\")\n",
        "        print(f\"   â€¢ Complex models provide minimal benefit\")\n",
        "        print(f\"   â€¢ Simple baseline is nearly optimal\")\n",
        "    else:\n",
        "        print(f\"   âŒ STICK WITH SEASONAL NAIVE\")\n",
        "        print(f\"   â€¢ Complex models perform worse than baseline\")\n",
        "        print(f\"   â€¢ High regime changes make forecasting extremely difficult\")\n",
        "\n",
        "    print(f\"\\nðŸ“ˆ OPERATIONAL INSIGHT:\")\n",
        "    print(f\"   â€¢ Your data has extreme volatility (confirmed by regime analysis)\")\n",
        "    print(f\"   â€¢ Focus on rapid adaptation rather than prediction accuracy\")\n",
        "    print(f\"   â€¢ Use forecasts for directional guidance, not precise planning\")\n",
        "\n",
        "# Run the comprehensive evaluation\n",
        "print(\"ðŸš€ Starting comprehensive leakage-free evaluation...\")\n",
        "if cv_splits:\n",
        "    results_df, avg_results = run_comprehensive_evaluation()\n",
        "\n",
        "    if avg_results is not None:\n",
        "        display_business_summary(avg_results)\n",
        "else:\n",
        "    print(\"âŒ No CV splits available - cannot run evaluation\")\n",
        "    results_df, avg_results = None, None\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 9: Ensemble Methods (FIXED - No Data Leakage)\n",
        "# ============================================================================\n",
        "\n",
        "def create_ensemble_forecasts():\n",
        "    \"\"\"Create ensemble forecasts combining multiple models - FULLY FIXED VERSION\"\"\"\n",
        "\n",
        "    print(\"\\nðŸ”— CREATING ENSEMBLE FORECASTS (LEAKAGE-FREE)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if results_df is None or len(results_df) == 0:\n",
        "        print(\"âŒ No base model results available for ensembling\")\n",
        "        return None, None\n",
        "\n",
        "    print(\"âš ï¸  Using short-window ensembles due to regime changes\")\n",
        "    print(\"ðŸ”’ Applying same leakage-free approach as main evaluation\")\n",
        "\n",
        "    ensemble_results = []\n",
        "\n",
        "    for split_idx, split in enumerate(cv_splits):\n",
        "        print(f\"\\nðŸ“Š Ensemble for Split {split_idx + 1}\")\n",
        "\n",
        "        # Get RAW train/test data (same approach as main evaluation)\n",
        "        train_data_raw = df.loc[split['train_idx']]\n",
        "        test_data_raw = df.loc[split['test_idx']]\n",
        "\n",
        "        # Limit training data to last 10 days for ensembles\n",
        "        if len(train_data_raw) > 10:\n",
        "            train_data_raw = train_data_raw.tail(10)\n",
        "            print(f\"  ðŸ“… Limited to last {len(train_data_raw)} days for training\")\n",
        "\n",
        "        # Apply feature engineering ONLY to this split (no data leakage)\n",
        "        train_features, test_features = create_features_no_leakage(train_data_raw, test_data_raw)\n",
        "\n",
        "        y_train = train_features['calls']\n",
        "        y_test = test_data_raw['calls'].values\n",
        "        forecast_steps = len(test_data_raw)\n",
        "\n",
        "        # Collect all model predictions\n",
        "        predictions = {}\n",
        "\n",
        "        print(\"  ðŸ”µ Baseline models for ensemble...\")\n",
        "        # Baseline models\n",
        "        baseline = BaselineModels()\n",
        "        baseline.fit_naive(y_train)\n",
        "        baseline.fit_seasonal_naive(y_train, season_length=7)\n",
        "        baseline.fit_mean(y_train)\n",
        "        baseline.fit_drift(y_train)\n",
        "\n",
        "        predictions['naive'] = baseline.predict(forecast_steps, 'naive')\n",
        "        predictions['seasonal_naive'] = baseline.predict(forecast_steps, 'seasonal_naive')\n",
        "        predictions['mean'] = baseline.predict(forecast_steps, 'mean')\n",
        "        predictions['drift'] = baseline.predict(forecast_steps, 'drift')\n",
        "\n",
        "        print(\"  ðŸŸ  Adaptive models for ensemble...\")\n",
        "        # Adaptive models\n",
        "        try:\n",
        "            adaptive_results = fit_adaptive_models(y_train, forecast_steps)\n",
        "            predictions.update(adaptive_results)\n",
        "        except Exception as e:\n",
        "            print(f\"    Adaptive models failed: {e}\")\n",
        "\n",
        "        print(\"  ðŸŸ¢ Traditional models for ensemble...\")\n",
        "        # Traditional models (if they work)\n",
        "        try:\n",
        "            traditional_results = fit_traditional_models(y_train, forecast_steps)\n",
        "            for model_name, pred in traditional_results.items():\n",
        "                predictions[f\"{model_name}_short\"] = pred\n",
        "        except Exception as e:\n",
        "            print(f\"    Traditional models failed: {e}\")\n",
        "\n",
        "        # Get seasonal naive baseline for relative evaluation\n",
        "        seasonal_naive_pred = predictions['seasonal_naive']\n",
        "\n",
        "        # Create ensembles\n",
        "        valid_predictions = {name: pred for name, pred in predictions.items()\n",
        "                           if len(pred) == forecast_steps and not np.any(np.isnan(pred))}\n",
        "\n",
        "        if len(valid_predictions) > 1:\n",
        "            pred_array = np.array(list(valid_predictions.values()))\n",
        "\n",
        "            print(f\"  ðŸ”— Creating ensembles from {len(valid_predictions)} models...\")\n",
        "\n",
        "            # 1. Simple average ensemble\n",
        "            ensemble_mean = np.mean(pred_array, axis=0)\n",
        "            metrics = evaluate_forecast_relative(y_test, ensemble_mean, seasonal_naive_pred, \"ensemble_mean\")\n",
        "            metrics['split'] = split_idx + 1\n",
        "            ensemble_results.append(metrics)\n",
        "\n",
        "            # 2. Median ensemble (robust to outliers)\n",
        "            ensemble_median = np.median(pred_array, axis=0)\n",
        "            metrics = evaluate_forecast_relative(y_test, ensemble_median, seasonal_naive_pred, \"ensemble_median\")\n",
        "            metrics['split'] = split_idx + 1\n",
        "            ensemble_results.append(metrics)\n",
        "\n",
        "            # 3. Adaptive-only ensemble (if we have adaptive models)\n",
        "            adaptive_preds = {name: pred for name, pred in valid_predictions.items() if 'adaptive' in name}\n",
        "            if len(adaptive_preds) > 1:\n",
        "                adaptive_array = np.array(list(adaptive_preds.values()))\n",
        "                ensemble_adaptive = np.mean(adaptive_array, axis=0)\n",
        "                metrics = evaluate_forecast_relative(y_test, ensemble_adaptive, seasonal_naive_pred, \"ensemble_adaptive_only\")\n",
        "                metrics['split'] = split_idx + 1\n",
        "                ensemble_results.append(metrics)\n",
        "        else:\n",
        "            print(f\"  âš ï¸  Not enough valid models for ensembling\")\n",
        "\n",
        "    # Add ensemble results to main results\n",
        "    if ensemble_results:\n",
        "        ensemble_df = pd.DataFrame(ensemble_results)\n",
        "        combined_results = pd.concat([results_df, ensemble_df], ignore_index=True)\n",
        "\n",
        "        # Recalculate averages including ensembles\n",
        "        avg_with_ensemble = combined_results.groupby('model').agg({\n",
        "            'mae': ['mean', 'std'],\n",
        "            'mape': ['mean', 'std'],\n",
        "            'rmse': ['mean', 'std'],\n",
        "            'r2': ['mean', 'std'],\n",
        "            'improvement_pct': ['mean', 'std'],\n",
        "            'mape_vs_baseline': ['mean', 'std']\n",
        "        }).round(3)\n",
        "\n",
        "        avg_with_ensemble.columns = [f\"{metric}_{stat}\" for metric, stat in avg_with_ensemble.columns]\n",
        "        avg_with_ensemble = avg_with_ensemble.sort_values('improvement_pct_mean', ascending=False)\n",
        "\n",
        "        print(\"\\nðŸ† UPDATED RANKINGS WITH ENSEMBLES\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Business-friendly display\n",
        "        display_results = pd.DataFrame({\n",
        "            'Model': avg_with_ensemble.index,\n",
        "            'MAPE (%)': avg_with_ensemble['mape_mean'].round(1),\n",
        "            'vs Seasonal Naive': avg_with_ensemble['improvement_pct_mean'].round(1).astype(str) + '%',\n",
        "            'Consistency (Â± std)': avg_with_ensemble['improvement_pct_std'].round(1).astype(str) + '%',\n",
        "            'Status': ['Better' if x > 0 else 'Worse' for x in avg_with_ensemble['improvement_pct_mean']]\n",
        "        })\n",
        "\n",
        "        print(display_results.to_string(index=False))\n",
        "\n",
        "        # Highlight ensemble performance\n",
        "        ensemble_models = [idx for idx in avg_with_ensemble.index if 'ensemble' in idx]\n",
        "        if ensemble_models:\n",
        "            print(f\"\\nðŸ”— ENSEMBLE ANALYSIS:\")\n",
        "            best_ensemble = None\n",
        "            best_improvement = -999\n",
        "\n",
        "            for model in ensemble_models:\n",
        "                improvement = avg_with_ensemble.loc[model, 'improvement_pct_mean']\n",
        "                print(f\"   â€¢ {model}: {improvement:.1f}% vs seasonal naive\")\n",
        "                if improvement > best_improvement:\n",
        "                    best_improvement = improvement\n",
        "                    best_ensemble = model\n",
        "\n",
        "            if best_improvement > 5:\n",
        "                print(f\"\\nâœ… ENSEMBLE SUCCESS: {best_ensemble} shows {best_improvement:.1f}% improvement\")\n",
        "                print(\"   ðŸ’¡ Model combination is adding value!\")\n",
        "            elif best_improvement > 0:\n",
        "                print(f\"\\nâš ï¸ MODEST ENSEMBLE GAIN: {best_ensemble} shows {best_improvement:.1f}% improvement\")\n",
        "                print(\"   ðŸ’¡ Small but consistent ensemble benefit\")\n",
        "            else:\n",
        "                print(f\"\\nâŒ ENSEMBLE UNDERPERFORMING: Best ensemble {best_improvement:.1f}% vs baseline\")\n",
        "                print(\"   ðŸ’¡ Individual models may be better than combinations\")\n",
        "\n",
        "        return combined_results, avg_with_ensemble\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ No ensemble results generated\")\n",
        "        return results_df, avg_results if 'avg_results' in globals() else None\n",
        "\n",
        "# Create ensembles\n",
        "if results_df is not None:\n",
        "    final_results_df, final_avg_results = create_ensemble_forecasts()\n",
        "else:\n",
        "    final_results_df, final_avg_results = None, None\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 10: Model Diagnostics and Residual Analysis\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_best_model_residuals():\n",
        "    \"\"\"Analyze residuals of the best performing model - FIXED for leakage-free approach\"\"\"\n",
        "\n",
        "    print(\"\\nðŸ”¬ RESIDUAL ANALYSIS OF BEST MODEL\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if final_avg_results is None:\n",
        "        print(\"âŒ No model results available for analysis\")\n",
        "        return\n",
        "\n",
        "    # Get best model\n",
        "    best_model = final_avg_results.index[0]\n",
        "    print(f\"ðŸ† Analyzing best model: {best_model}\")\n",
        "\n",
        "    # Use the raw dataset for final model fitting and residual analysis\n",
        "    # Split into train/test (last 30 days for testing)\n",
        "    split_date = df.index[-31] if len(df) > 31 else df.index[len(df)//2]  # Fallback if less than 31 days\n",
        "    train_data_raw = df[df.index < split_date]\n",
        "    test_data_raw = df[df.index >= split_date]\n",
        "\n",
        "    # Apply feature engineering to this final split (consistent with CV approach)\n",
        "    train_features, test_features = create_features_no_leakage(train_data_raw, test_data_raw)\n",
        "\n",
        "    y_train = train_features['calls']\n",
        "    y_test = test_data_raw['calls']\n",
        "\n",
        "    print(f\"ðŸ“Š Using {len(y_train)} days for training, {len(y_test)} days for testing\")\n",
        "\n",
        "    # Fit the best model using same approach as CV\n",
        "    if best_model.startswith('baseline_'):\n",
        "        model_type = best_model.replace('baseline_', '')\n",
        "        baseline = BaselineModels()\n",
        "        baseline.fit_naive(y_train)\n",
        "        baseline.fit_seasonal_naive(y_train, season_length=7)\n",
        "        baseline.fit_mean(y_train)\n",
        "        baseline.fit_drift(y_train)\n",
        "\n",
        "        # Get in-sample predictions for residual analysis\n",
        "        if model_type == 'seasonal_naive':\n",
        "            # For seasonal naive, use the pattern on training data\n",
        "            residuals = []\n",
        "            season_values = y_train.iloc[-7:].values\n",
        "            for i in range(len(y_train)):\n",
        "                expected = season_values[i % 7]\n",
        "                residuals.append(y_train.iloc[i] - expected)\n",
        "            residuals = np.array(residuals)\n",
        "        else:\n",
        "            # Simple residuals for other baseline models\n",
        "            if model_type == 'naive':\n",
        "                pred_train = np.full(len(y_train), y_train.iloc[-1])\n",
        "            elif model_type == 'mean':\n",
        "                pred_train = np.full(len(y_train), y_train.mean())\n",
        "            else:  # drift\n",
        "                n = len(y_train)\n",
        "                slope = (y_train.iloc[-1] - y_train.iloc[0]) / (n - 1) if n > 1 else 0\n",
        "                pred_train = np.array([y_train.iloc[0] + slope * i for i in range(n)])\n",
        "\n",
        "            residuals = y_train.values - pred_train\n",
        "\n",
        "        test_pred = baseline.predict(len(y_test), model_type)\n",
        "\n",
        "    elif 'adaptive' in best_model:\n",
        "        # Adaptive model residuals\n",
        "        adaptive_results = fit_adaptive_models(y_train, len(y_test))\n",
        "        test_pred = adaptive_results.get(best_model, np.full(len(y_test), y_train.mean()))\n",
        "\n",
        "        # Simplified residuals for adaptive models\n",
        "        residuals = y_train.values - y_train.mean()\n",
        "\n",
        "    elif best_model in ['holt_winters_short', 'sarima_short', 'prophet_short']:\n",
        "        base_model = best_model.replace('_short', '')\n",
        "        traditional_results = fit_traditional_models(y_train, len(y_test))\n",
        "        test_pred = traditional_results.get(base_model, np.full(len(y_test), y_train.mean()))\n",
        "\n",
        "        # Simplified residuals for traditional models\n",
        "        residuals = y_train.values - y_train.mean()\n",
        "\n",
        "    elif 'market_enhanced' in best_model:\n",
        "        # Market-enhanced ML model\n",
        "        try:\n",
        "            # Use same feature preparation as in CV\n",
        "            feature_cols = [col for col in train_features.columns if col not in ['calls']]\n",
        "            market_feature_priorities = ['^VIX_close', 'SPY_close', 'QQQ_close', 'BTC-USD_close']\n",
        "            market_derived_features = [col for col in train_features.columns if any(x in col.lower() for x in\n",
        "                                     ['vix_high', 'market_stress', 'spy_returns'])]\n",
        "            feature_cols.extend(market_derived_features)\n",
        "\n",
        "            X_train_ml = train_features[feature_cols].dropna()\n",
        "            y_train_ml = train_features.loc[X_train_ml.index, 'calls']\n",
        "            X_test_ml = test_features[feature_cols].reindex(test_data_raw.index).dropna()\n",
        "\n",
        "            if len(X_train_ml) > 5 and len(X_test_ml) > 0:\n",
        "                ml_results, scaler = fit_ml_models(X_train_ml, y_train_ml, X_test_ml)\n",
        "                base_model_name = best_model.replace('_market_enhanced', '')\n",
        "\n",
        "                if base_model_name in ml_results:\n",
        "                    test_pred = ml_results[base_model_name]\n",
        "                    # Use model's fitted values for residuals if available\n",
        "                    residuals = y_train_ml.values - y_train_ml.mean()  # Simplified\n",
        "                else:\n",
        "                    test_pred = np.full(len(y_test), y_train.mean())\n",
        "                    residuals = y_train.values - y_train.mean()\n",
        "            else:\n",
        "                test_pred = np.full(len(y_test), y_train.mean())\n",
        "                residuals = y_train.values - y_train.mean()\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ML model analysis failed: {e}\")\n",
        "            test_pred = np.full(len(y_test), y_train.mean())\n",
        "            residuals = y_train.values - y_train.mean()\n",
        "\n",
        "    else:\n",
        "        # Fallback for other models\n",
        "        residuals = y_train.values - y_train.mean()\n",
        "        test_pred = np.full(len(y_test), y_train.mean())\n",
        "\n",
        "    # Remove NaN residuals\n",
        "    residuals_clean = residuals[~np.isnan(residuals)]\n",
        "\n",
        "    if len(residuals_clean) == 0:\n",
        "        print(\"âŒ No valid residuals for analysis\")\n",
        "        return\n",
        "\n",
        "    # Residual analysis plots\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # 1. Residuals over time\n",
        "    axes[0,0].plot(residuals_clean)\n",
        "    axes[0,0].set_title('Residuals Over Time')\n",
        "    axes[0,0].set_ylabel('Residuals')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    axes[0,0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # 2. Residual distribution\n",
        "    axes[0,1].hist(residuals_clean, bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[0,1].set_title('Residual Distribution')\n",
        "    axes[0,1].set_xlabel('Residuals')\n",
        "    axes[0,1].set_ylabel('Frequency')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Q-Q plot\n",
        "    stats.probplot(residuals_clean, dist=\"norm\", plot=axes[0,2])\n",
        "    axes[0,2].set_title('Q-Q Plot (Normality Check)')\n",
        "    axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. ACF of residuals\n",
        "    if len(residuals_clean) > 20:\n",
        "        plot_acf(residuals_clean, ax=axes[1,0], lags=min(20, len(residuals_clean)//4))\n",
        "        axes[1,0].set_title('ACF of Residuals')\n",
        "    else:\n",
        "        axes[1,0].text(0.5, 0.5, 'Insufficient data\\nfor ACF plot', ha='center', va='center', transform=axes[1,0].transAxes)\n",
        "        axes[1,0].set_title('ACF of Residuals (insufficient data)')\n",
        "\n",
        "    # 5. Residuals vs fitted (simplified)\n",
        "    fitted_approx = y_train.values - residuals_clean[:len(y_train)]\n",
        "    if len(fitted_approx) == len(residuals_clean):\n",
        "        axes[1,1].scatter(fitted_approx, residuals_clean, alpha=0.6)\n",
        "        axes[1,1].set_xlabel('Fitted Values')\n",
        "        axes[1,1].set_ylabel('Residuals')\n",
        "        axes[1,1].set_title('Residuals vs Fitted')\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "        axes[1,1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "    else:\n",
        "        axes[1,1].text(0.5, 0.5, 'Cannot align\\nfitted values', ha='center', va='center', transform=axes[1,1].transAxes)\n",
        "        axes[1,1].set_title('Residuals vs Fitted (alignment issue)')\n",
        "\n",
        "    # 6. Forecast vs actual\n",
        "    axes[1,2].plot(y_test.index, y_test.values, label='Actual', linewidth=2)\n",
        "    if len(test_pred) == len(y_test):\n",
        "        axes[1,2].plot(y_test.index, test_pred, label='Forecast', linewidth=2, alpha=0.8)\n",
        "    axes[1,2].set_title('Forecast vs Actual (Test Set)')\n",
        "    axes[1,2].set_ylabel('Calls')\n",
        "    axes[1,2].legend()\n",
        "    axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Statistical tests\n",
        "    print(f\"\\nðŸ“Š RESIDUAL STATISTICS:\")\n",
        "    print(f\"   Mean: {np.mean(residuals_clean):.3f}\")\n",
        "    print(f\"   Std: {np.std(residuals_clean):.3f}\")\n",
        "    print(f\"   Skewness: {stats.skew(residuals_clean):.3f}\")\n",
        "    print(f\"   Kurtosis: {stats.kurtosis(residuals_clean):.3f}\")\n",
        "\n",
        "    # Normality tests\n",
        "    if len(residuals_clean) > 8:  # Minimum for Shapiro test\n",
        "        try:\n",
        "            shapiro_stat, shapiro_p = shapiro(residuals_clean)\n",
        "            jb_stat, jb_p = jarque_bera(residuals_clean)\n",
        "\n",
        "            print(f\"\\nðŸ§ª NORMALITY TESTS:\")\n",
        "            print(f\"   Shapiro-Wilk: statistic={shapiro_stat:.4f}, p-value={shapiro_p:.4f}\")\n",
        "            print(f\"   Jarque-Bera: statistic={jb_stat:.4f}, p-value={jb_p:.4f}\")\n",
        "\n",
        "            if shapiro_p > 0.05 and jb_p > 0.05:\n",
        "                print(\"   âœ… Residuals appear normally distributed\")\n",
        "            else:\n",
        "                print(\"   âŒ Residuals are not normally distributed\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nðŸ§ª Normality tests failed: {e}\")\n",
        "\n",
        "    # Test set performance\n",
        "    if len(test_pred) == len(y_test):\n",
        "        seasonal_naive_baseline = BaselineModels()\n",
        "        seasonal_naive_baseline.fit_seasonal_naive(y_train, season_length=7)\n",
        "        baseline_pred = seasonal_naive_baseline.predict(len(y_test), 'seasonal_naive')\n",
        "\n",
        "        test_metrics = evaluate_forecast_relative(y_test.values, test_pred, baseline_pred, best_model)\n",
        "        print(f\"\\nðŸŽ¯ TEST SET PERFORMANCE:\")\n",
        "        print(f\"   MAPE: {test_metrics['mape']:.2f}%\")\n",
        "        print(f\"   MAE: {test_metrics['mae']:.1f}\")\n",
        "        print(f\"   vs Seasonal Naive: {test_metrics['improvement_pct']:.1f}%\")\n",
        "        print(f\"   RÂ²: {test_metrics['r2']:.3f}\")\n",
        "    else:\n",
        "        print(f\"\\nðŸŽ¯ TEST SET PERFORMANCE: Unable to calculate (prediction length mismatch)\")\n",
        "\n",
        "# Run residual analysis\n",
        "if final_avg_results is not None:\n",
        "    analyze_best_model_residuals()\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 11: Final Recommendations and Next Steps\n",
        "# ============================================================================\n",
        "\n",
        "def generate_recommendations():\n",
        "    \"\"\"Generate final recommendations based on results - ADAPTED FOR REGIME CHANGE DATA\"\"\"\n",
        "\n",
        "    print(\"\\nðŸ’¡ REGIME CHANGE ADAPTED RECOMMENDATIONS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if final_avg_results is None:\n",
        "        print(\"âŒ No results available for recommendations\")\n",
        "        return\n",
        "\n",
        "    best_model = final_avg_results.index[0]\n",
        "    best_mape = final_avg_results.loc[best_model, 'mape_mean']\n",
        "    best_r2 = final_avg_results.loc[best_model, 'r2_mean']\n",
        "\n",
        "    print(f\"ðŸ† BEST MODEL FOR REGIME CHANGE DATA: {best_model}\")\n",
        "    print(f\"   MAPE: {best_mape:.2f}%\")\n",
        "    print(f\"   RÂ²: {best_r2:.3f}\")\n",
        "    print()\n",
        "\n",
        "    # Performance assessment adapted for regime change environment\n",
        "    if best_mape <= 15:\n",
        "        print(\"âœ… EXCELLENT performance given extreme regime changes\")\n",
        "    elif best_mape <= 25:\n",
        "        print(\"âœ… GOOD performance for high volatility environment\")\n",
        "    elif best_mape <= 35:\n",
        "        print(\"âš ï¸ ACCEPTABLE given structural instability\")\n",
        "    else:\n",
        "        print(\"âŒ Consider even simpler models or different approach\")\n",
        "\n",
        "    print(f\"\\nðŸ“‹ REGIME CHANGE SPECIFIC RECOMMENDATIONS:\")\n",
        "\n",
        "    # Check if adaptive models performed well\n",
        "    adaptive_in_top3 = any('adaptive' in model for model in final_avg_results.index[:3])\n",
        "\n",
        "    if adaptive_in_top3:\n",
        "        print(\"   âœ… Adaptive models are working - continue with short-window approaches\")\n",
        "        print(\"   â€¢ Implement DAILY model retraining\")\n",
        "        print(\"   â€¢ Use EXPONENTIAL SMOOTHING with high alpha (0.8-0.9)\")\n",
        "        print(\"   â€¢ Focus on 1-3 day forecasts maximum\")\n",
        "    else:\n",
        "        print(\"   âš ï¸ Even adaptive models struggling - consider:\")\n",
        "        print(\"   â€¢ HOURLY forecasting instead of daily\")\n",
        "        print(\"   â€¢ REAL-TIME updating with streaming data\")\n",
        "        print(\"   â€¢ BUSINESS RULE-BASED forecasts during unstable periods\")\n",
        "\n",
        "    # Specific tactical recommendations\n",
        "    print(f\"\\nðŸŽ¯ IMMEDIATE TACTICAL ACTIONS:\")\n",
        "    print(\"   1. ABANDON long-term forecasting (>3 days)\")\n",
        "    print(\"   2. SET UP change point detection alerts\")\n",
        "    print(\"   3. IMPLEMENT rolling window validation (max 7 days)\")\n",
        "    print(\"   4. CREATE forecast confidence bands (very wide)\")\n",
        "    print(\"   5. ESTABLISH business escalation for detected regime changes\")\n",
        "\n",
        "    # Model-specific guidance\n",
        "    if 'adaptive' in best_model:\n",
        "        print(f\"\\nðŸ”§ ADAPTIVE MODEL OPTIMIZATION:\")\n",
        "        print(\"   â€¢ Tune smoothing parameters daily\")\n",
        "        print(\"   â€¢ Monitor forecast accuracy hourly\")\n",
        "        print(\"   â€¢ Have fallback rules for extreme changes\")\n",
        "\n",
        "    elif 'baseline' in best_model:\n",
        "        print(f\"\\nðŸ”§ BASELINE MODEL SUCCESS INDICATES:\")\n",
        "        print(\"   â€¢ Data has extremely high noise-to-signal ratio\")\n",
        "        print(\"   â€¢ Simple is better in chaos\")\n",
        "        print(\"   â€¢ Focus on business process improvements vs. forecasting\")\n",
        "\n",
        "    # Risk management recommendations\n",
        "    print(f\"\\nâš ï¸ RISK MANAGEMENT FOR REGIME CHANGE ENVIRONMENT:\")\n",
        "    print(\"   â€¢ BUFFER INVENTORY: Increase safety stock significantly\")\n",
        "    print(\"   â€¢ STAFFING FLEXIBILITY: Cross-train agents for rapid scaling\")\n",
        "    print(\"   â€¢ SCENARIO PLANNING: Prepare for sudden volume changes\")\n",
        "    print(\"   â€¢ EARLY WARNING SYSTEM: Real-time anomaly detection\")\n",
        "    print(\"   â€¢ BUSINESS INTELLIGENCE: Identify regime change triggers\")\n",
        "\n",
        "    # Technology recommendations\n",
        "    print(f\"\\nðŸ”§ TECHNOLOGY IMPROVEMENTS:\")\n",
        "    print(\"   â€¢ STREAMING ANALYTICS: Real-time model updates\")\n",
        "    print(\"   â€¢ ENSEMBLE FORECASTING: Combine multiple short-term models\")\n",
        "    print(\"   â€¢ ONLINE LEARNING: Models that adapt with each observation\")\n",
        "    print(\"   â€¢ CHANGE DETECTION: Automated structural break identification\")\n",
        "\n",
        "    print(f\"\\nðŸŽ¯ SUCCESS METRICS FOR UNSTABLE DATA:\")\n",
        "    print(\"   â€¢ DIRECTIONAL ACCURACY: % of correct up/down predictions\")\n",
        "    print(\"   â€¢ FORECAST HORIZON: How many days ahead remain useful\")\n",
        "    print(\"   â€¢ CHANGE DETECTION SPEED: How quickly models adapt\")\n",
        "    print(\"   â€¢ BUSINESS VALUE: Impact on operational decisions\")\n",
        "\n",
        "    print(f\"\\nâš¡ REGIME CHANGE REALITY CHECK:\")\n",
        "    print(\"   â€¢ With 37 changes in 36 days, perfect forecasting is impossible\")\n",
        "    print(\"   â€¢ Focus on RAPID ADAPTATION over accuracy\")\n",
        "    print(\"   â€¢ Invest in FLEXIBILITY over prediction precision\")\n",
        "    print(\"   â€¢ Consider if the underlying business process can be stabilized\")\n",
        "\n",
        "# Update ensemble creation for regime change data\n",
        "def create_ensemble_forecasts():\n",
        "    \"\"\"Create ensemble forecasts - ADAPTED for regime change environments with no data leakage\"\"\"\n",
        "\n",
        "    print(\"\\nðŸ”— CREATING ADAPTIVE ENSEMBLE FORECASTS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if results_df is None or len(results_df) == 0:\n",
        "        print(\"âŒ No base model results available for ensembling\")\n",
        "        return results_df, avg_results if 'avg_results' in globals() else None\n",
        "\n",
        "    print(\"âš ï¸  Using short-window ensembles due to regime changes\")\n",
        "    print(\"ðŸ”’ Applying leakage-free feature engineering for ensembles\")\n",
        "\n",
        "    ensemble_results = []\n",
        "\n",
        "    for split_idx, split in enumerate(cv_splits):\n",
        "        print(f\"\\nðŸ“Š Adaptive Ensemble for Split {split_idx + 1}\")\n",
        "\n",
        "        # Get raw train/test data (same as main evaluation)\n",
        "        train_data_raw = df.loc[split['train_idx']]\n",
        "        test_data_raw = df.loc[split['test_idx']]\n",
        "\n",
        "        # Limit training to last 10 days for ensembles\n",
        "        if len(train_data_raw) > 10:\n",
        "            train_data_raw = train_data_raw.tail(10)\n",
        "\n",
        "        # Apply feature engineering to this split's data\n",
        "        train_features, test_features = create_features_no_leakage(train_data_raw, test_data_raw)\n",
        "\n",
        "        y_train = train_features['calls']\n",
        "        y_test = test_data_raw['calls'].values\n",
        "        forecast_steps = len(test_data_raw)\n",
        "\n",
        "        # Collect predictions from adaptive models only\n",
        "        predictions = {}\n",
        "\n",
        "        # Adaptive models\n",
        "        adaptive_results = fit_adaptive_models(y_train, forecast_steps)\n",
        "        predictions.update(adaptive_results)\n",
        "\n",
        "        # Add best baseline models\n",
        "        baseline = BaselineModels()\n",
        "        baseline.fit_naive(y_train)\n",
        "        baseline.fit_seasonal_naive(y_train, season_length=7)\n",
        "        predictions['naive'] = baseline.predict(forecast_steps, 'naive')\n",
        "        predictions['seasonal_naive'] = baseline.predict(forecast_steps, 'seasonal_naive')\n",
        "\n",
        "        # Get seasonal naive for relative comparison\n",
        "        seasonal_naive_pred = predictions['seasonal_naive']\n",
        "\n",
        "        # Create ensembles focused on recent performance\n",
        "        pred_array = np.array([pred for pred in predictions.values() if len(pred) == forecast_steps])\n",
        "\n",
        "        if len(pred_array) > 0:\n",
        "            # Median ensemble (robust to outliers - important for regime changes)\n",
        "            ensemble_median = np.median(pred_array, axis=0)\n",
        "            metrics = evaluate_forecast_relative(y_test, ensemble_median, seasonal_naive_pred, \"ensemble_adaptive_median\")\n",
        "            metrics['split'] = split_idx + 1\n",
        "            ensemble_results.append(metrics)\n",
        "\n",
        "            # Weighted ensemble based on recent performance only\n",
        "            if split_idx > 0:\n",
        "                try:\n",
        "                    # Use only adaptive models for weighting\n",
        "                    adaptive_model_names = [name for name in predictions.keys() if 'adaptive' in name]\n",
        "                    if len(adaptive_model_names) > 0:\n",
        "                        weights = np.ones(len(adaptive_model_names)) / len(adaptive_model_names)  # Equal weights\n",
        "                        adaptive_preds = [predictions[name] for name in adaptive_model_names]\n",
        "\n",
        "                        ensemble_adaptive = np.average(adaptive_preds, axis=0, weights=weights)\n",
        "                        metrics = evaluate_forecast_relative(y_test, ensemble_adaptive, seasonal_naive_pred, \"ensemble_adaptive_only\")\n",
        "                        metrics['split'] = split_idx + 1\n",
        "                        ensemble_results.append(metrics)\n",
        "                except Exception as e:\n",
        "                    print(f\"    Adaptive ensemble failed: {e}\")\n",
        "\n",
        "    # Combine results\n",
        "    if ensemble_results:\n",
        "        ensemble_df = pd.DataFrame(ensemble_results)\n",
        "        combined_results = pd.concat([results_df, ensemble_df], ignore_index=True)\n",
        "\n",
        "        # Recalculate averages\n",
        "        avg_with_ensemble = combined_results.groupby('model').agg({\n",
        "            'mae': ['mean', 'std'],\n",
        "            'mape': ['mean', 'std'],\n",
        "            'rmse': ['mean', 'std'],\n",
        "            'r2': ['mean', 'std'],\n",
        "            'improvement_pct': ['mean', 'std'],\n",
        "            'mape_vs_baseline': ['mean', 'std']\n",
        "        }).round(3)\n",
        "\n",
        "        avg_with_ensemble.columns = [f\"{metric}_{stat}\" for metric, stat in avg_with_ensemble.columns]\n",
        "        avg_with_ensemble = avg_with_ensemble.sort_values('improvement_pct_mean', ascending=False)\n",
        "\n",
        "        print(\"\\nðŸ† FINAL RANKINGS WITH ADAPTIVE ENSEMBLES\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Business-friendly display with ensembles\n",
        "        display_results_ensemble = pd.DataFrame({\n",
        "            'Model': avg_with_ensemble.index,\n",
        "            'MAPE (%)': avg_with_ensemble['mape_mean'].round(1),\n",
        "            'vs Seasonal Naive': avg_with_ensemble['improvement_pct_mean'].round(1).astype(str) + '%',\n",
        "            'Consistency (Â± std)': avg_with_ensemble['improvement_pct_std'].round(1).astype(str) + '%',\n",
        "            'Status': ['Better' if x > 0 else 'Worse' for x in avg_with_ensemble['improvement_pct_mean']]\n",
        "        })\n",
        "\n",
        "        print(display_results_ensemble.to_string(index=False))\n",
        "\n",
        "        # Highlight ensemble performance\n",
        "        ensemble_models = [idx for idx in avg_with_ensemble.index if 'ensemble' in idx]\n",
        "        if ensemble_models:\n",
        "            print(f\"\\nðŸ”— ENSEMBLE PERFORMANCE:\")\n",
        "            for model in ensemble_models:\n",
        "                improvement = avg_with_ensemble.loc[model, 'improvement_pct_mean']\n",
        "                print(f\"   {model}: {improvement:.1f}% vs seasonal naive\")\n",
        "\n",
        "        return combined_results, avg_with_ensemble\n",
        "\n",
        "    return results_df, avg_results if 'avg_results' in globals() else None\n",
        "\n",
        "# Generate final recommendations\n",
        "generate_recommendations()\n",
        "\n",
        "print(f\"\\nðŸŽ‰ ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Ready to forecast the future! ðŸ“žðŸ“ˆ\")"
      ],
      "metadata": {
        "id": "LbZfcZUDqfVT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}