{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/CB_Step_5_Deep_Learning_and_Hybrid_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn tensorflow prophet xgboost neuralprophet"
      ],
      "metadata": {
        "id": "VA1lpScxlO8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from prophet import Prophet\n",
        "from xgboost import XGBRegressor\n",
        "from neuralprophet import NeuralProphet\n",
        "import torch  # Added for safe_globals\n",
        "\n",
        "# Fix for UnpicklingError: Import the actual class and add to safe_globals\n",
        "from neuralprophet.configure import ConfigSeasonality\n",
        "torch.serialization.add_safe_globals([ConfigSeasonality])\n",
        "\n",
        "# Step 1: Load the data from CSV file with explicit date format to suppress parsing warning\n",
        "df = pd.read_csv('enhanced_eda_data.csv', parse_dates=['date'], date_format='%m/%d/%y', index_col='date')\n",
        "\n",
        "# Assume 'Calls' is the target column\n",
        "target = 'calls'\n",
        "\n",
        "# Prepare data: Sort by date if not already\n",
        "df = df.sort_index()\n",
        "\n",
        "# Feature Engineering (similar to ML: lags, rollings, dummies)\n",
        "df['Lag1'] = df[target].shift(1)\n",
        "df['Lag7'] = df[target].shift(7)\n",
        "df['Rolling_Mean_7'] = df[target].rolling(window=7).mean()\n",
        "df['Rolling_Std_7'] = df[target].rolling(window=7).std()\n",
        "df = pd.get_dummies(df, columns=['DayOfWeek'], drop_first=True)\n",
        "\n",
        "# Select features (numeric except target)\n",
        "features = [col for col in df.columns if col != target and df[col].dtype in [np.float64, np.int64, bool]]\n",
        "\n",
        "# Drop NaNs\n",
        "df = df.dropna()\n",
        "\n",
        "# For DL models, scale data\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df[features + [target]]), index=df.index, columns=features + [target])\n",
        "\n",
        "# Time series cross-validation: 5 splits\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Function to calculate metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100  # As percentage\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
        "\n",
        "# Dictionary to store average metrics for each model\n",
        "model_metrics = {}\n",
        "\n",
        "# Helper to create sequences for LSTM (timesteps=7)\n",
        "def create_sequences(data, timesteps=7):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(data) - timesteps):\n",
        "        X_seq.append(data.iloc[i:i+timesteps].values)\n",
        "        y_seq.append(data.iloc[i+timesteps][target])  # Predict next value\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# 1. LSTM Network\n",
        "lstm_preds = []\n",
        "lstm_trues = []\n",
        "for train_idx, test_idx in tscv.split(df_scaled):\n",
        "    train = df_scaled.iloc[train_idx]\n",
        "    test = df_scaled.iloc[test_idx]\n",
        "\n",
        "    # Create sequences\n",
        "    X_train_seq, y_train_seq = create_sequences(train)\n",
        "    X_test_seq, y_test_seq = create_sequences(test)\n",
        "\n",
        "    # Build LSTM model with Input layer to suppress warning\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(7, len(train.columns))))\n",
        "    model.add(LSTM(50, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Fit (with verbose=1 for debugging if needed; set to 0 for silent)\n",
        "    model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predict (adjust for sequence length)\n",
        "    pred = model.predict(X_test_seq, verbose=0).flatten()\n",
        "\n",
        "    lstm_preds.extend(pred)\n",
        "    lstm_trues.extend(y_test_seq)\n",
        "\n",
        "# Inverse scale predictions and trues\n",
        "lstm_preds_inv = scaler.inverse_transform(np.concatenate((np.zeros((len(lstm_preds), len(features))), np.array(lstm_preds).reshape(-1,1)), axis=1))[:, -1]\n",
        "lstm_trues_inv = scaler.inverse_transform(np.concatenate((np.zeros((len(lstm_trues), len(features))), np.array(lstm_trues).reshape(-1,1)), axis=1))[:, -1]\n",
        "\n",
        "lstm_metrics = calculate_metrics(lstm_trues_inv, lstm_preds_inv)\n",
        "model_metrics['LSTM'] = lstm_metrics\n",
        "\n",
        "# 2. Neural Prophet (with matplotlib backend to avoid Plotly error)\n",
        "np_preds = []\n",
        "np_trues = []\n",
        "for train_idx, test_idx in tscv.split(df):\n",
        "    train_df = df.iloc[train_idx].reset_index().rename(columns={'date': 'ds', 'calls': 'y'})\n",
        "    test_df = df.iloc[test_idx].reset_index().rename(columns={'date': 'ds', 'calls': 'y'})\n",
        "\n",
        "    # Fit NeuralProphet with matplotlib and explicit seasonalities\n",
        "    model = NeuralProphet(epochs=50, batch_size=32, learning_rate=0.01,  # Manual LR to avoid finder warning\n",
        "                          yearly_seasonality=False, daily_seasonality=False)  # Explicit to suppress auto warnings\n",
        "    model.set_plotting_backend('matplotlib')\n",
        "    model.fit(train_df[['ds', 'y']], freq='D')\n",
        "\n",
        "    # Make future dataframe\n",
        "    future = model.make_future_dataframe(train_df[['ds', 'y']], periods=len(test_df))\n",
        "\n",
        "    # Predict\n",
        "    forecast = model.predict(future)\n",
        "    pred = forecast['yhat1'].tail(len(test_df)).values\n",
        "\n",
        "    np_preds.extend(pred)\n",
        "    np_trues.extend(test_df['y'])\n",
        "\n",
        "np_metrics = calculate_metrics(np_trues, np_preds)\n",
        "model_metrics['Neural Prophet'] = np_metrics\n",
        "\n",
        "# 3. Hybrid: Prophet + XGBoost (Prophet for trend/seasonal, XGBoost on residuals)\n",
        "hybrid_preds = []\n",
        "hybrid_trues = []\n",
        "for train_idx, test_idx in tscv.split(df):\n",
        "    train_df = df.iloc[train_idx].reset_index().rename(columns={'date': 'ds', 'calls': 'y'})\n",
        "    test_df = df.iloc[test_idx].reset_index().rename(columns={'date': 'ds', 'calls': 'y'})\n",
        "\n",
        "    # Step 1: Fit Prophet\n",
        "    prophet_model = Prophet(weekly_seasonality=True)\n",
        "    prophet_model.fit(train_df[['ds', 'y']])\n",
        "\n",
        "    # Predict on train and test\n",
        "    train_future = prophet_model.make_future_dataframe(periods=0)\n",
        "    train_forecast = prophet_model.predict(train_future)\n",
        "    train_residuals = train_df['y'] - train_forecast['yhat']\n",
        "\n",
        "    test_future = prophet_model.make_future_dataframe(periods=len(test_df))\n",
        "    test_forecast = prophet_model.predict(test_future)\n",
        "    prophet_test_pred = test_forecast['yhat'].tail(len(test_df))\n",
        "\n",
        "    # Step 2: Fit XGBoost on residuals using features\n",
        "    X_train = train_df[features]\n",
        "    y_res_train = train_residuals\n",
        "\n",
        "    xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "    xgb_model.fit(X_train, y_res_train)\n",
        "\n",
        "    # Predict residuals on test\n",
        "    X_test = test_df[features]\n",
        "    res_pred = xgb_model.predict(X_test)\n",
        "\n",
        "    # Combine: Prophet pred + residual pred\n",
        "    pred = prophet_test_pred + res_pred\n",
        "\n",
        "    hybrid_preds.extend(pred)\n",
        "    hybrid_trues.extend(test_df['y'])\n",
        "\n",
        "hybrid_metrics = calculate_metrics(hybrid_trues, hybrid_preds)\n",
        "model_metrics['Prophet + XGBoost Hybrid'] = hybrid_metrics\n",
        "\n",
        "# Summarize performance\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "metrics_df = pd.DataFrame(model_metrics).T\n",
        "print(metrics_df)\n",
        "\n",
        "# Pick winner: Lowest MAE (primary metric)\n",
        "winner = metrics_df['MAE'].idxmin()\n",
        "print(f\"\\nChampion DL/Hybrid Model: {winner}\")\n",
        "print(f\"Metrics: {metrics_df.loc[winner].to_dict()}\")"
      ],
      "metadata": {
        "id": "HakXAQA2pStF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for interpretation\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "# Suppress a common SHAP warning for TensorFlow\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='shap.explainers._deep.deep_tf')\n",
        "\n",
        "print(\"--- Preparing data and models for interpretation ---\")\n",
        "\n",
        "# 1. Create a final, persistent train/test split from the last CV fold\n",
        "# This ensures we have X_train, X_test, etc., available outside of a loop.\n",
        "all_splits = list(tscv.split(df))\n",
        "train_idx, test_idx = all_splits[-1] # Use the last split\n",
        "\n",
        "# Original unscaled data split\n",
        "train_df_final = df.iloc[train_idx]\n",
        "test_df_final = df.iloc[test_idx]\n",
        "\n",
        "# Scaled data split for LSTM\n",
        "train_scaled_final = df_scaled.iloc[train_idx]\n",
        "test_scaled_final = df_scaled.iloc[test_idx]\n",
        "\n",
        "# Prepare data for models that need specific column names ('ds', 'y')\n",
        "train_prophet_format = train_df_final.reset_index().rename(columns={'date': 'ds', 'calls': 'y'})\n",
        "test_prophet_format = test_df_final.reset_index().rename(columns={'date': 'ds', 'calls': 'y'})\n",
        "\n",
        "\n",
        "# --- Re-train the models one last time on the final split ---\n",
        "\n",
        "# 2a. Train Final LSTM Model\n",
        "print(\"\\nTraining final LSTM model...\")\n",
        "X_train_seq, y_train_seq = create_sequences(train_scaled_final)\n",
        "X_test_seq, y_test_seq = create_sequences(test_scaled_final) # This will be our test set for SHAP\n",
        "\n",
        "final_lstm_model = Sequential()\n",
        "final_lstm_model.add(Input(shape=(7, len(train_scaled_final.columns))))\n",
        "final_lstm_model.add(LSTM(50, activation='relu'))\n",
        "final_lstm_model.add(Dense(1))\n",
        "final_lstm_model.compile(optimizer='adam', loss='mse')\n",
        "final_lstm_model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=32, verbose=0)\n",
        "print(\"LSTM training complete.\")\n",
        "\n",
        "\n",
        "# 2b. Train Final Neural Prophet Model\n",
        "print(\"\\nTraining final Neural Prophet model...\")\n",
        "final_np_model = NeuralProphet(epochs=50, batch_size=32, learning_rate=0.01,\n",
        "                               yearly_seasonality=False, daily_seasonality=False)\n",
        "final_np_model.set_plotting_backend('matplotlib')\n",
        "final_np_model.fit(train_prophet_format[['ds', 'y']], freq='D')\n",
        "print(\"Neural Prophet training complete.\")\n",
        "\n",
        "\n",
        "# 2c. Train Final Prophet + XGBoost Hybrid Model\n",
        "print(\"\\nTraining final Prophet + XGBoost Hybrid model...\")\n",
        "# Prophet part\n",
        "final_prophet_model = Prophet(weekly_seasonality=True)\n",
        "final_prophet_model.fit(train_prophet_format[['ds', 'y']])\n",
        "\n",
        "# Create Prophet forecast to calculate residuals for XGBoost\n",
        "train_forecast_prophet = final_prophet_model.predict(train_prophet_format[['ds']])\n",
        "train_residuals = train_prophet_format['y'] - train_forecast_prophet['yhat']\n",
        "\n",
        "# Create the forecast object needed for plotting components later\n",
        "future_df = final_prophet_model.make_future_dataframe(periods=len(test_prophet_format))\n",
        "final_forecast_object = final_prophet_model.predict(future_df)\n",
        "\n",
        "# XGBoost part (training on residuals)\n",
        "X_train_xgb = train_df_final[features]\n",
        "X_test_xgb = test_df_final[features] # This will be our test set for SHAP\n",
        "\n",
        "final_xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "final_xgb_model.fit(X_train_xgb, train_residuals)\n",
        "print(\"Hybrid model training complete.\")\n",
        "\n",
        "\n",
        "# --- Run Interpretability Analysis ---\n",
        "print(\"\\n--- Generating Model Interpretation Plots ---\")\n",
        "\n",
        "# 1. LSTM Interpretability with SHAP\n",
        "try:\n",
        "    # Use a subset of the training data as the background for the explainer\n",
        "    background_data = X_train_seq[:100]\n",
        "    explainer_lstm = shap.DeepExplainer(final_lstm_model, background_data)\n",
        "    shap_values_lstm = explainer_lstm.shap_values(X_test_seq)\n",
        "\n",
        "    # SHAP expects a 2D array for plotting, but LSTM values are 3D (samples, timesteps, features)\n",
        "    # We can average the SHAP values over the timesteps for a summary\n",
        "    shap_values_lstm_avg = np.mean(shap_values_lstm[0], axis=1)\n",
        "\n",
        "    # We need a 2D version of X_test_seq for the plot's feature values\n",
        "    X_test_for_plot = np.mean(X_test_seq, axis=1)\n",
        "\n",
        "    print(\"\\nDisplaying SHAP plot for LSTM...\")\n",
        "    shap.summary_plot(shap_values_lstm_avg, pd.DataFrame(X_test_for_plot, columns=df_scaled.columns),\n",
        "                      show=False)\n",
        "    plt.title(\"SHAP Feature Importance for LSTM (Averaged over Timesteps)\")\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during LSTM SHAP analysis: {e}\")\n",
        "\n",
        "\n",
        "# 2. NeuralProphet Interpretability\n",
        "try:\n",
        "    print(\"\\nDisplaying plot for Neural Prophet...\")\n",
        "    fig_params = final_np_model.plot_parameters()\n",
        "    plt.title(\"NeuralProphet Component Contributions\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during NeuralProphet plot generation: {e}\")\n",
        "\n",
        "\n",
        "# 3. Prophet + XGBoost Hybrid Interpretability\n",
        "# 3a. Prophet Components\n",
        "try:\n",
        "    print(\"\\nDisplaying component plot for Prophet (from Hybrid)...\")\n",
        "    fig_components = final_prophet_model.plot_components(final_forecast_object)\n",
        "    plt.suptitle(\"Prophet Component Contributions in Hybrid Model\", y=1.02)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Prophet components plot generation: {e}\")\n",
        "\n",
        "\n",
        "# 3b. XGBoost with SHAP (more insightful than the basic importance plot)\n",
        "try:\n",
        "    print(\"\\nDisplaying SHAP plot for XGBoost (from Hybrid)...\")\n",
        "    explainer_xgb = shap.TreeExplainer(final_xgb_model)\n",
        "    shap_values_xgb = explainer_xgb.shap_values(X_test_xgb)\n",
        "\n",
        "    shap.summary_plot(shap_values_xgb, X_test_xgb, show=False)\n",
        "    plt.title(\"SHAP Feature Importance for XGBoost in Hybrid Model\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during XGBoost SHAP analysis: {e}\")"
      ],
      "metadata": {
        "id": "diAGkJIKy40U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}