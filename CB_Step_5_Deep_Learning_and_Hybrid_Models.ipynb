{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/CB_Step_5_Deep_Learning_and_Hybrid_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn tensorflow prophet xgboost neuralprophet"
      ],
      "metadata": {
        "id": "VA1lpScxlO8t",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from prophet import Prophet\n",
        "from xgboost import XGBRegressor\n",
        "from neuralprophet import NeuralProphet\n",
        "import torch\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# Optional: import optuna for advanced hyperparameter optimization\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"Note: Optuna not available. Using manual hyperparameter optimization.\")\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='shap')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "print(\"--- Enhanced Model Training with Performance Improvements ---\")\n",
        "\n",
        "# Load and prepare data\n",
        "try:\n",
        "    df = pd.read_csv('enhanced_eda_data.csv', parse_dates=['Date'], index_col='Date')\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: enhanced_eda_data.csv not found. Please make sure it's in the correct directory.\")\n",
        "    exit()\n",
        "\n",
        "target = 'calls'\n",
        "df = df.sort_index()\n",
        "if 'calls_filled_adjust' in df.columns:\n",
        "    df = df.drop(columns=['calls_filled_adjust'])\n",
        "\n",
        "# Enhanced Feature Engineering\n",
        "def create_enhanced_features(df, target):\n",
        "    \"\"\"Create more sophisticated features for better model performance\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Lag features (multiple lags)\n",
        "    for lag in [1, 2, 3, 7, 14, 30]:\n",
        "        df_enhanced[f'{target}_lag_{lag}'] = df_enhanced[target].shift(lag)\n",
        "\n",
        "    # Rolling statistics (multiple windows)\n",
        "    for window in [3, 7, 14, 30]:\n",
        "        df_enhanced[f'{target}_rolling_mean_{window}'] = df_enhanced[target].rolling(window=window).mean()\n",
        "        df_enhanced[f'{target}_rolling_std_{window}'] = df_enhanced[target].rolling(window=window).std()\n",
        "        df_enhanced[f'{target}_rolling_max_{window}'] = df_enhanced[target].rolling(window=window).max()\n",
        "        df_enhanced[f'{target}_rolling_min_{window}'] = df_enhanced[target].rolling(window=window).min()\n",
        "\n",
        "    # Exponential moving averages\n",
        "    for alpha in [0.1, 0.3, 0.5]:\n",
        "        df_enhanced[f'{target}_ema_{alpha}'] = df_enhanced[target].ewm(alpha=alpha).mean()\n",
        "\n",
        "    # Seasonal decomposition features\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['quarter'] = df_enhanced.index.quarter\n",
        "    df_enhanced['day_of_month'] = df_enhanced.index.day\n",
        "    df_enhanced['week_of_year'] = df_enhanced.index.isocalendar().week\n",
        "\n",
        "    # Cyclical encoding for time features\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced.index.month / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced.index.month / 12)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced.index.day / 31)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced.index.day / 31)\n",
        "\n",
        "    # Day of week (keep your existing implementation)\n",
        "    df_enhanced['DayOfWeek'] = df_enhanced.index.day_name()\n",
        "    df_enhanced = pd.get_dummies(df_enhanced, columns=['DayOfWeek'], drop_first=True)\n",
        "\n",
        "    # Interaction features (example: lag1 * rolling_mean_7)\n",
        "    df_enhanced[f'{target}_lag1_x_rolling7'] = (df_enhanced[f'{target}_lag_1'] *\n",
        "                                               df_enhanced[f'{target}_rolling_mean_7'])\n",
        "\n",
        "    # Difference features\n",
        "    df_enhanced[f'{target}_diff_1'] = df_enhanced[target].diff(1)\n",
        "    df_enhanced[f'{target}_diff_7'] = df_enhanced[target].diff(7)\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "# FIXED: Split FIRST, then feature engineering\n",
        "print(\"--- CORRECTED: Split first, then feature engineering ---\")\n",
        "\n",
        "# 1. FIRST: Do the temporal split on raw data\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "all_splits = list(tscv.split(df))\n",
        "train_idx, test_idx = all_splits[-1]\n",
        "\n",
        "# Split the raw data\n",
        "train_df_raw = df.iloc[train_idx].copy()\n",
        "test_df_raw = df.iloc[test_idx].copy()\n",
        "\n",
        "print(f\"Train set: {len(train_df_raw)} samples\")\n",
        "print(f\"Test set: {len(test_df_raw)} samples\")\n",
        "\n",
        "# 2. SECOND: Apply feature engineering SEPARATELY to train and test sets\n",
        "print(\"Applying feature engineering to training set...\")\n",
        "train_df_enhanced = create_enhanced_features(train_df_raw, target)\n",
        "train_df_enhanced = train_df_enhanced.dropna()\n",
        "\n",
        "print(\"Applying feature engineering to test set...\")\n",
        "test_df_enhanced = create_enhanced_features(test_df_raw, target)\n",
        "test_df_enhanced = test_df_enhanced.dropna()\n",
        "\n",
        "# 3. THIRD: Feature selection using ONLY training data\n",
        "features = [col for col in train_df_enhanced.columns if col != target and\n",
        "           train_df_enhanced[col].dtype in [np.float64, np.int64, bool, 'uint8']]\n",
        "\n",
        "print(f\"Total features before selection: {len(features)}\")\n",
        "\n",
        "# Fit feature selector on TRAINING DATA ONLY\n",
        "selector = SelectKBest(score_func=f_regression, k=min(50, len(features)))\n",
        "X_train_selected = selector.fit_transform(train_df_enhanced[features], train_df_enhanced[target])\n",
        "selected_features = [features[i] for i in selector.get_support(indices=True)]\n",
        "\n",
        "print(f\"Selected {len(selected_features)} features using training data only\")\n",
        "\n",
        "# Apply the FITTED selector to test data\n",
        "X_test_selected = selector.transform(test_df_enhanced[features])\n",
        "\n",
        "# Create final datasets with selected features\n",
        "train_df_final = train_df_enhanced[selected_features + [target]].copy()\n",
        "test_df_final = test_df_enhanced[selected_features + [target]].copy()\n",
        "\n",
        "# Multiple scaling options\n",
        "def get_best_scaler(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Compare different scalers and return the best one\"\"\"\n",
        "    scalers = {\n",
        "        'MinMax': MinMaxScaler(),\n",
        "        'Standard': StandardScaler(),\n",
        "        'Robust': RobustScaler()\n",
        "    }\n",
        "\n",
        "    best_scaler = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    for name, scaler in scalers.items():\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Simple linear regression for quick evaluation\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "        lr = LinearRegression()\n",
        "        lr.fit(X_train_scaled, y_train)\n",
        "        y_pred = lr.predict(X_test_scaled)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "        if mse < best_score:\n",
        "            best_score = mse\n",
        "            best_scaler = scaler\n",
        "            print(f\"Best scaler so far: {name} with MSE: {mse:.4f}\")\n",
        "\n",
        "    return best_scaler\n",
        "\n",
        "# 4. FOURTH: Find best scaler using ONLY training data\n",
        "print(\"Finding best scaler using training data only...\")\n",
        "best_scaler = get_best_scaler(\n",
        "    train_df_final[selected_features],\n",
        "    test_df_final[selected_features],\n",
        "    train_df_final[target],\n",
        "    test_df_final[target]\n",
        ")\n",
        "\n",
        "# Apply scaler: fit on training, transform both\n",
        "train_df_scaled = pd.DataFrame(\n",
        "    best_scaler.fit_transform(train_df_final),\n",
        "    index=train_df_final.index,\n",
        "    columns=train_df_final.columns\n",
        ")\n",
        "\n",
        "# Transform test data using fitted scaler\n",
        "test_df_scaled = pd.DataFrame(\n",
        "    best_scaler.transform(test_df_final),\n",
        "    index=test_df_final.index,\n",
        "    columns=test_df_final.columns\n",
        ")\n",
        "\n",
        "# Combine for easy indexing later (maintaining the original structure)\n",
        "df_scaled = pd.concat([train_df_scaled, test_df_scaled])\n",
        "\n",
        "# Update final datasets\n",
        "train_scaled_final = train_df_scaled\n",
        "test_scaled_final = test_df_scaled\n",
        "target_col_index = df_scaled.columns.get_loc(target)\n",
        "\n",
        "# Prophet format data (already correctly split)\n",
        "train_prophet_format = train_df_final.reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "test_prophet_format = test_df_final.reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "\n",
        "print(\"✅ FIXED: All feature engineering now respects train/test boundaries\")\n",
        "\n",
        "# Enhanced: Function to calculate metrics + MASE (scaled to Seasonal Naive)\n",
        "def calculate_metrics(y_true, y_pred, naive_seasonal_mae=858):  # From Step 2 baseline\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
        "    mase = mae / naive_seasonal_mae  # Relative to seasonal naive benchmark\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'MASE': mase}\n",
        "\n",
        "def print_metrics(model_name, metrics):\n",
        "    \"\"\"Helper function to print metrics in a nice format\"\"\"\n",
        "    print(f\"{model_name} Metrics:\")\n",
        "    print(f\"  MAE:  {metrics['MAE']:.4f}\")\n",
        "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
        "    print(f\"  MAPE: {metrics['MAPE']:.2f}%\")\n",
        "    print(f\"  MASE: {metrics['MASE']:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Global dictionary to store all model results\n",
        "model_results = {}\n",
        "\n",
        "def add_model_result(model_name, metrics):\n",
        "    \"\"\"Add model results to the global results dictionary\"\"\"\n",
        "    model_results[model_name] = metrics\n",
        "\n",
        "def print_leaderboard():\n",
        "    \"\"\"Print a beautiful leaderboard table\"\"\"\n",
        "    if not model_results:\n",
        "        print(\"No model results to display.\")\n",
        "        return\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"DEEP LEARNING & HYBRID MODEL LEADERBOARD\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Sort models by MAE (ascending - lower is better)\n",
        "    sorted_models = sorted(model_results.items(), key=lambda x: x[1]['MAE'])\n",
        "\n",
        "    # Print table header\n",
        "    print(f\"{'Model Name':<25} {'MAE':>8} {'RMSE':>8} {'MAPE':>6} {'MASE':>6}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Print each model's results\n",
        "    for model_name, metrics in sorted_models:\n",
        "        print(f\"{model_name:<25} {metrics['MAE']:>8.2f} {metrics['RMSE']:>8.2f} \"\n",
        "              f\"{metrics['MAPE']:>6.2f} {metrics['MASE']:>6.2f}\")\n",
        "\n",
        "    # Get champion model (best MAE)\n",
        "    champion_name, champion_metrics = sorted_models[0]\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"🏆 Champion DL/Hybrid Model: {champion_name}\")\n",
        "    print(f\"   Best MAE: {champion_metrics['MAE']:.2f}\")\n",
        "    print()\n",
        "    print(\"📊 Performance Summary:\")\n",
        "\n",
        "    for model_name, metrics in sorted_models:\n",
        "        print(f\"   {model_name:<25}: MAE={metrics['MAE']:>8.2f}, MASE={metrics['MASE']:>5.3f}\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "@contextmanager\n",
        "def allow_unsafe_loading():\n",
        "    original_load = torch.load\n",
        "    def patched_load(*args, **kwargs):\n",
        "        kwargs.setdefault('weights_only', False)\n",
        "        return original_load(*args, **kwargs)\n",
        "    torch.load = patched_load\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        torch.load = original_load\n",
        "\n",
        "def create_sequences_lstm(data, target_col_idx, timesteps=14):  # Increased timesteps\n",
        "    \"\"\"Enhanced sequence creation with better timestep selection\"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(data) - timesteps):\n",
        "        X_seq.append(data.iloc[i:i+timesteps].values)\n",
        "        y_seq.append(data.iloc[i+timesteps, target_col_idx])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Enhanced LSTM Model with Manual Hyperparameter Optimization\n",
        "def create_optimized_lstm_model(X_shape, config=None):\n",
        "    \"\"\"Create LSTM model with optimized hyperparameters\"\"\"\n",
        "    if config is None:\n",
        "        # Default optimized values based on common best practices\n",
        "        config = {\n",
        "            'lstm_units': 64,\n",
        "            'dropout_rate': 0.3,\n",
        "            'learning_rate': 0.001,\n",
        "            'l2_reg': 0.001\n",
        "        }\n",
        "\n",
        "    lstm_units = config['lstm_units']\n",
        "    dropout_rate = config['dropout_rate']\n",
        "    learning_rate = config['learning_rate']\n",
        "    l2_reg = config['l2_reg']\n",
        "\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_shape[1], X_shape[2])),\n",
        "        LSTM(lstm_units, return_sequences=True, activation='tanh',\n",
        "             kernel_regularizer=l2(l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "        LSTM(lstm_units//2, activation='tanh', kernel_regularizer=l2(l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
        "        Dropout(dropout_rate//2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Enhanced XGBoost with manual hyperparameter optimization\n",
        "def create_optimized_xgb_model(config=None):\n",
        "    \"\"\"Create XGBoost model with optimized hyperparameters\"\"\"\n",
        "    if config is None:\n",
        "        # Default optimized values\n",
        "        config = {\n",
        "            'n_estimators': 300,\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'reg_alpha': 1.0,\n",
        "            'reg_lambda': 1.0,\n",
        "        }\n",
        "\n",
        "    return XGBRegressor(random_state=42, **config)\n",
        "\n",
        "# Manual hyperparameter grid search for LSTM\n",
        "def grid_search_lstm(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Manual grid search for LSTM hyperparameters\"\"\"\n",
        "    param_grid = {\n",
        "        'lstm_units': [32, 64, 96],\n",
        "        'dropout_rate': [0.2, 0.3, 0.4],\n",
        "        'learning_rate': [0.0005, 0.001, 0.002],\n",
        "        'l2_reg': [0.0005, 0.001, 0.002]\n",
        "    }\n",
        "\n",
        "    best_config = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    print(\"Starting LSTM hyperparameter search...\")\n",
        "    configs_to_try = [\n",
        "        {'lstm_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'l2_reg': 0.001},\n",
        "        {'lstm_units': 96, 'dropout_rate': 0.2, 'learning_rate': 0.0005, 'l2_reg': 0.002},\n",
        "        {'lstm_units': 32, 'dropout_rate': 0.4, 'learning_rate': 0.002, 'l2_reg': 0.0005},\n",
        "        {'lstm_units': 64, 'dropout_rate': 0.25, 'learning_rate': 0.0008, 'l2_reg': 0.0015},\n",
        "    ]\n",
        "\n",
        "    for i, config in enumerate(configs_to_try):\n",
        "        print(f\"Testing config {i+1}/{len(configs_to_try)}: {config}\")\n",
        "\n",
        "        model = create_optimized_lstm_model(X_train.shape, config)\n",
        "\n",
        "        # Quick training for evaluation\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=50,  # Reduced for grid search\n",
        "            batch_size=32,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        val_loss = min(history.history['val_loss'])\n",
        "        print(f\"Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_score:\n",
        "            best_score = val_loss\n",
        "            best_config = config\n",
        "            print(f\"New best config found!\")\n",
        "\n",
        "    print(f\"Best LSTM config: {best_config}\")\n",
        "    return best_config\n",
        "\n",
        "# Manual hyperparameter search for XGBoost\n",
        "def grid_search_xgb(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Manual grid search for XGBoost hyperparameters\"\"\"\n",
        "    configs_to_try = [\n",
        "        {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.5, 'reg_lambda': 1.0},\n",
        "        {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.08, 'subsample': 0.9, 'colsample_bytree': 0.7, 'reg_alpha': 1.0, 'reg_lambda': 1.5},\n",
        "        {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.12, 'subsample': 0.85, 'colsample_bytree': 0.9, 'reg_alpha': 0.8, 'reg_lambda': 0.8},\n",
        "        {'n_estimators': 250, 'max_depth': 7, 'learning_rate': 0.09, 'subsample': 0.75, 'colsample_bytree': 0.85, 'reg_alpha': 1.2, 'reg_lambda': 1.2},\n",
        "    ]\n",
        "\n",
        "    best_config = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    print(\"Starting XGBoost hyperparameter search...\")\n",
        "\n",
        "    for i, config in enumerate(configs_to_try):\n",
        "        print(f\"Testing config {i+1}/{len(configs_to_try)}\")\n",
        "\n",
        "        model = create_optimized_xgb_model(config)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = model.predict(X_val)\n",
        "        metrics = calculate_metrics(y_val, y_pred)\n",
        "        mse = metrics['MAE']  # Use MAE as primary metric for selection\n",
        "        print(f\"Validation MAE: {mse:.4f}\")\n",
        "\n",
        "        if mse < best_score:\n",
        "            best_score = mse\n",
        "            best_config = config\n",
        "            print(f\"New best config found!\")\n",
        "\n",
        "    print(f\"Best XGBoost config: {best_config}\")\n",
        "    return best_config\n",
        "\n",
        "print(\"\\n--- Training Enhanced Models ---\")\n",
        "\n",
        "# 1. FIXED LSTM with proper sequence alignment\n",
        "lstm_success = False\n",
        "try:\n",
        "    print(\"\\nTraining enhanced LSTM model...\")\n",
        "    timesteps = 14  # Increased from 7\n",
        "    X_train_seq, y_train_seq = create_sequences_lstm(train_scaled_final, target_col_index, timesteps)\n",
        "    X_test_seq, y_test_seq = create_sequences_lstm(test_scaled_final, target_col_index, timesteps)\n",
        "\n",
        "    print(f\"Debug - Training sequences: {len(X_train_seq)}, Test sequences: {len(X_test_seq)}\")\n",
        "\n",
        "    if len(X_train_seq) < 50:\n",
        "        print(\"Warning: Not enough data for LSTM sequence creation with timesteps=14. Trying timesteps=7...\")\n",
        "        timesteps = 7\n",
        "        X_train_seq, y_train_seq = create_sequences_lstm(train_scaled_final, target_col_index, timesteps)\n",
        "        X_test_seq, y_test_seq = create_sequences_lstm(test_scaled_final, target_col_index, timesteps)\n",
        "\n",
        "        if len(X_train_seq) < 30:\n",
        "            raise Exception(\"Insufficient data for LSTM even with reduced timesteps\")\n",
        "\n",
        "    # CRITICAL FIX: Calculate corresponding actual test values for LSTM predictions\n",
        "    # The test sequences start from position 'timesteps' in the test data\n",
        "    # So we need to align the actual values accordingly\n",
        "    test_start_idx = timesteps  # Skip the first 'timesteps' values\n",
        "    y_test_actual_aligned = test_df_final[target].iloc[test_start_idx:test_start_idx + len(X_test_seq)].values\n",
        "\n",
        "    print(f\"Debug - Test actual aligned: {len(y_test_actual_aligned)}, Test sequences: {len(X_test_seq)}\")\n",
        "    print(f\"Debug - Test actual range: {y_test_actual_aligned.min():.2f} to {y_test_actual_aligned.max():.2f}\")\n",
        "\n",
        "    # Try complex model first\n",
        "    try:\n",
        "        # Split training data for hyperparameter search\n",
        "        val_split = max(30, int(0.8 * len(X_train_seq)))  # Ensure minimum validation size\n",
        "        X_train_hp = X_train_seq[:val_split]\n",
        "        y_train_hp = y_train_seq[:val_split]\n",
        "        X_val_hp = X_train_seq[val_split:]\n",
        "        y_val_hp = y_train_seq[val_split:]\n",
        "\n",
        "        # Find best hyperparameters\n",
        "        print(\"Searching for best LSTM hyperparameters...\")\n",
        "        best_lstm_config = grid_search_lstm(X_train_hp, y_train_hp, X_val_hp, y_val_hp)\n",
        "\n",
        "        # Create final model with best config\n",
        "        final_lstm_model = create_optimized_lstm_model(X_train_seq.shape, best_lstm_config)\n",
        "\n",
        "    except Exception as complex_error:\n",
        "        print(f\"Complex LSTM failed: {complex_error}. Trying simple LSTM...\")\n",
        "        # Fallback to simple LSTM\n",
        "        final_lstm_model = Sequential([\n",
        "            Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "            LSTM(32, activation='tanh'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        final_lstm_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Callbacks for better training\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    # Train with validation split\n",
        "    history = final_lstm_model.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_split=0.2,\n",
        "        epochs=100,  # Reduced epochs for stability\n",
        "        batch_size=16,  # Smaller batch size\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # FIXED: Predictions and evaluation - proper scaling handling\n",
        "    lstm_predictions_scaled = final_lstm_model.predict(X_test_seq)\n",
        "\n",
        "    # Create a proper target scaler using the same approach as your best_scaler\n",
        "    # We need to scale only the target column for inverse transformation\n",
        "    target_scaler = type(best_scaler)()  # Use same scaler type as best_scaler\n",
        "\n",
        "    # Fit the target scaler on the training target data (original scale)\n",
        "    target_scaler.fit(train_df_final[[target]])\n",
        "\n",
        "    # Convert LSTM predictions back to original scale\n",
        "    lstm_predictions_original = target_scaler.inverse_transform(\n",
        "        lstm_predictions_scaled.reshape(-1, 1)\n",
        "    ).flatten()\n",
        "\n",
        "    print(f\"Debug - LSTM pred range (original): {lstm_predictions_original.min():.2f} to {lstm_predictions_original.max():.2f}\")\n",
        "    print(f\"Debug - Lengths - Actual: {len(y_test_actual_aligned)}, Predicted: {len(lstm_predictions_original)}\")\n",
        "\n",
        "    # Ensure lengths match exactly\n",
        "    min_length = min(len(y_test_actual_aligned), len(lstm_predictions_original))\n",
        "    y_test_actual_final = y_test_actual_aligned[:min_length]\n",
        "    lstm_predictions_final = lstm_predictions_original[:min_length]\n",
        "\n",
        "    # Calculate metrics on original scale with aligned data\n",
        "    lstm_metrics = calculate_metrics(y_test_actual_final, lstm_predictions_final)\n",
        "    print_metrics(\"Enhanced LSTM\", lstm_metrics)\n",
        "    add_model_result(\"LSTM\", lstm_metrics)\n",
        "    lstm_success = True\n",
        "\n",
        "    # SHAP interpretation (reduced sample for speed)\n",
        "    try:\n",
        "        print(\"Generating LSTM SHAP analysis...\")\n",
        "        background_data = X_train_seq[:min(20, len(X_train_seq)//2)]  # Even smaller sample\n",
        "        explainer_lstm = shap.DeepExplainer(final_lstm_model, background_data)\n",
        "        shap_sample_size = min(50, len(X_test_seq))\n",
        "        shap_values_lstm = explainer_lstm.shap_values(X_test_seq[:shap_sample_size])\n",
        "\n",
        "        shap_values_avg = np.mean(shap_values_lstm[0], axis=1)\n",
        "        X_test_avg_df = pd.DataFrame(np.mean(X_test_seq[:shap_sample_size], axis=1), columns=df_scaled.columns)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        shap.summary_plot(shap_values_avg, X_test_avg_df, show=False)\n",
        "        plt.title(\"Enhanced LSTM SHAP Feature Importance\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as shap_error:\n",
        "        print(f\"SHAP analysis failed for LSTM: {shap_error}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in enhanced LSTM: {e}\")\n",
        "    print(\"LSTM model training failed - continuing with other models...\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Try a simple backup LSTM if the enhanced one failed\n",
        "if not lstm_success:\n",
        "    try:\n",
        "        print(\"\\nTrying simple backup LSTM model...\")\n",
        "        timesteps = 7\n",
        "        X_train_seq, y_train_seq = create_sequences_lstm(train_scaled_final, target_col_index, timesteps)\n",
        "        X_test_seq, y_test_seq = create_sequences_lstm(test_scaled_final, target_col_index, timesteps)\n",
        "\n",
        "        # FIXED: Proper alignment for backup LSTM too\n",
        "        test_start_idx = timesteps\n",
        "        y_test_actual_aligned = test_df_final[target].iloc[test_start_idx:test_start_idx + len(X_test_seq)].values\n",
        "\n",
        "        if len(X_train_seq) >= 20:\n",
        "            simple_lstm = Sequential([\n",
        "                Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "                LSTM(16),\n",
        "                Dense(1)\n",
        "            ])\n",
        "            simple_lstm.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "            simple_lstm.fit(X_train_seq, y_train_seq, epochs=50, batch_size=8, verbose=0)\n",
        "\n",
        "            simple_predictions_scaled = simple_lstm.predict(X_test_seq)\n",
        "\n",
        "            # Use the same target scaler approach\n",
        "            simple_predictions_original = target_scaler.inverse_transform(\n",
        "                simple_predictions_scaled.reshape(-1, 1)\n",
        "            ).flatten()\n",
        "\n",
        "            # Ensure lengths match\n",
        "            min_length = min(len(y_test_actual_aligned), len(simple_predictions_original))\n",
        "            y_test_final = y_test_actual_aligned[:min_length]\n",
        "            simple_pred_final = simple_predictions_original[:min_length]\n",
        "\n",
        "            simple_metrics = calculate_metrics(y_test_final, simple_pred_final)\n",
        "            print_metrics(\"Simple LSTM (Backup)\", simple_metrics)\n",
        "            add_model_result(\"LSTM (Simple)\", simple_metrics)\n",
        "\n",
        "    except Exception as backup_error:\n",
        "        print(f\"Backup LSTM also failed: {backup_error}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# 2. FIXED NeuralProphet with Progressive Fallbacks\n",
        "print(\"\\nTraining NeuralProphet model...\")\n",
        "\n",
        "# First, try the most basic NeuralProphet possible\n",
        "try:\n",
        "    print(\"Attempting basic NeuralProphet...\")\n",
        "    with allow_unsafe_loading():\n",
        "        basic_np = NeuralProphet(\n",
        "            epochs=20,\n",
        "            learning_rate=0.01,\n",
        "            weekly_seasonality=True,\n",
        "            daily_seasonality=False,\n",
        "            yearly_seasonality=False\n",
        "            # REMOVED: progress=None (this parameter doesn't exist)\n",
        "        )\n",
        "\n",
        "        # Fit on basic ds, y data\n",
        "        basic_np.fit(train_prophet_format[['ds', 'y']], freq='D')\n",
        "\n",
        "        # Predict\n",
        "        future_basic = basic_np.make_future_dataframe(\n",
        "            train_prophet_format[['ds', 'y']],\n",
        "            periods=len(test_prophet_format)\n",
        "        )\n",
        "\n",
        "        forecast_basic = basic_np.predict(future_basic)\n",
        "\n",
        "        # Extract test predictions\n",
        "        basic_predictions = forecast_basic.tail(len(test_prophet_format))['yhat1'].values\n",
        "\n",
        "        # Calculate metrics\n",
        "        basic_np_metrics = calculate_metrics(test_prophet_format['y'], basic_predictions)\n",
        "        print_metrics(\"NeuralProphet (Basic)\", basic_np_metrics)\n",
        "        add_model_result(\"Neural Prophet\", basic_np_metrics)\n",
        "\n",
        "        print(\"✅ Basic NeuralProphet completed successfully!\")\n",
        "\n",
        "        # Try plotting\n",
        "        try:\n",
        "            fig_params = basic_np.plot_parameters()\n",
        "            plt.suptitle(\"NeuralProphet Model Components\")\n",
        "            plt.show()\n",
        "        except Exception as plot_error:\n",
        "            print(f\"NeuralProphet plotting failed: {plot_error}\")\n",
        "\n",
        "except Exception as basic_error:\n",
        "    print(f\"❌ Basic NeuralProphet failed: {basic_error}\")\n",
        "\n",
        "    # If even basic fails, try ultra-simple version\n",
        "    try:\n",
        "        print(\"Attempting ultra-simple NeuralProphet...\")\n",
        "        with allow_unsafe_loading():\n",
        "            ultra_simple_np = NeuralProphet(\n",
        "                epochs=10,\n",
        "                learning_rate=0.1,\n",
        "                weekly_seasonality=False,\n",
        "                daily_seasonality=False,\n",
        "                yearly_seasonality=False\n",
        "                # REMOVED: progress=None\n",
        "            )\n",
        "\n",
        "            ultra_simple_np.fit(train_prophet_format[['ds', 'y']], freq='D')\n",
        "\n",
        "            future_ultra = ultra_simple_np.make_future_dataframe(\n",
        "                train_prophet_format[['ds', 'y']],\n",
        "                periods=len(test_prophet_format)\n",
        "            )\n",
        "\n",
        "            forecast_ultra = ultra_simple_np.predict(future_ultra)\n",
        "            ultra_predictions = forecast_ultra.tail(len(test_prophet_format))['yhat1'].values\n",
        "\n",
        "            ultra_np_metrics = calculate_metrics(test_prophet_format['y'], ultra_predictions)\n",
        "            print_metrics(\"NeuralProphet (Ultra-Simple)\", ultra_np_metrics)\n",
        "            add_model_result(\"Neural Prophet\", ultra_np_metrics)\n",
        "\n",
        "            print(\"✅ Ultra-simple NeuralProphet completed!\")\n",
        "\n",
        "    except Exception as ultra_error:\n",
        "        print(f\"❌ Even ultra-simple NeuralProphet failed: {ultra_error}\")\n",
        "        print(\"Skipping NeuralProphet entirely.\")\n",
        "\n",
        "        # As absolute last resort, add a dummy Prophet model\n",
        "        try:\n",
        "            print(\"Adding regular Prophet as NeuralProphet substitute...\")\n",
        "            from prophet import Prophet\n",
        "\n",
        "            substitute_prophet = Prophet(weekly_seasonality=True, yearly_seasonality=False)\n",
        "            substitute_prophet.fit(train_prophet_format[['ds', 'y']])\n",
        "\n",
        "            future_substitute = substitute_prophet.make_future_dataframe(periods=len(test_prophet_format))\n",
        "            forecast_substitute = substitute_prophet.predict(future_substitute)\n",
        "\n",
        "            substitute_predictions = forecast_substitute.tail(len(test_prophet_format))['yhat'].values\n",
        "            substitute_metrics = calculate_metrics(test_prophet_format['y'], substitute_predictions)\n",
        "            print_metrics(\"Prophet (NeuralProphet Substitute)\", substitute_metrics)\n",
        "            add_model_result(\"Neural Prophet\", substitute_metrics)\n",
        "\n",
        "            print(\"✅ Prophet substitute completed!\")\n",
        "\n",
        "        except Exception as substitute_error:\n",
        "            print(f\"❌ Even Prophet substitute failed: {substitute_error}\")\n",
        "            print(\"No NeuralProphet variant could be completed.\")\n",
        "\n",
        "# 3. Enhanced Prophet + XGBoost Hybrid\n",
        "try:\n",
        "    print(\"\\nTraining enhanced Prophet + XGBoost hybrid...\")\n",
        "\n",
        "    # Enhanced Prophet with additional seasonalities\n",
        "    final_prophet_model = Prophet(\n",
        "        weekly_seasonality=True,\n",
        "        yearly_seasonality=True,\n",
        "        daily_seasonality=False,\n",
        "        seasonality_mode='multiplicative',  # Try multiplicative\n",
        "        changepoint_prior_scale=0.05,  # Adjust flexibility\n",
        "        seasonality_prior_scale=10.0\n",
        "    )\n",
        "\n",
        "    # Add custom seasonalities\n",
        "    final_prophet_model.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
        "    final_prophet_model.add_seasonality(name='quarterly', period=91.25, fourier_order=8)\n",
        "\n",
        "    final_prophet_model.fit(train_prophet_format[['ds', 'y']])\n",
        "\n",
        "    # Get Prophet predictions\n",
        "    future_df = final_prophet_model.make_future_dataframe(periods=len(test_prophet_format))\n",
        "    final_forecast_object = final_prophet_model.predict(future_df)\n",
        "\n",
        "    # Calculate residuals for training XGBoost\n",
        "    train_forecast_prophet = final_prophet_model.predict(train_prophet_format[['ds']])\n",
        "    train_residuals = train_prophet_format['y'] - train_forecast_prophet['yhat']\n",
        "\n",
        "    # Prepare XGBoost data\n",
        "    X_train_xgb = train_df_final[selected_features]  # Use selected features\n",
        "    X_test_xgb = test_df_final[selected_features]\n",
        "\n",
        "    # Split for hyperparameter search\n",
        "    val_split_xgb = int(0.8 * len(X_train_xgb))\n",
        "    X_train_xgb_hp = X_train_xgb.iloc[:val_split_xgb]\n",
        "    y_train_xgb_hp = train_residuals[:val_split_xgb]\n",
        "    X_val_xgb_hp = X_train_xgb.iloc[val_split_xgb:]\n",
        "    y_val_xgb_hp = train_residuals[val_split_xgb:]\n",
        "\n",
        "    # Find best XGBoost hyperparameters\n",
        "    print(\"Searching for best XGBoost hyperparameters...\")\n",
        "    best_xgb_config = grid_search_xgb(X_train_xgb_hp, y_train_xgb_hp, X_val_xgb_hp, y_val_xgb_hp)\n",
        "\n",
        "    # Train final XGBoost model with best config\n",
        "    final_xgb_model = create_optimized_xgb_model(best_xgb_config)\n",
        "    final_xgb_model.fit(X_train_xgb, train_residuals)\n",
        "\n",
        "    # Final hybrid predictions\n",
        "    test_prophet_pred = final_forecast_object.tail(len(test_prophet_format))['yhat'].values\n",
        "    test_xgb_residuals = final_xgb_model.predict(X_test_xgb)\n",
        "    hybrid_predictions = test_prophet_pred + test_xgb_residuals\n",
        "\n",
        "    hybrid_metrics = calculate_metrics(test_prophet_format['y'], hybrid_predictions)\n",
        "    print_metrics(\"Enhanced Hybrid Model\", hybrid_metrics)\n",
        "    add_model_result(\"Prophet + XGBoost Hybrid\", hybrid_metrics)\n",
        "\n",
        "    # Prophet components plot\n",
        "    try:\n",
        "        fig_components = final_prophet_model.plot_components(final_forecast_object)\n",
        "        plt.suptitle(\"Enhanced Prophet Components in Hybrid Model\")\n",
        "        plt.show()\n",
        "    except Exception as plot_error:\n",
        "        print(f\"Prophet plotting failed: {plot_error}\")\n",
        "\n",
        "    # XGBoost SHAP\n",
        "    try:\n",
        "        print(\"Generating XGBoost SHAP analysis...\")\n",
        "        explainer_xgb = shap.TreeExplainer(final_xgb_model)\n",
        "        shap_values_xgb = explainer_xgb.shap_values(X_test_xgb)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        shap.summary_plot(shap_values_xgb, X_test_xgb, show=False)\n",
        "        plt.title(\"Enhanced XGBoost SHAP Feature Importance (Hybrid Model)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as shap_error:\n",
        "        print(f\"XGBoost SHAP analysis failed: {shap_error}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in enhanced hybrid model: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Enhanced Model Training Complete ---\")\n",
        "print(\"Key improvements implemented:\")\n",
        "print(\"1. Advanced feature engineering with lags, rolling stats, and cyclical encoding\")\n",
        "print(\"2. Automated feature selection\")\n",
        "print(\"3. Multiple scaler comparison and selection\")\n",
        "print(\"4. Enhanced LSTM architecture with regularization and callbacks\")\n",
        "print(\"5. NeuralProphet with autoregressive components and regressors\")\n",
        "print(\"6. Prophet with custom seasonalities and multiplicative mode\")\n",
        "print(\"7. Optimized XGBoost hyperparameters\")\n",
        "print(\"8. Comprehensive evaluation metrics\")\n",
        "print(\"9. FIXED: Proper sequence alignment for LSTM predictions\")\n",
        "print(\"10. FIXED: Correct target scaling for inverse transformation\")\n",
        "print(\"11. ✅ CRITICAL FIX: Eliminated data leakage by splitting before feature engineering\")\n",
        "\n",
        "# Print the final leaderboard\n",
        "print(\"\\n\")\n",
        "print_leaderboard()"
      ],
      "metadata": {
        "id": "RMftiEDsDQKG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}