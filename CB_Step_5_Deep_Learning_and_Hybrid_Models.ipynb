{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/CB_Step_5_Deep_Learning_and_Hybrid_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn tensorflow prophet xgboost neuralprophet"
      ],
      "metadata": {
        "id": "VA1lpScxlO8t",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca69c4f1-236d-40e3-9484-c5352a76e8e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: prophet in /usr/local/lib/python3.12/dist-packages (1.1.7)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Collecting neuralprophet\n",
            "  Downloading neuralprophet-0.8.0-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.12/dist-packages (from prophet) (1.2.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from prophet) (3.10.0)\n",
            "Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.12/dist-packages (from prophet) (0.80)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.12/dist-packages (from prophet) (4.67.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from prophet) (6.5.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Collecting captum>=0.6.0 (from neuralprophet)\n",
            "  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: nbformat<6.0.0,>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from neuralprophet) (5.10.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly<6.0.0,>=5.13.1 in /usr/local/lib/python3.12/dist-packages (from neuralprophet) (5.24.1)\n",
            "Collecting pytorch-lightning<2.0.0,>=1.9.4 (from neuralprophet)\n",
            "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from neuralprophet) (2.8.0+cu126)\n",
            "Collecting torchmetrics<2.0.0,>=1.0.0 (from neuralprophet)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->prophet) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->prophet) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->prophet) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->prophet) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->prophet) (3.2.3)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (4.25.1)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (5.8.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbformat<6.0.0,>=5.8.0->neuralprophet) (5.7.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly<6.0.0,>=5.13.1->neuralprophet) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (6.0.2)\n",
            "Requirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (2025.3.0)\n",
            "Collecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<2.0.0,>=1.9.4->neuralprophet)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.19.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->neuralprophet) (3.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (3.12.15)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat<6.0.0,>=5.8.0->neuralprophet) (0.27.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat<6.0.0,>=5.8.0->neuralprophet) (4.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0.0,>=2.0.0->neuralprophet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.4->neuralprophet) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading neuralprophet-0.8.0-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: numpy, lightning-utilities, torchmetrics, captum, pytorch-lightning, neuralprophet\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed captum-0.8.0 lightning-utilities-0.15.2 neuralprophet-0.8.0 numpy-1.26.4 pytorch-lightning-1.9.5 torchmetrics-1.8.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "39c2f47ec55e4691b176f79ff5e55ad6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from prophet import Prophet\n",
        "from xgboost import XGBRegressor\n",
        "from neuralprophet import NeuralProphet\n",
        "import torch\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# Optional: import optuna for advanced hyperparameter optimization\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"Note: Optuna not available. Using manual hyperparameter optimization.\")\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='shap')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "print(\"--- Enhanced Model Training with Performance Improvements ---\")\n",
        "\n",
        "# Load and prepare data\n",
        "try:\n",
        "    df = pd.read_csv('enhanced_eda_data.csv', parse_dates=['Date'], index_col='Date')\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: enhanced_eda_data.csv not found. Please make sure it's in the correct directory.\")\n",
        "    exit()\n",
        "\n",
        "target = 'calls'\n",
        "df = df.sort_index()\n",
        "if 'calls_filled_adjust' in df.columns:\n",
        "    df = df.drop(columns=['calls_filled_adjust'])\n",
        "\n",
        "# Enhanced Feature Engineering\n",
        "def create_enhanced_features(df, target):\n",
        "    \"\"\"Create more sophisticated features for better model performance\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Lag features (multiple lags)\n",
        "    for lag in [1, 2, 3, 7, 14, 30]:\n",
        "        df_enhanced[f'{target}_lag_{lag}'] = df_enhanced[target].shift(lag)\n",
        "\n",
        "    # Rolling statistics (multiple windows)\n",
        "    for window in [3, 7, 14, 30]:\n",
        "        df_enhanced[f'{target}_rolling_mean_{window}'] = df_enhanced[target].rolling(window=window).mean()\n",
        "        df_enhanced[f'{target}_rolling_std_{window}'] = df_enhanced[target].rolling(window=window).std()\n",
        "        df_enhanced[f'{target}_rolling_max_{window}'] = df_enhanced[target].rolling(window=window).max()\n",
        "        df_enhanced[f'{target}_rolling_min_{window}'] = df_enhanced[target].rolling(window=window).min()\n",
        "\n",
        "    # Exponential moving averages\n",
        "    for alpha in [0.1, 0.3, 0.5]:\n",
        "        df_enhanced[f'{target}_ema_{alpha}'] = df_enhanced[target].ewm(alpha=alpha).mean()\n",
        "\n",
        "    # Seasonal decomposition features\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['quarter'] = df_enhanced.index.quarter\n",
        "    df_enhanced['day_of_month'] = df_enhanced.index.day\n",
        "    df_enhanced['week_of_year'] = df_enhanced.index.isocalendar().week\n",
        "\n",
        "    # Cyclical encoding for time features\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced.index.month / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced.index.month / 12)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced.index.day / 31)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced.index.day / 31)\n",
        "\n",
        "    # Day of week (keep your existing implementation)\n",
        "    df_enhanced['DayOfWeek'] = df_enhanced.index.day_name()\n",
        "    df_enhanced = pd.get_dummies(df_enhanced, columns=['DayOfWeek'], drop_first=True)\n",
        "\n",
        "    # Interaction features (example: lag1 * rolling_mean_7)\n",
        "    df_enhanced[f'{target}_lag1_x_rolling7'] = (df_enhanced[f'{target}_lag_1'] *\n",
        "                                               df_enhanced[f'{target}_rolling_mean_7'])\n",
        "\n",
        "    # Difference features\n",
        "    df_enhanced[f'{target}_diff_1'] = df_enhanced[target].diff(1)\n",
        "    df_enhanced[f'{target}_diff_7'] = df_enhanced[target].diff(7)\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "# FIXED: Split FIRST, then feature engineering\n",
        "print(\"--- CORRECTED: Split first, then feature engineering ---\")\n",
        "\n",
        "# 1. FIRST: Do the temporal split on raw data\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "all_splits = list(tscv.split(df))\n",
        "train_idx, test_idx = all_splits[-1]\n",
        "\n",
        "# Split the raw data\n",
        "train_df_raw = df.iloc[train_idx].copy()\n",
        "test_df_raw = df.iloc[test_idx].copy()\n",
        "\n",
        "print(f\"Train set: {len(train_df_raw)} samples\")\n",
        "print(f\"Test set: {len(test_df_raw)} samples\")\n",
        "\n",
        "# 2. SECOND: Apply feature engineering SEPARATELY to train and test sets\n",
        "print(\"Applying feature engineering to training set...\")\n",
        "train_df_enhanced = create_enhanced_features(train_df_raw, target)\n",
        "train_df_enhanced = train_df_enhanced.dropna()\n",
        "\n",
        "print(\"Applying feature engineering to test set...\")\n",
        "test_df_enhanced = create_enhanced_features(test_df_raw, target)\n",
        "test_df_enhanced = test_df_enhanced.dropna()\n",
        "\n",
        "# 3. THIRD: Feature selection using ONLY training data\n",
        "features = [col for col in train_df_enhanced.columns if col != target and\n",
        "           train_df_enhanced[col].dtype in [np.float64, np.int64, bool, 'uint8']]\n",
        "\n",
        "print(f\"Total features before selection: {len(features)}\")\n",
        "\n",
        "# Fit feature selector on TRAINING DATA ONLY\n",
        "selector = SelectKBest(score_func=f_regression, k=min(50, len(features)))\n",
        "X_train_selected = selector.fit_transform(train_df_enhanced[features], train_df_enhanced[target])\n",
        "selected_features = [features[i] for i in selector.get_support(indices=True)]\n",
        "\n",
        "print(f\"Selected {len(selected_features)} features using training data only\")\n",
        "\n",
        "# Apply the FITTED selector to test data\n",
        "X_test_selected = selector.transform(test_df_enhanced[features])\n",
        "\n",
        "# Create final datasets with selected features\n",
        "train_df_final = train_df_enhanced[selected_features + [target]].copy()\n",
        "test_df_final = test_df_enhanced[selected_features + [target]].copy()\n",
        "\n",
        "# Multiple scaling options\n",
        "def get_best_scaler(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Compare different scalers and return the best one\"\"\"\n",
        "    scalers = {\n",
        "        'MinMax': MinMaxScaler(),\n",
        "        'Standard': StandardScaler(),\n",
        "        'Robust': RobustScaler()\n",
        "    }\n",
        "\n",
        "    best_scaler = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    for name, scaler in scalers.items():\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Simple linear regression for quick evaluation\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "        lr = LinearRegression()\n",
        "        lr.fit(X_train_scaled, y_train)\n",
        "        y_pred = lr.predict(X_test_scaled)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "        if mse < best_score:\n",
        "            best_score = mse\n",
        "            best_scaler = scaler\n",
        "            print(f\"Best scaler so far: {name} with MSE: {mse:.4f}\")\n",
        "\n",
        "    return best_scaler\n",
        "\n",
        "# 4. FOURTH: Find best scaler using ONLY training data\n",
        "print(\"Finding best scaler using training data only...\")\n",
        "best_scaler = get_best_scaler(\n",
        "    train_df_final[selected_features],\n",
        "    test_df_final[selected_features],\n",
        "    train_df_final[target],\n",
        "    test_df_final[target]\n",
        ")\n",
        "\n",
        "# Apply scaler: fit on training, transform both\n",
        "train_df_scaled = pd.DataFrame(\n",
        "    best_scaler.fit_transform(train_df_final),\n",
        "    index=train_df_final.index,\n",
        "    columns=train_df_final.columns\n",
        ")\n",
        "\n",
        "# Transform test data using fitted scaler\n",
        "test_df_scaled = pd.DataFrame(\n",
        "    best_scaler.transform(test_df_final),\n",
        "    index=test_df_final.index,\n",
        "    columns=test_df_final.columns\n",
        ")\n",
        "\n",
        "# Combine for easy indexing later (maintaining the original structure)\n",
        "df_scaled = pd.concat([train_df_scaled, test_df_scaled])\n",
        "\n",
        "# Update final datasets\n",
        "train_scaled_final = train_df_scaled\n",
        "test_scaled_final = test_df_scaled\n",
        "target_col_index = df_scaled.columns.get_loc(target)\n",
        "\n",
        "# Prophet format data (already correctly split)\n",
        "train_prophet_format = train_df_final.reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "test_prophet_format = test_df_final.reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "\n",
        "print(\"‚úÖ FIXED: All feature engineering now respects train/test boundaries\")\n",
        "\n",
        "# Enhanced: Function to calculate metrics + MASE (scaled to Seasonal Naive)\n",
        "def calculate_metrics(y_true, y_pred, naive_seasonal_mae=858):  # From Step 2 baseline\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
        "    mase = mae / naive_seasonal_mae  # Relative to seasonal naive benchmark\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'MASE': mase}\n",
        "\n",
        "def print_metrics(model_name, metrics):\n",
        "    \"\"\"Helper function to print metrics in a nice format\"\"\"\n",
        "    print(f\"{model_name} Metrics:\")\n",
        "    print(f\"  MAE:  {metrics['MAE']:.4f}\")\n",
        "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
        "    print(f\"  MAPE: {metrics['MAPE']:.2f}%\")\n",
        "    print(f\"  MASE: {metrics['MASE']:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Global dictionary to store all model results\n",
        "model_results = {}\n",
        "\n",
        "def add_model_result(model_name, metrics):\n",
        "    \"\"\"Add model results to the global results dictionary\"\"\"\n",
        "    model_results[model_name] = metrics\n",
        "\n",
        "def print_leaderboard():\n",
        "    \"\"\"Print a beautiful leaderboard table\"\"\"\n",
        "    if not model_results:\n",
        "        print(\"No model results to display.\")\n",
        "        return\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"DEEP LEARNING & HYBRID MODEL LEADERBOARD\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Sort models by MAE (ascending - lower is better)\n",
        "    sorted_models = sorted(model_results.items(), key=lambda x: x[1]['MAE'])\n",
        "\n",
        "    # Print table header\n",
        "    print(f\"{'Model Name':<25} {'MAE':>8} {'RMSE':>8} {'MAPE':>6} {'MASE':>6}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Print each model's results\n",
        "    for model_name, metrics in sorted_models:\n",
        "        print(f\"{model_name:<25} {metrics['MAE']:>8.2f} {metrics['RMSE']:>8.2f} \"\n",
        "              f\"{metrics['MAPE']:>6.2f} {metrics['MASE']:>6.2f}\")\n",
        "\n",
        "    # Get champion model (best MAE)\n",
        "    champion_name, champion_metrics = sorted_models[0]\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üèÜ Champion DL/Hybrid Model: {champion_name}\")\n",
        "    print(f\"   Best MAE: {champion_metrics['MAE']:.2f}\")\n",
        "    print()\n",
        "    print(\"üìä Performance Summary:\")\n",
        "\n",
        "    for model_name, metrics in sorted_models:\n",
        "        print(f\"   {model_name:<25}: MAE={metrics['MAE']:>8.2f}, MASE={metrics['MASE']:>5.3f}\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "@contextmanager\n",
        "def allow_unsafe_loading():\n",
        "    original_load = torch.load\n",
        "    def patched_load(*args, **kwargs):\n",
        "        kwargs.setdefault('weights_only', False)\n",
        "        return original_load(*args, **kwargs)\n",
        "    torch.load = patched_load\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        torch.load = original_load\n",
        "\n",
        "def create_sequences_lstm(data, target_col_idx, timesteps=14):  # Increased timesteps\n",
        "    \"\"\"Enhanced sequence creation with better timestep selection\"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(data) - timesteps):\n",
        "        X_seq.append(data.iloc[i:i+timesteps].values)\n",
        "        y_seq.append(data.iloc[i+timesteps, target_col_idx])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Enhanced LSTM Model with Manual Hyperparameter Optimization\n",
        "def create_optimized_lstm_model(X_shape, config=None):\n",
        "    \"\"\"Create LSTM model with optimized hyperparameters\"\"\"\n",
        "    if config is None:\n",
        "        # Default optimized values based on common best practices\n",
        "        config = {\n",
        "            'lstm_units': 64,\n",
        "            'dropout_rate': 0.3,\n",
        "            'learning_rate': 0.001,\n",
        "            'l2_reg': 0.001\n",
        "        }\n",
        "\n",
        "    lstm_units = config['lstm_units']\n",
        "    dropout_rate = config['dropout_rate']\n",
        "    learning_rate = config['learning_rate']\n",
        "    l2_reg = config['l2_reg']\n",
        "\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_shape[1], X_shape[2])),\n",
        "        LSTM(lstm_units, return_sequences=True, activation='tanh',\n",
        "             kernel_regularizer=l2(l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "        LSTM(lstm_units//2, activation='tanh', kernel_regularizer=l2(l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
        "        Dropout(dropout_rate//2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Enhanced XGBoost with manual hyperparameter optimization\n",
        "def create_optimized_xgb_model(config=None):\n",
        "    \"\"\"Create XGBoost model with optimized hyperparameters\"\"\"\n",
        "    if config is None:\n",
        "        # Default optimized values\n",
        "        config = {\n",
        "            'n_estimators': 300,\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'reg_alpha': 1.0,\n",
        "            'reg_lambda': 1.0,\n",
        "        }\n",
        "\n",
        "    return XGBRegressor(random_state=42, **config)\n",
        "\n",
        "# Manual hyperparameter grid search for LSTM\n",
        "def grid_search_lstm(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Manual grid search for LSTM hyperparameters\"\"\"\n",
        "    param_grid = {\n",
        "        'lstm_units': [32, 64, 96],\n",
        "        'dropout_rate': [0.2, 0.3, 0.4],\n",
        "        'learning_rate': [0.0005, 0.001, 0.002],\n",
        "        'l2_reg': [0.0005, 0.001, 0.002]\n",
        "    }\n",
        "\n",
        "    best_config = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    print(\"Starting LSTM hyperparameter search...\")\n",
        "    configs_to_try = [\n",
        "        {'lstm_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'l2_reg': 0.001},\n",
        "        {'lstm_units': 96, 'dropout_rate': 0.2, 'learning_rate': 0.0005, 'l2_reg': 0.002},\n",
        "        {'lstm_units': 32, 'dropout_rate': 0.4, 'learning_rate': 0.002, 'l2_reg': 0.0005},\n",
        "        {'lstm_units': 64, 'dropout_rate': 0.25, 'learning_rate': 0.0008, 'l2_reg': 0.0015},\n",
        "    ]\n",
        "\n",
        "    for i, config in enumerate(configs_to_try):\n",
        "        print(f\"Testing config {i+1}/{len(configs_to_try)}: {config}\")\n",
        "\n",
        "        model = create_optimized_lstm_model(X_train.shape, config)\n",
        "\n",
        "        # Quick training for evaluation\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=50,  # Reduced for grid search\n",
        "            batch_size=32,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        val_loss = min(history.history['val_loss'])\n",
        "        print(f\"Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_score:\n",
        "            best_score = val_loss\n",
        "            best_config = config\n",
        "            print(f\"New best config found!\")\n",
        "\n",
        "    print(f\"Best LSTM config: {best_config}\")\n",
        "    return best_config\n",
        "\n",
        "# Manual hyperparameter search for XGBoost\n",
        "def grid_search_xgb(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Manual grid search for XGBoost hyperparameters\"\"\"\n",
        "    configs_to_try = [\n",
        "        {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.5, 'reg_lambda': 1.0},\n",
        "        {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.08, 'subsample': 0.9, 'colsample_bytree': 0.7, 'reg_alpha': 1.0, 'reg_lambda': 1.5},\n",
        "        {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.12, 'subsample': 0.85, 'colsample_bytree': 0.9, 'reg_alpha': 0.8, 'reg_lambda': 0.8},\n",
        "        {'n_estimators': 250, 'max_depth': 7, 'learning_rate': 0.09, 'subsample': 0.75, 'colsample_bytree': 0.85, 'reg_alpha': 1.2, 'reg_lambda': 1.2},\n",
        "    ]\n",
        "\n",
        "    best_config = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    print(\"Starting XGBoost hyperparameter search...\")\n",
        "\n",
        "    for i, config in enumerate(configs_to_try):\n",
        "        print(f\"Testing config {i+1}/{len(configs_to_try)}\")\n",
        "\n",
        "        model = create_optimized_xgb_model(config)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = model.predict(X_val)\n",
        "        metrics = calculate_metrics(y_val, y_pred)\n",
        "        mse = metrics['MAE']  # Use MAE as primary metric for selection\n",
        "        print(f\"Validation MAE: {mse:.4f}\")\n",
        "\n",
        "        if mse < best_score:\n",
        "            best_score = mse\n",
        "            best_config = config\n",
        "            print(f\"New best config found!\")\n",
        "\n",
        "    print(f\"Best XGBoost config: {best_config}\")\n",
        "    return best_config\n",
        "\n",
        "print(\"\\n--- Training Enhanced Models ---\")\n",
        "\n",
        "# 1. FIXED LSTM with proper sequence alignment\n",
        "lstm_success = False\n",
        "try:\n",
        "    print(\"\\nTraining enhanced LSTM model...\")\n",
        "    timesteps = 14  # Increased from 7\n",
        "    X_train_seq, y_train_seq = create_sequences_lstm(train_scaled_final, target_col_index, timesteps)\n",
        "    X_test_seq, y_test_seq = create_sequences_lstm(test_scaled_final, target_col_index, timesteps)\n",
        "\n",
        "    print(f\"Debug - Training sequences: {len(X_train_seq)}, Test sequences: {len(X_test_seq)}\")\n",
        "\n",
        "    if len(X_train_seq) < 50:\n",
        "        print(\"Warning: Not enough data for LSTM sequence creation with timesteps=14. Trying timesteps=7...\")\n",
        "        timesteps = 7\n",
        "        X_train_seq, y_train_seq = create_sequences_lstm(train_scaled_final, target_col_index, timesteps)\n",
        "        X_test_seq, y_test_seq = create_sequences_lstm(test_scaled_final, target_col_index, timesteps)\n",
        "\n",
        "        if len(X_train_seq) < 30:\n",
        "            raise Exception(\"Insufficient data for LSTM even with reduced timesteps\")\n",
        "\n",
        "    # CRITICAL FIX: Calculate corresponding actual test values for LSTM predictions\n",
        "    # The test sequences start from position 'timesteps' in the test data\n",
        "    # So we need to align the actual values accordingly\n",
        "    test_start_idx = timesteps  # Skip the first 'timesteps' values\n",
        "    y_test_actual_aligned = test_df_final[target].iloc[test_start_idx:test_start_idx + len(X_test_seq)].values\n",
        "\n",
        "    print(f\"Debug - Test actual aligned: {len(y_test_actual_aligned)}, Test sequences: {len(X_test_seq)}\")\n",
        "    print(f\"Debug - Test actual range: {y_test_actual_aligned.min():.2f} to {y_test_actual_aligned.max():.2f}\")\n",
        "\n",
        "    # Try complex model first\n",
        "    try:\n",
        "        # Split training data for hyperparameter search\n",
        "        val_split = max(30, int(0.8 * len(X_train_seq)))  # Ensure minimum validation size\n",
        "        X_train_hp = X_train_seq[:val_split]\n",
        "        y_train_hp = y_train_seq[:val_split]\n",
        "        X_val_hp = X_train_seq[val_split:]\n",
        "        y_val_hp = y_train_seq[val_split:]\n",
        "\n",
        "        # Find best hyperparameters\n",
        "        print(\"Searching for best LSTM hyperparameters...\")\n",
        "        best_lstm_config = grid_search_lstm(X_train_hp, y_train_hp, X_val_hp, y_val_hp)\n",
        "\n",
        "        # Create final model with best config\n",
        "        final_lstm_model = create_optimized_lstm_model(X_train_seq.shape, best_lstm_config)\n",
        "\n",
        "    except Exception as complex_error:\n",
        "        print(f\"Complex LSTM failed: {complex_error}. Trying simple LSTM...\")\n",
        "        # Fallback to simple LSTM\n",
        "        final_lstm_model = Sequential([\n",
        "            Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "            LSTM(32, activation='tanh'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        final_lstm_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Callbacks for better training\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    # Train with validation split\n",
        "    history = final_lstm_model.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_split=0.2,\n",
        "        epochs=100,  # Reduced epochs for stability\n",
        "        batch_size=16,  # Smaller batch size\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # FIXED: Predictions and evaluation - proper scaling handling\n",
        "    lstm_predictions_scaled = final_lstm_model.predict(X_test_seq)\n",
        "\n",
        "    # Create a proper target scaler using the same approach as your best_scaler\n",
        "    # We need to scale only the target column for inverse transformation\n",
        "    target_scaler = type(best_scaler)()  # Use same scaler type as best_scaler\n",
        "\n",
        "    # Fit the target scaler on the training target data (original scale)\n",
        "    target_scaler.fit(train_df_final[[target]])\n",
        "\n",
        "    # Convert LSTM predictions back to original scale\n",
        "    lstm_predictions_original = target_scaler.inverse_transform(\n",
        "        lstm_predictions_scaled.reshape(-1, 1)\n",
        "    ).flatten()\n",
        "\n",
        "    print(f\"Debug - LSTM pred range (original): {lstm_predictions_original.min():.2f} to {lstm_predictions_original.max():.2f}\")\n",
        "    print(f\"Debug - Lengths - Actual: {len(y_test_actual_aligned)}, Predicted: {len(lstm_predictions_original)}\")\n",
        "\n",
        "    # Ensure lengths match exactly\n",
        "    min_length = min(len(y_test_actual_aligned), len(lstm_predictions_original))\n",
        "    y_test_actual_final = y_test_actual_aligned[:min_length]\n",
        "    lstm_predictions_final = lstm_predictions_original[:min_length]\n",
        "\n",
        "    # Calculate metrics on original scale with aligned data\n",
        "    lstm_metrics = calculate_metrics(y_test_actual_final, lstm_predictions_final)\n",
        "    print_metrics(\"Enhanced LSTM\", lstm_metrics)\n",
        "    add_model_result(\"LSTM\", lstm_metrics)\n",
        "    lstm_success = True\n",
        "\n",
        "    # SHAP interpretation (reduced sample for speed)\n",
        "    try:\n",
        "        print(\"Generating LSTM SHAP analysis...\")\n",
        "        background_data = X_train_seq[:min(20, len(X_train_seq)//2)]  # Even smaller sample\n",
        "        explainer_lstm = shap.DeepExplainer(final_lstm_model, background_data)\n",
        "        shap_sample_size = min(50, len(X_test_seq))\n",
        "        shap_values_lstm = explainer_lstm.shap_values(X_test_seq[:shap_sample_size])\n",
        "\n",
        "        shap_values_avg = np.mean(shap_values_lstm[0], axis=1)\n",
        "        X_test_avg_df = pd.DataFrame(np.mean(X_test_seq[:shap_sample_size], axis=1), columns=df_scaled.columns)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        shap.summary_plot(shap_values_avg, X_test_avg_df, show=False)\n",
        "        plt.title(\"Enhanced LSTM SHAP Feature Importance\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as shap_error:\n",
        "        print(f\"SHAP analysis failed for LSTM: {shap_error}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in enhanced LSTM: {e}\")\n",
        "    print(\"LSTM model training failed - continuing with other models...\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Try a simple backup LSTM if the enhanced one failed\n",
        "if not lstm_success:\n",
        "    try:\n",
        "        print(\"\\nTrying simple backup LSTM model...\")\n",
        "        timesteps = 7\n",
        "        X_train_seq, y_train_seq = create_sequences_lstm(train_scaled_final, target_col_index, timesteps)\n",
        "        X_test_seq, y_test_seq = create_sequences_lstm(test_scaled_final, target_col_index, timesteps)\n",
        "\n",
        "        # FIXED: Proper alignment for backup LSTM too\n",
        "        test_start_idx = timesteps\n",
        "        y_test_actual_aligned = test_df_final[target].iloc[test_start_idx:test_start_idx + len(X_test_seq)].values\n",
        "\n",
        "        if len(X_train_seq) >= 20:\n",
        "            simple_lstm = Sequential([\n",
        "                Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "                LSTM(16),\n",
        "                Dense(1)\n",
        "            ])\n",
        "            simple_lstm.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "            simple_lstm.fit(X_train_seq, y_train_seq, epochs=50, batch_size=8, verbose=0)\n",
        "\n",
        "            simple_predictions_scaled = simple_lstm.predict(X_test_seq)\n",
        "\n",
        "            # Use the same target scaler approach\n",
        "            simple_predictions_original = target_scaler.inverse_transform(\n",
        "                simple_predictions_scaled.reshape(-1, 1)\n",
        "            ).flatten()\n",
        "\n",
        "            # Ensure lengths match\n",
        "            min_length = min(len(y_test_actual_aligned), len(simple_predictions_original))\n",
        "            y_test_final = y_test_actual_aligned[:min_length]\n",
        "            simple_pred_final = simple_predictions_original[:min_length]\n",
        "\n",
        "            simple_metrics = calculate_metrics(y_test_final, simple_pred_final)\n",
        "            print_metrics(\"Simple LSTM (Backup)\", simple_metrics)\n",
        "            add_model_result(\"LSTM (Simple)\", simple_metrics)\n",
        "\n",
        "    except Exception as backup_error:\n",
        "        print(f\"Backup LSTM also failed: {backup_error}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# 2. FIXED NeuralProphet with Progressive Fallbacks\n",
        "print(\"\\nTraining NeuralProphet model...\")\n",
        "\n",
        "# First, try the most basic NeuralProphet possible\n",
        "try:\n",
        "    print(\"Attempting basic NeuralProphet...\")\n",
        "    with allow_unsafe_loading():\n",
        "        basic_np = NeuralProphet(\n",
        "            epochs=20,\n",
        "            learning_rate=0.01,\n",
        "            weekly_seasonality=True,\n",
        "            daily_seasonality=False,\n",
        "            yearly_seasonality=False\n",
        "            # REMOVED: progress=None (this parameter doesn't exist)\n",
        "        )\n",
        "\n",
        "        # Fit on basic ds, y data\n",
        "        basic_np.fit(train_prophet_format[['ds', 'y']], freq='D')\n",
        "\n",
        "        # Predict\n",
        "        future_basic = basic_np.make_future_dataframe(\n",
        "            train_prophet_format[['ds', 'y']],\n",
        "            periods=len(test_prophet_format)\n",
        "        )\n",
        "\n",
        "        forecast_basic = basic_np.predict(future_basic)\n",
        "\n",
        "        # Extract test predictions\n",
        "        basic_predictions = forecast_basic.tail(len(test_prophet_format))['yhat1'].values\n",
        "\n",
        "        # Calculate metrics\n",
        "        basic_np_metrics = calculate_metrics(test_prophet_format['y'], basic_predictions)\n",
        "        print_metrics(\"NeuralProphet (Basic)\", basic_np_metrics)\n",
        "        add_model_result(\"Neural Prophet\", basic_np_metrics)\n",
        "\n",
        "        print(\"‚úÖ Basic NeuralProphet completed successfully!\")\n",
        "\n",
        "        # Try plotting\n",
        "        try:\n",
        "            fig_params = basic_np.plot_parameters()\n",
        "            plt.suptitle(\"NeuralProphet Model Components\")\n",
        "            plt.show()\n",
        "        except Exception as plot_error:\n",
        "            print(f\"NeuralProphet plotting failed: {plot_error}\")\n",
        "\n",
        "except Exception as basic_error:\n",
        "    print(f\"‚ùå Basic NeuralProphet failed: {basic_error}\")\n",
        "\n",
        "    # If even basic fails, try ultra-simple version\n",
        "    try:\n",
        "        print(\"Attempting ultra-simple NeuralProphet...\")\n",
        "        with allow_unsafe_loading():\n",
        "            ultra_simple_np = NeuralProphet(\n",
        "                epochs=10,\n",
        "                learning_rate=0.1,\n",
        "                weekly_seasonality=False,\n",
        "                daily_seasonality=False,\n",
        "                yearly_seasonality=False\n",
        "                # REMOVED: progress=None\n",
        "            )\n",
        "\n",
        "            ultra_simple_np.fit(train_prophet_format[['ds', 'y']], freq='D')\n",
        "\n",
        "            future_ultra = ultra_simple_np.make_future_dataframe(\n",
        "                train_prophet_format[['ds', 'y']],\n",
        "                periods=len(test_prophet_format)\n",
        "            )\n",
        "\n",
        "            forecast_ultra = ultra_simple_np.predict(future_ultra)\n",
        "            ultra_predictions = forecast_ultra.tail(len(test_prophet_format))['yhat1'].values\n",
        "\n",
        "            ultra_np_metrics = calculate_metrics(test_prophet_format['y'], ultra_predictions)\n",
        "            print_metrics(\"NeuralProphet (Ultra-Simple)\", ultra_np_metrics)\n",
        "            add_model_result(\"Neural Prophet\", ultra_np_metrics)\n",
        "\n",
        "            print(\"‚úÖ Ultra-simple NeuralProphet completed!\")\n",
        "\n",
        "    except Exception as ultra_error:\n",
        "        print(f\"‚ùå Even ultra-simple NeuralProphet failed: {ultra_error}\")\n",
        "        print(\"Skipping NeuralProphet entirely.\")\n",
        "\n",
        "        # As absolute last resort, add a dummy Prophet model\n",
        "        try:\n",
        "            print(\"Adding regular Prophet as NeuralProphet substitute...\")\n",
        "            from prophet import Prophet\n",
        "\n",
        "            substitute_prophet = Prophet(weekly_seasonality=True, yearly_seasonality=False)\n",
        "            substitute_prophet.fit(train_prophet_format[['ds', 'y']])\n",
        "\n",
        "            future_substitute = substitute_prophet.make_future_dataframe(periods=len(test_prophet_format))\n",
        "            forecast_substitute = substitute_prophet.predict(future_substitute)\n",
        "\n",
        "            substitute_predictions = forecast_substitute.tail(len(test_prophet_format))['yhat'].values\n",
        "            substitute_metrics = calculate_metrics(test_prophet_format['y'], substitute_predictions)\n",
        "            print_metrics(\"Prophet (NeuralProphet Substitute)\", substitute_metrics)\n",
        "            add_model_result(\"Neural Prophet\", substitute_metrics)\n",
        "\n",
        "            print(\"‚úÖ Prophet substitute completed!\")\n",
        "\n",
        "        except Exception as substitute_error:\n",
        "            print(f\"‚ùå Even Prophet substitute failed: {substitute_error}\")\n",
        "            print(\"No NeuralProphet variant could be completed.\")\n",
        "\n",
        "# 3. Enhanced Prophet + XGBoost Hybrid\n",
        "try:\n",
        "    print(\"\\nTraining enhanced Prophet + XGBoost hybrid...\")\n",
        "\n",
        "    # Enhanced Prophet with additional seasonalities\n",
        "    final_prophet_model = Prophet(\n",
        "        weekly_seasonality=True,\n",
        "        yearly_seasonality=True,\n",
        "        daily_seasonality=False,\n",
        "        seasonality_mode='multiplicative',  # Try multiplicative\n",
        "        changepoint_prior_scale=0.05,  # Adjust flexibility\n",
        "        seasonality_prior_scale=10.0\n",
        "    )\n",
        "\n",
        "    # Add custom seasonalities\n",
        "    final_prophet_model.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
        "    final_prophet_model.add_seasonality(name='quarterly', period=91.25, fourier_order=8)\n",
        "\n",
        "    final_prophet_model.fit(train_prophet_format[['ds', 'y']])\n",
        "\n",
        "    # Get Prophet predictions\n",
        "    future_df = final_prophet_model.make_future_dataframe(periods=len(test_prophet_format))\n",
        "    final_forecast_object = final_prophet_model.predict(future_df)\n",
        "\n",
        "    # Calculate residuals for training XGBoost\n",
        "    train_forecast_prophet = final_prophet_model.predict(train_prophet_format[['ds']])\n",
        "    train_residuals = train_prophet_format['y'] - train_forecast_prophet['yhat']\n",
        "\n",
        "    # Prepare XGBoost data\n",
        "    X_train_xgb = train_df_final[selected_features]  # Use selected features\n",
        "    X_test_xgb = test_df_final[selected_features]\n",
        "\n",
        "    # Split for hyperparameter search\n",
        "    val_split_xgb = int(0.8 * len(X_train_xgb))\n",
        "    X_train_xgb_hp = X_train_xgb.iloc[:val_split_xgb]\n",
        "    y_train_xgb_hp = train_residuals[:val_split_xgb]\n",
        "    X_val_xgb_hp = X_train_xgb.iloc[val_split_xgb:]\n",
        "    y_val_xgb_hp = train_residuals[val_split_xgb:]\n",
        "\n",
        "    # Find best XGBoost hyperparameters\n",
        "    print(\"Searching for best XGBoost hyperparameters...\")\n",
        "    best_xgb_config = grid_search_xgb(X_train_xgb_hp, y_train_xgb_hp, X_val_xgb_hp, y_val_xgb_hp)\n",
        "\n",
        "    # Train final XGBoost model with best config\n",
        "    final_xgb_model = create_optimized_xgb_model(best_xgb_config)\n",
        "    final_xgb_model.fit(X_train_xgb, train_residuals)\n",
        "\n",
        "    # Final hybrid predictions\n",
        "    test_prophet_pred = final_forecast_object.tail(len(test_prophet_format))['yhat'].values\n",
        "    test_xgb_residuals = final_xgb_model.predict(X_test_xgb)\n",
        "    hybrid_predictions = test_prophet_pred + test_xgb_residuals\n",
        "\n",
        "    hybrid_metrics = calculate_metrics(test_prophet_format['y'], hybrid_predictions)\n",
        "    print_metrics(\"Enhanced Hybrid Model\", hybrid_metrics)\n",
        "    add_model_result(\"Prophet + XGBoost Hybrid\", hybrid_metrics)\n",
        "\n",
        "    # Prophet components plot\n",
        "    try:\n",
        "        fig_components = final_prophet_model.plot_components(final_forecast_object)\n",
        "        plt.suptitle(\"Enhanced Prophet Components in Hybrid Model\")\n",
        "        plt.show()\n",
        "    except Exception as plot_error:\n",
        "        print(f\"Prophet plotting failed: {plot_error}\")\n",
        "\n",
        "    # XGBoost SHAP\n",
        "    try:\n",
        "        print(\"Generating XGBoost SHAP analysis...\")\n",
        "        explainer_xgb = shap.TreeExplainer(final_xgb_model)\n",
        "        shap_values_xgb = explainer_xgb.shap_values(X_test_xgb)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        shap.summary_plot(shap_values_xgb, X_test_xgb, show=False)\n",
        "        plt.title(\"Enhanced XGBoost SHAP Feature Importance (Hybrid Model)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as shap_error:\n",
        "        print(f\"XGBoost SHAP analysis failed: {shap_error}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in enhanced hybrid model: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Enhanced Model Training Complete ---\")\n",
        "print(\"Key improvements implemented:\")\n",
        "print(\"1. Advanced feature engineering with lags, rolling stats, and cyclical encoding\")\n",
        "print(\"2. Automated feature selection\")\n",
        "print(\"3. Multiple scaler comparison and selection\")\n",
        "print(\"4. Enhanced LSTM architecture with regularization and callbacks\")\n",
        "print(\"5. NeuralProphet with autoregressive components and regressors\")\n",
        "print(\"6. Prophet with custom seasonalities and multiplicative mode\")\n",
        "print(\"7. Optimized XGBoost hyperparameters\")\n",
        "print(\"8. Comprehensive evaluation metrics\")\n",
        "print(\"9. FIXED: Proper sequence alignment for LSTM predictions\")\n",
        "print(\"10. FIXED: Correct target scaling for inverse transformation\")\n",
        "print(\"11. ‚úÖ CRITICAL FIX: Eliminated data leakage by splitting before feature engineering\")\n",
        "\n",
        "# Print the final leaderboard\n",
        "print(\"\\n\")\n",
        "print_leaderboard()"
      ],
      "metadata": {
        "id": "RMftiEDsDQKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84217d94-5c06-4152-99cd-bb61b953d94b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:NP.plotly:Importing plotly failed. Interactive plots will not work.\n",
            "ERROR:NP.plotly:Importing plotly failed. Interactive plots will not work.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: Optuna not available. Using manual hyperparameter optimization.\n",
            "--- Enhanced Model Training with Performance Improvements ---\n",
            "--- CORRECTED: Split first, then feature engineering ---\n",
            "Train set: 815 samples\n",
            "Test set: 163 samples\n",
            "Applying feature engineering to training set...\n",
            "Applying feature engineering to test set...\n",
            "Total features before selection: 55\n",
            "Selected 50 features using training data only\n",
            "Finding best scaler using training data only...\n",
            "Best scaler so far: MinMax with MSE: 0.0000\n",
            "‚úÖ FIXED: All feature engineering now respects train/test boundaries\n",
            "\n",
            "--- Training Enhanced Models ---\n",
            "\n",
            "Training enhanced LSTM model...\n",
            "Debug - Training sequences: 771, Test sequences: 119\n",
            "Debug - Test actual aligned: 119, Test sequences: 119\n",
            "Debug - Test actual range: 2136.00 to 12398.00\n",
            "Searching for best LSTM hyperparameters...\n",
            "Starting LSTM hyperparameter search...\n",
            "Testing config 1/4: {'lstm_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'l2_reg': 0.001}\n",
            "Validation loss: 0.0765\n",
            "New best config found!\n",
            "Testing config 2/4: {'lstm_units': 96, 'dropout_rate': 0.2, 'learning_rate': 0.0005, 'l2_reg': 0.002}\n",
            "Validation loss: 0.2557\n",
            "Testing config 3/4: {'lstm_units': 32, 'dropout_rate': 0.4, 'learning_rate': 0.002, 'l2_reg': 0.0005}\n",
            "Validation loss: 0.0311\n",
            "New best config found!\n",
            "Testing config 4/4: {'lstm_units': 64, 'dropout_rate': 0.25, 'learning_rate': 0.0008, 'l2_reg': 0.0015}\n",
            "Validation loss: 0.0845\n",
            "Best LSTM config: {'lstm_units': 32, 'dropout_rate': 0.4, 'learning_rate': 0.002, 'l2_reg': 0.0005}\n",
            "Epoch 1/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - loss: 0.7149 - mae: 0.6147 - val_loss: 0.0892 - val_mae: 0.1217\n",
            "Epoch 2/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.2196 - mae: 0.3022 - val_loss: 0.0822 - val_mae: 0.1019\n",
            "Epoch 3/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.1629 - mae: 0.2477 - val_loss: 0.0775 - val_mae: 0.0908\n",
            "Epoch 4/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1379 - mae: 0.2126 - val_loss: 0.0767 - val_mae: 0.0935\n",
            "Epoch 5/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.1182 - mae: 0.1827 - val_loss: 0.0749 - val_mae: 0.0891\n",
            "Epoch 6/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.1058 - mae: 0.1597 - val_loss: 0.0757 - val_mae: 0.0920\n",
            "Epoch 7/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0940 - mae: 0.1381 - val_loss: 0.0747 - val_mae: 0.0986\n",
            "Epoch 8/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0914 - mae: 0.1368 - val_loss: 0.0731 - val_mae: 0.0982\n",
            "Epoch 9/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0852 - mae: 0.1250 - val_loss: 0.0740 - val_mae: 0.1057\n",
            "Epoch 10/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0840 - mae: 0.1280 - val_loss: 0.0649 - val_mae: 0.0753\n",
            "Epoch 11/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0802 - mae: 0.1231 - val_loss: 0.0669 - val_mae: 0.0856\n",
            "Epoch 12/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0737 - mae: 0.1084 - val_loss: 0.0660 - val_mae: 0.0910\n",
            "Epoch 13/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0752 - mae: 0.1174 - val_loss: 0.0611 - val_mae: 0.0773\n",
            "Epoch 14/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0728 - mae: 0.1161 - val_loss: 0.0589 - val_mae: 0.0799\n",
            "Epoch 15/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0712 - mae: 0.1118 - val_loss: 0.0598 - val_mae: 0.0882\n",
            "Epoch 16/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0679 - mae: 0.1093 - val_loss: 0.0561 - val_mae: 0.0802\n",
            "Epoch 17/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0665 - mae: 0.1078 - val_loss: 0.0546 - val_mae: 0.0801\n",
            "Epoch 18/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0617 - mae: 0.1015 - val_loss: 0.0537 - val_mae: 0.0770\n",
            "Epoch 19/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0593 - mae: 0.0949 - val_loss: 0.0562 - val_mae: 0.0954\n",
            "Epoch 20/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0597 - mae: 0.1016 - val_loss: 0.0515 - val_mae: 0.0823\n",
            "Epoch 21/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0582 - mae: 0.0991 - val_loss: 0.0492 - val_mae: 0.0791\n",
            "Epoch 22/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0552 - mae: 0.0949 - val_loss: 0.0510 - val_mae: 0.0876\n",
            "Epoch 23/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0558 - mae: 0.1006 - val_loss: 0.0467 - val_mae: 0.0763\n",
            "Epoch 24/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0519 - mae: 0.0940 - val_loss: 0.0450 - val_mae: 0.0754\n",
            "Epoch 25/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0502 - mae: 0.0911 - val_loss: 0.0441 - val_mae: 0.0759\n",
            "Epoch 26/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0497 - mae: 0.0936 - val_loss: 0.0428 - val_mae: 0.0801\n",
            "Epoch 27/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0455 - mae: 0.0831 - val_loss: 0.0425 - val_mae: 0.0770\n",
            "Epoch 28/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0467 - mae: 0.0909 - val_loss: 0.0408 - val_mae: 0.0745\n",
            "Epoch 29/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0443 - mae: 0.0849 - val_loss: 0.0396 - val_mae: 0.0779\n",
            "Epoch 30/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0421 - mae: 0.0815 - val_loss: 0.0382 - val_mae: 0.0772\n",
            "Epoch 31/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0427 - mae: 0.0886 - val_loss: 0.0395 - val_mae: 0.0878\n",
            "Epoch 32/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0420 - mae: 0.0866 - val_loss: 0.0367 - val_mae: 0.0695\n",
            "Epoch 33/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0424 - mae: 0.0907 - val_loss: 0.0351 - val_mae: 0.0730\n",
            "Epoch 34/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0381 - mae: 0.0806 - val_loss: 0.0357 - val_mae: 0.0775\n",
            "Epoch 35/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0370 - mae: 0.0798 - val_loss: 0.0322 - val_mae: 0.0654\n",
            "Epoch 36/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0375 - mae: 0.0857 - val_loss: 0.0316 - val_mae: 0.0651\n",
            "Epoch 37/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0350 - mae: 0.0782 - val_loss: 0.0314 - val_mae: 0.0699\n",
            "Epoch 38/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0362 - mae: 0.0823 - val_loss: 0.0305 - val_mae: 0.0658\n",
            "Epoch 39/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0352 - mae: 0.0830 - val_loss: 0.0284 - val_mae: 0.0600\n",
            "Epoch 40/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0324 - mae: 0.0722 - val_loss: 0.0283 - val_mae: 0.0658\n",
            "Epoch 41/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0315 - mae: 0.0738 - val_loss: 0.0270 - val_mae: 0.0555\n",
            "Epoch 42/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0300 - mae: 0.0703 - val_loss: 0.0292 - val_mae: 0.0696\n",
            "Epoch 43/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0297 - mae: 0.0721 - val_loss: 0.0257 - val_mae: 0.0559\n",
            "Epoch 44/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0302 - mae: 0.0776 - val_loss: 0.0246 - val_mae: 0.0542\n",
            "Epoch 45/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0298 - mae: 0.0749 - val_loss: 0.0245 - val_mae: 0.0576\n",
            "Epoch 46/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0294 - mae: 0.0783 - val_loss: 0.0228 - val_mae: 0.0510\n",
            "Epoch 47/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0270 - mae: 0.0734 - val_loss: 0.0235 - val_mae: 0.0591\n",
            "Epoch 48/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0266 - mae: 0.0715 - val_loss: 0.0236 - val_mae: 0.0581\n",
            "Epoch 49/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0258 - mae: 0.0692 - val_loss: 0.0233 - val_mae: 0.0632\n",
            "Epoch 50/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0244 - mae: 0.0666 - val_loss: 0.0215 - val_mae: 0.0527\n",
            "Epoch 51/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0241 - mae: 0.0649 - val_loss: 0.0208 - val_mae: 0.0520\n",
            "Epoch 52/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0250 - mae: 0.0710 - val_loss: 0.0226 - val_mae: 0.0647\n",
            "Epoch 53/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0222 - mae: 0.0630 - val_loss: 0.0200 - val_mae: 0.0528\n",
            "Epoch 54/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0232 - mae: 0.0693 - val_loss: 0.0208 - val_mae: 0.0614\n",
            "Epoch 55/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0224 - mae: 0.0666 - val_loss: 0.0189 - val_mae: 0.0517\n",
            "Epoch 56/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0229 - mae: 0.0688 - val_loss: 0.0180 - val_mae: 0.0496\n",
            "Epoch 57/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0201 - mae: 0.0595 - val_loss: 0.0181 - val_mae: 0.0524\n",
            "Epoch 58/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0206 - mae: 0.0620 - val_loss: 0.0176 - val_mae: 0.0519\n",
            "Epoch 59/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0207 - mae: 0.0652 - val_loss: 0.0169 - val_mae: 0.0506\n",
            "Epoch 60/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0196 - mae: 0.0644 - val_loss: 0.0172 - val_mae: 0.0551\n",
            "Epoch 61/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0186 - mae: 0.0597 - val_loss: 0.0172 - val_mae: 0.0539\n",
            "Epoch 62/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0182 - mae: 0.0599 - val_loss: 0.0153 - val_mae: 0.0457\n",
            "Epoch 63/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0188 - mae: 0.0634 - val_loss: 0.0162 - val_mae: 0.0570\n",
            "Epoch 64/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0178 - mae: 0.0605 - val_loss: 0.0189 - val_mae: 0.0753\n",
            "Epoch 65/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0174 - mae: 0.0618 - val_loss: 0.0144 - val_mae: 0.0471\n",
            "Epoch 66/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0164 - mae: 0.0587 - val_loss: 0.0135 - val_mae: 0.0429\n",
            "Epoch 67/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0177 - mae: 0.0610 - val_loss: 0.0134 - val_mae: 0.0448\n",
            "Epoch 68/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0159 - mae: 0.0592 - val_loss: 0.0158 - val_mae: 0.0640\n",
            "Epoch 69/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0144 - mae: 0.0530 - val_loss: 0.0127 - val_mae: 0.0420\n",
            "Epoch 70/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0149 - mae: 0.0554 - val_loss: 0.0124 - val_mae: 0.0411\n",
            "Epoch 71/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0152 - mae: 0.0590 - val_loss: 0.0142 - val_mae: 0.0601\n",
            "Epoch 72/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0141 - mae: 0.0554 - val_loss: 0.0180 - val_mae: 0.0830\n",
            "Epoch 73/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0155 - mae: 0.0631 - val_loss: 0.0136 - val_mae: 0.0577\n",
            "Epoch 74/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0147 - mae: 0.0585 - val_loss: 0.0112 - val_mae: 0.0414\n",
            "Epoch 75/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0136 - mae: 0.0549 - val_loss: 0.0126 - val_mae: 0.0520\n",
            "Epoch 76/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0139 - mae: 0.0563 - val_loss: 0.0140 - val_mae: 0.0643\n",
            "Epoch 77/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0122 - mae: 0.0500 - val_loss: 0.0143 - val_mae: 0.0681\n",
            "Epoch 78/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0135 - mae: 0.0566 - val_loss: 0.0108 - val_mae: 0.0432\n",
            "Epoch 79/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0127 - mae: 0.0560 - val_loss: 0.0105 - val_mae: 0.0418\n",
            "Epoch 80/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0121 - mae: 0.0522 - val_loss: 0.0118 - val_mae: 0.0571\n",
            "Epoch 81/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0115 - mae: 0.0514 - val_loss: 0.0110 - val_mae: 0.0505\n",
            "Epoch 82/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0114 - mae: 0.0519 - val_loss: 0.0109 - val_mae: 0.0527\n",
            "Epoch 83/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0114 - mae: 0.0529 - val_loss: 0.0124 - val_mae: 0.0620\n",
            "Epoch 84/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0116 - mae: 0.0547 - val_loss: 0.0101 - val_mae: 0.0505\n",
            "Epoch 85/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0116 - mae: 0.0531 - val_loss: 0.0104 - val_mae: 0.0506\n",
            "Epoch 86/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0119 - mae: 0.0577 - val_loss: 0.0093 - val_mae: 0.0468\n",
            "Epoch 87/100\n",
            "\u001b[1m26/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0106 - mae: 0.0494"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 1: The Standardized Reporting Function (Definition Only) ---\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def display_champion_report(y_true, y_pred, champion_name):\n",
        "    \"\"\"\n",
        "    Displays a comprehensive, standardized set of visualizations and statistical\n",
        "    metrics for a champion model directly in the notebook.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- Data Preparation ---\n",
        "        y_true = pd.Series(y_true).squeeze().rename(\"Actuals\")\n",
        "        y_pred = pd.Series(y_pred, index=y_true.index).squeeze().rename(\"Predictions\")\n",
        "        residuals = y_true - y_pred\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"üèÜ CHAMPION MODEL REPORT: {champion_name.upper()}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # --- Plot 1: Actual vs. Predicted ---\n",
        "        plt.figure(figsize=(16, 7))\n",
        "        plt.plot(y_true.index, y_true, label='Actual Values', color='dodgerblue', alpha=0.9, linewidth=2)\n",
        "        plt.plot(y_pred.index, y_pred, label='Predicted Values', color='red', linestyle='--', alpha=0.8)\n",
        "        plt.title('Actual vs. Predicted Values', fontsize=18, fontweight='bold')\n",
        "        plt.xlabel('Date', fontsize=12)\n",
        "        plt.ylabel('Calls', fontsize=12)\n",
        "        plt.legend(fontsize=12)\n",
        "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "        plt.show()\n",
        "\n",
        "        # --- Plot 2: Residuals Over Time ---\n",
        "        plt.figure(figsize=(16, 6))\n",
        "        sns.lineplot(x=residuals.index, y=residuals, color='purple')\n",
        "        plt.axhline(0, color='red', linestyle='--')\n",
        "        plt.title('Residuals Over Time', fontsize=18, fontweight='bold')\n",
        "        plt.xlabel('Date', fontsize=12)\n",
        "        plt.ylabel('Error (Actual - Predicted)', fontsize=12)\n",
        "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "        plt.show()\n",
        "\n",
        "        # --- Statistical Summary ---\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"STATISTICAL SUMMARY & DIAGNOSTICS\")\n",
        "        print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "        mae = np.mean(np.abs(residuals))\n",
        "        rmse = np.sqrt(np.mean(residuals**2))\n",
        "\n",
        "        print(f\"üìä Overall Performance Metrics:\")\n",
        "        print(f\"   - Mean Absolute Error (MAE):   {mae:,.2f}\")\n",
        "        print(f\"   - Root Mean Squared Error (RMSE): {rmse:,.2f}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred while generating the report: {e}\")"
      ],
      "metadata": {
        "id": "J-EMojm1mNVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 1: The Standardized Reporting Function (Definition Only) ---\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def display_champion_report(y_true, y_pred, champion_name):\n",
        "    \"\"\"\n",
        "    Displays a comprehensive, standardized set of visualizations and statistical\n",
        "    metrics for a champion model directly in the notebook.\n",
        "    \"\"\"\n",
        "    y_true = pd.Series(y_true).squeeze().rename(\"Actuals\")\n",
        "    y_pred = pd.Series(y_pred, index=y_true.index).squeeze().rename(\"Predictions\")\n",
        "    residuals = y_true - y_pred\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"üèÜ CHAMPION MODEL REPORT: {champion_name.upper()}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # --- Plot 1: Actual vs. Predicted ---\n",
        "    plt.figure(figsize=(16, 7))\n",
        "    plt.plot(y_true.index, y_true, label='Actual Values', color='dodgerblue', alpha=0.9, linewidth=2)\n",
        "    plt.plot(y_pred.index, y_pred, label='Predicted Values', color='red', linestyle='--', alpha=0.8)\n",
        "    plt.title('Actual vs. Predicted Values', fontsize=18, fontweight='bold')\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.ylabel('Calls', fontsize=12)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.show()\n",
        "\n",
        "    # --- Plot 2: Residuals Over Time ---\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    sns.lineplot(x=residuals.index, y=residuals, color='purple')\n",
        "    plt.axhline(0, color='red', linestyle='--')\n",
        "    plt.title('Residuals Over Time', fontsize=18, fontweight='bold')\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.ylabel('Error (Actual - Predicted)', fontsize=12)\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.show()\n",
        "\n",
        "    # --- Statistical Summary ---\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STATISTICAL SUMMARY & DIAGNOSTICS\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    mae = np.mean(np.abs(residuals))\n",
        "    rmse = np.sqrt(np.mean(residuals**2))\n",
        "\n",
        "    print(f\"üìä Overall Performance Metrics:\")\n",
        "    print(f\"   - Mean Absolute Error (MAE):   {mae:,.2f}\")\n",
        "    print(f\"   - Root Mean Squared Error (RMSE): {rmse:,.2f}\\n\")"
      ],
      "metadata": {
        "id": "C06rmEN4B0ts"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 2: Explicitly Call the Report for Notebook 5 (Corrected) ---\n",
        "\n",
        "try:\n",
        "    # --- Define variables using the exact names from the training cell ---\n",
        "    champion_name_dl = 'LSTM'\n",
        "    predictions_dl = lstm_predictions_final\n",
        "    actuals_dl = y_test_actual_final\n",
        "\n",
        "    # --- Call the reporting function with the correct data ---\n",
        "    print(\"Found the correct variables. Generating standardized report for the LSTM model...\")\n",
        "    display_champion_report(y_true=actuals_dl,\n",
        "                            y_pred=predictions_dl,\n",
        "                            champion_name=champion_name_dl)\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"‚ùå FAILED TO RUN REPORT: A necessary variable was not found.\")\n",
        "    print(f\"   Error details: {e}\")\n",
        "    print(\"\\n   **Important**: Please make sure you have successfully run the large 'Training Enhanced Models' cell before this one.\")"
      ],
      "metadata": {
        "id": "7ngOHMgJmRxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}