{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/CB_Step_5_Deep_Learning_and_Hybrid_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn tensorflow prophet xgboost neuralprophet"
      ],
      "metadata": {
        "id": "VA1lpScxlO8t",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from prophet import Prophet\n",
        "from xgboost import XGBRegressor\n",
        "from neuralprophet import NeuralProphet\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# APPROACH 1: Comprehensive allowlisting with dynamic import handling\n",
        "def safe_import_and_allowlist():\n",
        "    \"\"\"Dynamically import and allowlist NeuralProphet components\"\"\"\n",
        "    components_to_allowlist = []\n",
        "\n",
        "    # Try to import all known NeuralProphet components\n",
        "    neuralprophet_imports = [\n",
        "        ('neuralprophet.configure', ['ConfigSeasonality', 'Train', 'Season', 'Trend', 'AR', 'Normalization']),\n",
        "        ('neuralprophet.df_utils', ['ShiftScale']),\n",
        "        ('neuralprophet.time_net', ['TimeNet']),\n",
        "        ('neuralprophet.components', ['Components']),\n",
        "    ]\n",
        "\n",
        "    for module_name, class_names in neuralprophet_imports:\n",
        "        try:\n",
        "            module = __import__(module_name, fromlist=class_names)\n",
        "            for class_name in class_names:\n",
        "                if hasattr(module, class_name):\n",
        "                    components_to_allowlist.append(getattr(module, class_name))\n",
        "                    print(f\"✓ Added {module_name}.{class_name} to allowlist\")\n",
        "        except ImportError as e:\n",
        "            print(f\"⚠ Could not import {module_name}: {e}\")\n",
        "\n",
        "    # Add PyTorch and NumPy components\n",
        "    pytorch_numpy_components = [\n",
        "        torch.nn.modules.loss.SmoothL1Loss,\n",
        "        torch.optim.AdamW,\n",
        "        torch.optim.lr_scheduler.OneCycleLR,\n",
        "        np.core.multiarray._reconstruct,\n",
        "        np.ndarray,\n",
        "        np.dtype,\n",
        "        np.dtypes.Float64DType\n",
        "    ]\n",
        "\n",
        "    components_to_allowlist.extend(pytorch_numpy_components)\n",
        "\n",
        "    # Apply the allowlist\n",
        "    torch.serialization.add_safe_globals(components_to_allowlist)\n",
        "    print(f\"✓ Total components allowlisted: {len(components_to_allowlist)}\")\n",
        "\n",
        "# APPROACH 2: Context manager for weights_only=False (recommended)\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def allow_unsafe_loading():\n",
        "    \"\"\"Context manager to temporarily disable weights_only for trusted libraries\"\"\"\n",
        "    original_load = torch.load\n",
        "    def patched_load(*args, **kwargs):\n",
        "        kwargs.setdefault('weights_only', False)\n",
        "        return original_load(*args, **kwargs)\n",
        "\n",
        "    torch.load = patched_load\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        torch.load = original_load\n",
        "\n",
        "# APPROACH 3: Simple global override (fastest)\n",
        "# Uncomment this line if you want the simplest solution:\n",
        "# torch.serialization._set_default_load_args({'weights_only': False})\n",
        "\n",
        "# Use the dynamic approach\n",
        "safe_import_and_allowlist()\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('enhanced_eda_data.csv', parse_dates=['Date'], index_col='Date')\n",
        "target = 'calls' # Corrected to lowercase\n",
        "\n",
        "# Prepare data\n",
        "df = df.sort_index()\n",
        "df['DayOfWeek'] = df.index.day_name()\n",
        "df = pd.get_dummies(df, columns=['DayOfWeek'], drop_first=True)\n",
        "df = df.dropna()\n",
        "\n",
        "features = [col for col in df.columns if col != target and df[col].dtype in [np.float64, np.int64, bool, 'uint8']]\n",
        "\n",
        "# Scale data for DL models\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
        "\n",
        "# Time series cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Calculate all three metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
        "\n",
        "model_metrics = {}\n",
        "\n",
        "# --- 1. LSTM Network ---\n",
        "print(\"--- Training LSTM ---\")\n",
        "def create_sequences_lstm(data, target_col_idx, timesteps=7):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(data) - timesteps):\n",
        "        X_seq.append(data.iloc[i:i+timesteps].values)\n",
        "        y_seq.append(data.iloc[i+timesteps, target_col_idx])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "target_col_index = df_scaled.columns.get_loc(target)\n",
        "lstm_trues, lstm_preds = [], []\n",
        "\n",
        "for train_idx, test_idx in tscv.split(df_scaled):\n",
        "    train, test = df_scaled.iloc[train_idx], df_scaled.iloc[test_idx]\n",
        "    X_train_seq, y_train_seq = create_sequences_lstm(train, target_col_index, timesteps=7)\n",
        "    X_test_seq, y_test_seq = create_sequences_lstm(test, target_col_index, timesteps=7)\n",
        "\n",
        "    model = Sequential([Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])), LSTM(50, activation='tanh'), Dense(1)])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "    pred_scaled = model.predict(X_test_seq, verbose=0)\n",
        "\n",
        "    lstm_preds.extend(pred_scaled.flatten())\n",
        "    lstm_trues.extend(y_test_seq.flatten())\n",
        "\n",
        "# Inverse transform all predictions and true values at once\n",
        "dummy_preds = np.zeros((len(lstm_preds), len(df.columns))); dummy_preds[:, target_col_index] = lstm_preds\n",
        "preds_inv = scaler.inverse_transform(dummy_preds)[:, target_col_index]\n",
        "\n",
        "dummy_trues = np.zeros((len(lstm_trues), len(df.columns))); dummy_trues[:, target_col_index] = lstm_trues\n",
        "trues_inv = scaler.inverse_transform(dummy_trues)[:, target_col_index]\n",
        "\n",
        "model_metrics['LSTM'] = calculate_metrics(trues_inv, preds_inv)\n",
        "\n",
        "# --- 2. Neural Prophet ---\n",
        "print(\"--- Training Neural Prophet ---\")\n",
        "np_trues, np_preds = [], []\n",
        "for train_idx, test_idx in tscv.split(df):\n",
        "    train_df = df.iloc[train_idx].reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "    test_df = df.iloc[test_idx]\n",
        "\n",
        "    # Use context manager to handle missing components gracefully\n",
        "    with allow_unsafe_loading():\n",
        "        model = NeuralProphet(epochs=50, batch_size=32)\n",
        "        model.fit(train_df[['ds', 'y']], freq='D', progress=None)\n",
        "\n",
        "        future = model.make_future_dataframe(train_df[['ds', 'y']], periods=len(test_df))\n",
        "        forecast = model.predict(future)\n",
        "\n",
        "    pred = forecast['yhat1'].tail(len(test_df)).values\n",
        "\n",
        "    np_preds.extend(pred)\n",
        "    np_trues.extend(test_df[target].values)\n",
        "model_metrics['Neural Prophet'] = calculate_metrics(np_trues, np_preds)\n",
        "\n",
        "# --- 3. Prophet + XGBoost Hybrid ---\n",
        "print(\"--- Training Prophet + XGBoost Hybrid ---\")\n",
        "hybrid_trues, hybrid_preds = [], []\n",
        "for train_idx, test_idx in tscv.split(df):\n",
        "    train_df_prophet = df.iloc[train_idx].reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "    test_df_prophet = df.iloc[test_idx].reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "\n",
        "    prophet_model = Prophet()\n",
        "    prophet_model.fit(train_df_prophet[['ds', 'y']])\n",
        "\n",
        "    train_forecast = prophet_model.predict(train_df_prophet[['ds']])\n",
        "    train_residuals = train_df_prophet['y'] - train_forecast['yhat']\n",
        "\n",
        "    test_future = prophet_model.make_future_dataframe(periods=len(test_df_prophet))\n",
        "    prophet_test_pred = prophet_model.predict(test_future)['yhat'].tail(len(test_df_prophet))\n",
        "\n",
        "    xgb_model = XGBRegressor(random_state=42)\n",
        "    xgb_model.fit(df.iloc[train_idx][features], train_residuals)\n",
        "\n",
        "    res_pred = xgb_model.predict(df.iloc[test_idx][features])\n",
        "    final_pred = prophet_test_pred.values + res_pred\n",
        "\n",
        "    hybrid_preds.extend(final_pred)\n",
        "    hybrid_trues.extend(test_df_prophet['y'].values)\n",
        "model_metrics['Prophet + XGBoost Hybrid'] = calculate_metrics(hybrid_trues, hybrid_preds)\n",
        "\n",
        "# --- Summarize ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\nDL & HYBRID MODEL LEADERBOARD\\n\" + \"=\"*50)\n",
        "metrics_df = pd.DataFrame(model_metrics).T.sort_values(\"MAE\")\n",
        "print(metrics_df)\n",
        "print(f\"\\n🏆 Champion DL/Hybrid Model: {metrics_df.index[0]}\")"
      ],
      "metadata": {
        "id": "LCKXEWQVzEME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from prophet import Prophet\n",
        "from xgboost import XGBRegressor\n",
        "from neuralprophet import NeuralProphet\n",
        "\n",
        "# Suppress a common SHAP warning related to TensorFlow internals\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='shap.explainers._deep.deep_tf')\n",
        "\n",
        "print(\"--- Preparing data and models for interpretation ---\")\n",
        "\n",
        "# --- Step 1: Create a single, final train/test split for consistent analysis ---\n",
        "# This assumes 'df', 'df_scaled', 'tscv', 'features', and 'target' exist from the previous cell.\n",
        "all_splits = list(tscv.split(df))\n",
        "train_idx, test_idx = all_splits[-1]\n",
        "\n",
        "# Unscaled data splits\n",
        "train_df_final = df.iloc[train_idx]\n",
        "test_df_final = df.iloc[test_idx]\n",
        "\n",
        "# Scaled data splits (for LSTM)\n",
        "train_scaled_final = df_scaled.iloc[train_idx]\n",
        "test_scaled_final = df_scaled.iloc[test_idx]\n",
        "\n",
        "# Prophet/NeuralProphet formatted data splits (correctly renaming 'Date' to 'ds')\n",
        "train_prophet_format = train_df_final.reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "test_prophet_format = test_df_final.reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "\n",
        "# Re-create the sequence helper function for self-containment\n",
        "def create_sequences(data, target_col, feature_cols, timesteps=7):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(data) - timesteps):\n",
        "        X_seq.append(data[feature_cols].iloc[i:i+timesteps].values)\n",
        "        y_seq.append(data[target_col].iloc[i+timesteps])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "\n",
        "# --- Step 2: Run Interpretability Analysis (Train -> Interpret for each model) ---\n",
        "print(\"\\n--- Generating Model Interpretation Plots ---\")\n",
        "\n",
        "# 1. LSTM Interpretability with SHAP\n",
        "try:\n",
        "    print(\"\\nTraining final LSTM model for interpretation...\")\n",
        "    # Create sequences using the final data split\n",
        "    X_train_seq, y_train_seq = create_sequences(train_scaled_final, target, df_scaled.columns, timesteps=7)\n",
        "    X_test_seq, _ = create_sequences(test_scaled_final, target, df_scaled.columns, timesteps=7)\n",
        "\n",
        "    # **CRITICAL FIX**: Define and train a fresh model instance here\n",
        "    final_lstm_model = Sequential([\n",
        "        Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "        LSTM(50, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    final_lstm_model.compile(optimizer='adam', loss='mse')\n",
        "    final_lstm_model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=32, verbose=0)\n",
        "    print(\"LSTM training complete.\")\n",
        "\n",
        "    # Now run SHAP on the clean, newly-trained model\n",
        "    background_data = X_train_seq[:100] # A subset of training data for the explainer\n",
        "    explainer_lstm = shap.DeepExplainer(final_lstm_model, background_data)\n",
        "    shap_values_lstm = explainer_lstm.shap_values(X_test_seq)\n",
        "\n",
        "    # Average SHAP values over timesteps for a summary plot\n",
        "    shap_values_lstm_avg = np.mean(shap_values_lstm[0], axis=1)\n",
        "\n",
        "    # Create a 2D DataFrame for plotting (features averaged over timesteps)\n",
        "    X_test_for_plot_df = pd.DataFrame(np.mean(X_test_seq, axis=1), columns=df_scaled.columns)\n",
        "\n",
        "    print(\"Displaying SHAP plot for LSTM...\")\n",
        "    plt.figure()\n",
        "    shap.summary_plot(shap_values_lstm_avg, X_test_for_plot_df, show=False)\n",
        "    plt.title(\"SHAP Feature Importance for LSTM (Averaged over Timesteps)\")\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during LSTM SHAP analysis: {e}\")\n",
        "\n",
        "# 2. NeuralProphet Interpretability\n",
        "try:\n",
        "    print(\"\\nTraining final Neural Prophet model for interpretation...\")\n",
        "    final_np_model = NeuralProphet(epochs=50, batch_size=32)\n",
        "    final_np_model.fit(train_prophet_format[['ds', 'y']], freq='D')\n",
        "    print(\"Neural Prophet training complete.\")\n",
        "\n",
        "    print(\"Displaying plot for Neural Prophet...\")\n",
        "    fig_params = final_np_model.plot_parameters()\n",
        "    plt.suptitle(\"NeuralProphet Model Components\", y=1.02)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during NeuralProphet plot generation: {e}\")\n",
        "\n",
        "# 3. Prophet + XGBoost Hybrid Interpretability\n",
        "# 3a. Prophet Components\n",
        "try:\n",
        "    print(\"\\nTraining final Prophet + XGBoost Hybrid model for interpretation...\")\n",
        "    # --- Prophet Part ---\n",
        "    final_prophet_model = Prophet(weekly_seasonality=True)\n",
        "    final_prophet_model.fit(train_prophet_format[['ds', 'y']])\n",
        "\n",
        "    # Create forecast object needed for plotting components\n",
        "    future_df = final_prophet_model.make_future_dataframe(periods=len(test_prophet_format))\n",
        "    final_forecast_object = final_prophet_model.predict(future_df)\n",
        "\n",
        "    print(\"Displaying component plot for Prophet (from Hybrid)...\")\n",
        "    fig_components = final_prophet_model.plot_components(final_forecast_object)\n",
        "    plt.suptitle(\"Prophet Component Contributions in Hybrid Model\", y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "    # --- XGBoost Part (needs residuals from Prophet) ---\n",
        "    # Calculate residuals on the training data\n",
        "    train_forecast_prophet = final_prophet_model.predict(train_prophet_format[['ds']])\n",
        "    train_residuals = train_prophet_format['y'] - train_forecast_prophet['yhat']\n",
        "\n",
        "    # Define feature sets for XGBoost\n",
        "    X_train_xgb = train_df_final[features]\n",
        "    X_test_xgb = test_df_final[features]\n",
        "\n",
        "    # Train XGBoost model on the residuals\n",
        "    final_xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "    final_xgb_model.fit(X_train_xgb, train_residuals)\n",
        "    print(\"Hybrid model training complete.\")\n",
        "\n",
        "    # 3b. XGBoost with SHAP\n",
        "    print(\"Displaying SHAP plot for XGBoost (from Hybrid)...\")\n",
        "    explainer_xgb = shap.TreeExplainer(final_xgb_model)\n",
        "    shap_values_xgb = explainer_xgb.shap_values(X_test_xgb)\n",
        "\n",
        "    plt.figure()\n",
        "    shap.summary_plot(shap_values_xgb, X_test_xgb, show=False)\n",
        "    plt.title(\"SHAP Feature Importance for XGBoost on Residuals (Hybrid Model)\")\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Hybrid model analysis: {e}\")"
      ],
      "metadata": {
        "id": "cEUCOR-5nZIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WEz1uQPf0sH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqNfvODG0sKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "32pELugq0sNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QmG3DRy80sPx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}