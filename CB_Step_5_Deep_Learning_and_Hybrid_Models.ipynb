{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-d-noble/callcenter/blob/main/CB_Step_5_Deep_Learning_and_Hybrid_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn tensorflow prophet xgboost neuralprophet"
      ],
      "metadata": {
        "id": "VA1lpScxlO8t",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from prophet import Prophet\n",
        "from xgboost import XGBRegressor\n",
        "from neuralprophet import NeuralProphet\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# APPROACH 1: Comprehensive allowlisting with dynamic import handling\n",
        "def safe_import_and_allowlist():\n",
        "    \"\"\"Dynamically import and allowlist NeuralProphet components and dependencies.\"\"\"\n",
        "    components_to_allowlist = []\n",
        "\n",
        "    # Try to import all known NeuralProphet components\n",
        "    neuralprophet_imports = [\n",
        "        ('neuralprophet.configure', ['ConfigSeasonality', 'Train', 'Season', 'Trend', 'AR', 'Normalization']),\n",
        "        ('neuralprophet.df_utils', ['ShiftScale']),\n",
        "        ('neuralprophet.time_net', ['TimeNet']),\n",
        "        ('neuralprophet.components', ['Components']),\n",
        "    ]\n",
        "\n",
        "    for module_name, class_names in neuralprophet_imports:\n",
        "        try:\n",
        "            module = __import__(module_name, fromlist=class_names)\n",
        "            for class_name in class_names:\n",
        "                if hasattr(module, class_name):\n",
        "                    components_to_allowlist.append(getattr(module, class_name))\n",
        "        except ImportError:\n",
        "            pass # Silently skip if a component is not found\n",
        "\n",
        "    # Add PyTorch, NumPy, and specific pandas components\n",
        "    pytorch_numpy_components = [\n",
        "        torch.nn.modules.loss.SmoothL1Loss,\n",
        "        torch.optim.AdamW,\n",
        "        torch.optim.lr_scheduler.OneCycleLR,\n",
        "        np.core.multiarray._reconstruct,\n",
        "        np.ndarray,\n",
        "        np.dtype,\n",
        "        np.dtypes.Float64DType,\n",
        "        pickle.Unpickler,\n",
        "    ]\n",
        "\n",
        "    # Try to add pandas timestamp components if they exist\n",
        "    try:\n",
        "        import pandas._libs.tslibs.timestamps as ts\n",
        "        if hasattr(ts, '_unpickle_timestamp'):\n",
        "            pytorch_numpy_components.append(ts._unpickle_timestamp)\n",
        "    except (ImportError, AttributeError):\n",
        "        pass\n",
        "\n",
        "    components_to_allowlist.extend(pytorch_numpy_components)\n",
        "\n",
        "    # Apply the allowlist safely\n",
        "    try:\n",
        "        torch.serialization.add_safe_globals(components_to_allowlist)\n",
        "        print(f\"‚úì Total components allowlisted: {len(components_to_allowlist)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not set allowlist: {e}\")\n",
        "\n",
        "# APPROACH 2: Context manager for weights_only=False (recommended)\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def allow_unsafe_loading():\n",
        "    \"\"\"Context manager to temporarily disable weights_only for trusted libraries\"\"\"\n",
        "    original_load = torch.load\n",
        "    def patched_load(*args, **kwargs):\n",
        "        kwargs.setdefault('weights_only', False)\n",
        "        return original_load(*args, **kwargs)\n",
        "\n",
        "    torch.load = patched_load\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        torch.load = original_load\n",
        "\n",
        "# Use the dynamic approach\n",
        "safe_import_and_allowlist()\n",
        "\n",
        "# Load data\n",
        "try:\n",
        "    df = pd.read_csv('enhanced_eda_data.csv', parse_dates=['Date'], index_col='Date')\n",
        "    print(\"‚úì Data loaded successfully\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'enhanced_eda_data.csv' not found. Please ensure the file exists in the current directory.\")\n",
        "    exit()\n",
        "\n",
        "target = 'calls' # Corrected to lowercase\n",
        "\n",
        "# Prepare data\n",
        "df = df.sort_index()\n",
        "\n",
        "# Drop the specified column if it exists\n",
        "if 'calls_filled_adjust' in df.columns:\n",
        "    df = df.drop(columns=['calls_filled_adjust'])\n",
        "    print(\"‚úì Dropped 'calls_filled_adjust' column.\")\n",
        "else:\n",
        "    print(\"‚Ñπ 'calls_filled_adjust' column not found.\")\n",
        "\n",
        "# Verify target column exists\n",
        "if target not in df.columns:\n",
        "    print(f\"Error: Target column '{target}' not found in data. Available columns: {list(df.columns)}\")\n",
        "    exit()\n",
        "\n",
        "# Add day of week features\n",
        "df['DayOfWeek'] = df.index.day_name()\n",
        "df = pd.get_dummies(df, columns=['DayOfWeek'], drop_first=True)\n",
        "df = df.dropna()\n",
        "\n",
        "print(f\"‚úì Data shape after preprocessing: {df.shape}\")\n",
        "print(f\"‚úì Target column '{target}' found with {df[target].notna().sum()} valid values\")\n",
        "\n",
        "# Select features (excluding target and non-numeric columns)\n",
        "features = [col for col in df.columns if col != target and df[col].dtype in [np.float64, np.int64, bool, 'uint8']]\n",
        "print(f\"‚úì Selected {len(features)} features: {features[:5]}...\")\n",
        "\n",
        "# Scale data for DL models\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
        "\n",
        "# Time series cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Enhanced: Function to calculate metrics + MASE (scaled to Seasonal Naive)\n",
        "def calculate_metrics(y_true, y_pred, naive_seasonal_mae=858): # From Step 2 baseline\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
        "    mase = mae / naive_seasonal_mae # Relative to seasonal naive benchmark\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'MASE': mase}\n",
        "\n",
        "model_metrics = {}\n",
        "\n",
        "# --- 1. LSTM Network ---\n",
        "print(\"\\n--- Training LSTM ---\")\n",
        "def create_sequences_lstm(data, target_col_idx, timesteps=7):\n",
        "    \"\"\"Create sequences for LSTM training\"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(data) - timesteps):\n",
        "        X_seq.append(data.iloc[i:i+timesteps].values)\n",
        "        y_seq.append(data.iloc[i+timesteps, target_col_idx])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "try:\n",
        "    target_col_index = df_scaled.columns.get_loc(target)\n",
        "    lstm_trues, lstm_preds = [], []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(df_scaled)):\n",
        "        print(f\"  LSTM Fold {fold_idx + 1}/5\")\n",
        "        train, test = df_scaled.iloc[train_idx], df_scaled.iloc[test_idx]\n",
        "\n",
        "        X_train_seq, y_train_seq = create_sequences_lstm(train, target_col_index, timesteps=7)\n",
        "        X_test_seq, y_test_seq = create_sequences_lstm(test, target_col_index, timesteps=7)\n",
        "\n",
        "        if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
        "            print(f\"    Skipping fold {fold_idx + 1}: insufficient data for sequences\")\n",
        "            continue\n",
        "\n",
        "        # Build and train LSTM model\n",
        "        model = Sequential([\n",
        "            Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "            LSTM(50, activation='tanh'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "        # Make predictions\n",
        "        pred_scaled = model.predict(X_test_seq, verbose=0)\n",
        "\n",
        "        lstm_preds.extend(pred_scaled.flatten())\n",
        "        lstm_trues.extend(y_test_seq.flatten())\n",
        "\n",
        "    # Inverse transform predictions\n",
        "    if len(lstm_preds) > 0:\n",
        "        dummy_preds = np.zeros((len(lstm_preds), len(df.columns)))\n",
        "        dummy_preds[:, target_col_index] = lstm_preds\n",
        "        preds_inv = scaler.inverse_transform(dummy_preds)[:, target_col_index]\n",
        "\n",
        "        dummy_trues = np.zeros((len(lstm_trues), len(df.columns)))\n",
        "        dummy_trues[:, target_col_index] = lstm_trues\n",
        "        trues_inv = scaler.inverse_transform(dummy_trues)[:, target_col_index]\n",
        "\n",
        "        model_metrics['LSTM'] = calculate_metrics(trues_inv, preds_inv)\n",
        "        print(\"  ‚úì LSTM training completed\")\n",
        "    else:\n",
        "        print(\"  ‚úó LSTM training failed: no predictions generated\")\n",
        "        model_metrics['LSTM'] = {'MAE': np.inf, 'RMSE': np.inf, 'MAPE': np.inf, 'MASE': np.inf}\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  ‚úó LSTM training failed: {e}\")\n",
        "    model_metrics['LSTM'] = {'MAE': np.inf, 'RMSE': np.inf, 'MAPE': np.inf, 'MASE': np.inf}\n",
        "\n",
        "# --- 2. Neural Prophet (Simplified Version) ---\n",
        "print(\"\\n--- Training Neural Prophet ---\")\n",
        "try:\n",
        "    np_trues, np_preds = [], []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(df)):\n",
        "        print(f\"  Neural Prophet Fold {fold_idx + 1}/5\")\n",
        "        try:\n",
        "            # Prepare data for Neural Prophet (only ds and y columns)\n",
        "            train_df_simple = df.iloc[train_idx].reset_index().rename(columns={'Date': 'ds', target: 'y'})[['ds', 'y']]\n",
        "            test_df_simple = df.iloc[test_idx]\n",
        "\n",
        "            with allow_unsafe_loading():\n",
        "                # Use a simplified Neural Prophet without external regressors for stability\n",
        "                model = NeuralProphet(\n",
        "                    epochs=50,\n",
        "                    batch_size=64,\n",
        "                    learning_rate=0.01,\n",
        "                    yearly_seasonality=True,\n",
        "                    weekly_seasonality=True,\n",
        "                    daily_seasonality=False,\n",
        "                    n_forecasts=1,\n",
        "                    trend_reg=0.1,\n",
        "                    seasonality_reg=0.1\n",
        "                )\n",
        "\n",
        "                # Fit the model with just time series data\n",
        "                model.fit(train_df_simple, freq='D', progress=None)\n",
        "\n",
        "                # Create future dataframe\n",
        "                future = model.make_future_dataframe(train_df_simple, periods=len(test_df_simple))\n",
        "\n",
        "                # Make predictions\n",
        "                forecast = model.predict(future)\n",
        "\n",
        "                # Extract predictions for test period\n",
        "                pred_values = forecast['yhat1'].tail(len(test_df_simple)).values\n",
        "\n",
        "                np_preds.extend(pred_values)\n",
        "                np_trues.extend(test_df_simple[target].values)\n",
        "                print(f\"    ‚úì Fold {fold_idx + 1} completed with {len(pred_values)} predictions\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ö† Error in fold {fold_idx + 1}: {str(e)[:100]}...\")\n",
        "            # Try even simpler version if the above fails\n",
        "            try:\n",
        "                print(f\"    Trying fallback approach for fold {fold_idx + 1}\")\n",
        "                train_df_simple = df.iloc[train_idx].reset_index().rename(columns={'Date': 'ds', target: 'y'})[['ds', 'y']]\n",
        "                test_df_simple = df.iloc[test_idx]\n",
        "\n",
        "                with allow_unsafe_loading():\n",
        "                    # Ultra-simple Neural Prophet\n",
        "                    model = NeuralProphet(\n",
        "                        epochs=30,\n",
        "                        weekly_seasonality=True,\n",
        "                        yearly_seasonality=False,\n",
        "                        daily_seasonality=False\n",
        "                    )\n",
        "\n",
        "                    model.fit(train_df_simple, freq='D', progress=None)\n",
        "                    future = model.make_future_dataframe(train_df_simple, periods=len(test_df_simple))\n",
        "                    forecast = model.predict(future)\n",
        "                    pred_values = forecast['yhat1'].tail(len(test_df_simple)).values\n",
        "\n",
        "                    np_preds.extend(pred_values)\n",
        "                    np_trues.extend(test_df_simple[target].values)\n",
        "                    print(f\"    ‚úì Fold {fold_idx + 1} completed with fallback approach\")\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f\"    ‚úó Both approaches failed for fold {fold_idx + 1}\")\n",
        "                continue\n",
        "\n",
        "    if len(np_preds) > 0:\n",
        "        model_metrics['Neural Prophet'] = calculate_metrics(np_trues, np_preds)\n",
        "        print(f\"  ‚úì Neural Prophet training completed with {len(np_preds)} total predictions\")\n",
        "    else:\n",
        "        print(\"  ‚úó Neural Prophet training failed: no predictions generated\")\n",
        "        model_metrics['Neural Prophet'] = {'MAE': np.inf, 'RMSE': np.inf, 'MAPE': np.inf, 'MASE': np.inf}\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  ‚úó Neural Prophet training failed: {e}\")\n",
        "    model_metrics['Neural Prophet'] = {'MAE': np.inf, 'RMSE': np.inf, 'MAPE': np.inf, 'MASE': np.inf}\n",
        "\n",
        "# --- 3. Prophet + XGBoost Hybrid ---\n",
        "print(\"\\n--- Training Prophet + XGBoost Hybrid ---\")\n",
        "try:\n",
        "    hybrid_trues, hybrid_preds = [], []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(df)):\n",
        "        print(f\"  Hybrid Fold {fold_idx + 1}/5\")\n",
        "        try:\n",
        "            # Prepare Prophet data\n",
        "            train_df_prophet = df.iloc[train_idx].reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "            test_df_prophet = df.iloc[test_idx].reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "\n",
        "            # Train Prophet\n",
        "            prophet_model = Prophet(\n",
        "                yearly_seasonality=False,\n",
        "                weekly_seasonality=True,\n",
        "                daily_seasonality=False\n",
        "            )\n",
        "            prophet_model.fit(train_df_prophet[['ds', 'y']])\n",
        "\n",
        "            # Get Prophet predictions for training data to calculate residuals\n",
        "            train_forecast = prophet_model.predict(train_df_prophet[['ds']])\n",
        "            train_residuals = train_df_prophet['y'].values - train_forecast['yhat'].values\n",
        "\n",
        "            # Get Prophet predictions for test data\n",
        "            test_future = test_df_prophet[['ds']]\n",
        "            prophet_test_forecast = prophet_model.predict(test_future)\n",
        "            prophet_test_pred = prophet_test_forecast['yhat'].values\n",
        "\n",
        "            # Train XGBoost on residuals\n",
        "            xgb_model = XGBRegressor(\n",
        "                n_estimators=100,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                random_state=42,\n",
        "                verbosity=0\n",
        "            )\n",
        "\n",
        "            # Ensure we have the right features for XGBoost\n",
        "            train_features = df.iloc[train_idx][features]\n",
        "            test_features = df.iloc[test_idx][features]\n",
        "\n",
        "            xgb_model.fit(train_features, train_residuals)\n",
        "\n",
        "            # Predict residuals for test set\n",
        "            res_pred = xgb_model.predict(test_features)\n",
        "\n",
        "            # Combine Prophet + XGBoost predictions\n",
        "            final_pred = prophet_test_pred + res_pred\n",
        "\n",
        "            hybrid_preds.extend(final_pred)\n",
        "            hybrid_trues.extend(test_df_prophet['y'].values)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error in fold {fold_idx + 1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if len(hybrid_preds) > 0:\n",
        "        model_metrics['Prophet + XGBoost Hybrid'] = calculate_metrics(hybrid_trues, hybrid_preds)\n",
        "        print(\"  ‚úì Hybrid training completed\")\n",
        "    else:\n",
        "        print(\"  ‚úó Hybrid training failed: no predictions generated\")\n",
        "        model_metrics['Prophet + XGBoost Hybrid'] = {'MAE': np.inf, 'RMSE': np.inf, 'MAPE': np.inf, 'MASE': np.inf}\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  ‚úó Hybrid training failed: {e}\")\n",
        "    model_metrics['Prophet + XGBoost Hybrid'] = {'MAE': np.inf, 'RMSE': np.inf, 'MAPE': np.inf, 'MASE': np.inf}\n",
        "\n",
        "# --- Summarize Results ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DEEP LEARNING & HYBRID MODEL LEADERBOARD\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if model_metrics:\n",
        "    metrics_df = pd.DataFrame(model_metrics).T\n",
        "\n",
        "    # Sort by MAE (lower is better)\n",
        "    metrics_df = metrics_df.sort_values(\"MAE\")\n",
        "\n",
        "    # Format the output nicely\n",
        "    print(metrics_df.round(2))\n",
        "\n",
        "    # Identify the best model\n",
        "    best_model = metrics_df.index[0]\n",
        "    best_mae = metrics_df.loc[best_model, 'MAE']\n",
        "\n",
        "    print(f\"\\nüèÜ Champion DL/Hybrid Model: {best_model}\")\n",
        "    print(f\"   Best MAE: {best_mae:.2f}\")\n",
        "\n",
        "    # Show performance summary\n",
        "    print(f\"\\nüìä Performance Summary:\")\n",
        "    for model_name in metrics_df.index:\n",
        "        mae = metrics_df.loc[model_name, 'MAE']\n",
        "        mase = metrics_df.loc[model_name, 'MASE']\n",
        "        if np.isfinite(mae):\n",
        "            print(f\"   {model_name:<25}: MAE={mae:8.2f}, MASE={mase:.3f}\")\n",
        "        else:\n",
        "            print(f\"   {model_name:<25}: Training failed\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No models completed successfully\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "id": "VYCC8HuPJ9RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PnahyF2tJ9Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from prophet import Prophet\n",
        "from xgboost import XGBRegressor\n",
        "from neuralprophet import NeuralProphet\n",
        "\n",
        "# Suppress a common SHAP warning related to TensorFlow internals\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='shap.explainers._deep.deep_tf')\n",
        "\n",
        "print(\"--- Preparing data and models for interpretation ---\")\n",
        "\n",
        "# --- Step 1: Create a single, final train/test split for consistent analysis ---\n",
        "# This assumes 'df', 'df_scaled', 'tscv', 'features', and 'target' exist from the previous cell.\n",
        "all_splits = list(tscv.split(df))\n",
        "train_idx, test_idx = all_splits[-1]\n",
        "\n",
        "# Unscaled data splits\n",
        "train_df_final = df.iloc[train_idx]\n",
        "test_df_final = df.iloc[test_idx]\n",
        "\n",
        "# Scaled data splits (for LSTM)\n",
        "train_scaled_final = df_scaled.iloc[train_idx]\n",
        "test_scaled_final = df_scaled.iloc[test_idx]\n",
        "\n",
        "# Prophet/NeuralProphet formatted data splits (correctly renaming 'Date' to 'ds')\n",
        "train_prophet_format = train_df_final.reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "test_prophet_format = test_df_final.reset_index().rename(columns={'Date': 'ds', target: 'y'})\n",
        "\n",
        "# Re-create the sequence helper function for self-containment\n",
        "def create_sequences(data, target_col, feature_cols, timesteps=7):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(data) - timesteps):\n",
        "        X_seq.append(data[feature_cols].iloc[i:i+timesteps].values)\n",
        "        y_seq.append(data[target_col].iloc[i+timesteps])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "\n",
        "# --- Step 2: Run Interpretability Analysis (Train -> Interpret for each model) ---\n",
        "print(\"\\n--- Generating Model Interpretation Plots ---\")\n",
        "\n",
        "# 1. LSTM Interpretability with SHAP\n",
        "try:\n",
        "    print(\"\\nTraining final LSTM model for interpretation...\")\n",
        "    # Create sequences using the final data split\n",
        "    X_train_seq, y_train_seq = create_sequences(train_scaled_final, target, df_scaled.columns, timesteps=7)\n",
        "    X_test_seq, _ = create_sequences(test_scaled_final, target, df_scaled.columns, timesteps=7)\n",
        "\n",
        "    # **CRITICAL FIX**: Define and train a fresh model instance here\n",
        "    final_lstm_model = Sequential([\n",
        "        Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "        LSTM(50, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    final_lstm_model.compile(optimizer='adam', loss='mse')\n",
        "    final_lstm_model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=32, verbose=0)\n",
        "    print(\"LSTM training complete.\")\n",
        "\n",
        "    # Now run SHAP on the clean, newly-trained model\n",
        "    background_data = X_train_seq[:100] # A subset of training data for the explainer\n",
        "    explainer_lstm = shap.DeepExplainer(final_lstm_model, background_data)\n",
        "    shap_values_lstm = explainer_lstm.shap_values(X_test_seq)\n",
        "\n",
        "    # Average SHAP values over timesteps for a summary plot\n",
        "    shap_values_lstm_avg = np.mean(shap_values_lstm[0], axis=1)\n",
        "\n",
        "    # Create a 2D DataFrame for plotting (features averaged over timesteps)\n",
        "    X_test_for_plot_df = pd.DataFrame(np.mean(X_test_seq, axis=1), columns=df_scaled.columns)\n",
        "\n",
        "    print(\"Displaying SHAP plot for LSTM...\")\n",
        "    plt.figure()\n",
        "    shap.summary_plot(shap_values_lstm_avg, X_test_for_plot_df, show=False)\n",
        "    plt.title(\"SHAP Feature Importance for LSTM (Averaged over Timesteps)\")\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during LSTM SHAP analysis: {e}\")\n",
        "\n",
        "# 2. NeuralProphet Interpretability\n",
        "try:\n",
        "    print(\"\\nTraining final Neural Prophet model for interpretation...\")\n",
        "    final_np_model = NeuralProphet(epochs=50, batch_size=32)\n",
        "    final_np_model.fit(train_prophet_format[['ds', 'y']], freq='D')\n",
        "    print(\"Neural Prophet training complete.\")\n",
        "\n",
        "    print(\"Displaying plot for Neural Prophet...\")\n",
        "    fig_params = final_np_model.plot_parameters()\n",
        "    plt.suptitle(\"NeuralProphet Model Components\", y=1.02)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during NeuralProphet plot generation: {e}\")\n",
        "\n",
        "# 3. Prophet + XGBoost Hybrid Interpretability\n",
        "# 3a. Prophet Components\n",
        "try:\n",
        "    print(\"\\nTraining final Prophet + XGBoost Hybrid model for interpretation...\")\n",
        "    # --- Prophet Part ---\n",
        "    final_prophet_model = Prophet(weekly_seasonality=True)\n",
        "    final_prophet_model.fit(train_prophet_format[['ds', 'y']])\n",
        "\n",
        "    # Create forecast object needed for plotting components\n",
        "    future_df = final_prophet_model.make_future_dataframe(periods=len(test_prophet_format))\n",
        "    final_forecast_object = final_prophet_model.predict(future_df)\n",
        "\n",
        "    print(\"Displaying component plot for Prophet (from Hybrid)...\")\n",
        "    fig_components = final_prophet_model.plot_components(final_forecast_object)\n",
        "    plt.suptitle(\"Prophet Component Contributions in Hybrid Model\", y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "    # --- XGBoost Part (needs residuals from Prophet) ---\n",
        "    # Calculate residuals on the training data\n",
        "    train_forecast_prophet = final_prophet_model.predict(train_prophet_format[['ds']])\n",
        "    train_residuals = train_prophet_format['y'] - train_forecast_prophet['yhat']\n",
        "\n",
        "    # Define feature sets for XGBoost\n",
        "    X_train_xgb = train_df_final[features]\n",
        "    X_test_xgb = test_df_final[features]\n",
        "\n",
        "    # Train XGBoost model on the residuals\n",
        "    final_xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "    final_xgb_model.fit(X_train_xgb, train_residuals)\n",
        "    print(\"Hybrid model training complete.\")\n",
        "\n",
        "    # 3b. XGBoost with SHAP\n",
        "    print(\"Displaying SHAP plot for XGBoost (from Hybrid)...\")\n",
        "    explainer_xgb = shap.TreeExplainer(final_xgb_model)\n",
        "    shap_values_xgb = explainer_xgb.shap_values(X_test_xgb)\n",
        "\n",
        "    plt.figure()\n",
        "    shap.summary_plot(shap_values_xgb, X_test_xgb, show=False)\n",
        "    plt.title(\"SHAP Feature Importance for XGBoost on Residuals (Hybrid Model)\")\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Hybrid model analysis: {e}\")"
      ],
      "metadata": {
        "id": "cEUCOR-5nZIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WEz1uQPf0sH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqNfvODG0sKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "32pELugq0sNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QmG3DRy80sPx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}